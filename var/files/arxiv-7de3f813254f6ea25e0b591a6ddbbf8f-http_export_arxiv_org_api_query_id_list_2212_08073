<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/JzJM5DSXO3FsVccKW1Q6sixNqvs</id>
  <title>arXiv Query: search_query=&amp;id_list=2212.08073&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-19T20:43:57Z</updated>
  <link href="https://arxiv.org/api/query?search_query=&amp;start=0&amp;max_results=10&amp;id_list=2212.08073" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>1</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2212.08073v1</id>
    <title>Constitutional AI: Harmlessness from AI Feedback</title>
    <updated>2022-12-15T06:19:23Z</updated>
    <link href="https://arxiv.org/abs/2212.08073v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2212.08073v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-12-15T06:19:23Z</published>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Yuntao Bai</name>
    </author>
    <author>
      <name>Saurav Kadavath</name>
    </author>
    <author>
      <name>Sandipan Kundu</name>
    </author>
    <author>
      <name>Amanda Askell</name>
    </author>
    <author>
      <name>Jackson Kernion</name>
    </author>
    <author>
      <name>Andy Jones</name>
    </author>
    <author>
      <name>Anna Chen</name>
    </author>
    <author>
      <name>Anna Goldie</name>
    </author>
    <author>
      <name>Azalia Mirhoseini</name>
    </author>
    <author>
      <name>Cameron McKinnon</name>
    </author>
    <author>
      <name>Carol Chen</name>
    </author>
    <author>
      <name>Catherine Olsson</name>
    </author>
    <author>
      <name>Christopher Olah</name>
    </author>
    <author>
      <name>Danny Hernandez</name>
    </author>
    <author>
      <name>Dawn Drain</name>
    </author>
    <author>
      <name>Deep Ganguli</name>
    </author>
    <author>
      <name>Dustin Li</name>
    </author>
    <author>
      <name>Eli Tran-Johnson</name>
    </author>
    <author>
      <name>Ethan Perez</name>
    </author>
    <author>
      <name>Jamie Kerr</name>
    </author>
    <author>
      <name>Jared Mueller</name>
    </author>
    <author>
      <name>Jeffrey Ladish</name>
    </author>
    <author>
      <name>Joshua Landau</name>
    </author>
    <author>
      <name>Kamal Ndousse</name>
    </author>
    <author>
      <name>Kamile Lukosuite</name>
    </author>
    <author>
      <name>Liane Lovitt</name>
    </author>
    <author>
      <name>Michael Sellitto</name>
    </author>
    <author>
      <name>Nelson Elhage</name>
    </author>
    <author>
      <name>Nicholas Schiefer</name>
    </author>
    <author>
      <name>Noemi Mercado</name>
    </author>
    <author>
      <name>Nova DasSarma</name>
    </author>
    <author>
      <name>Robert Lasenby</name>
    </author>
    <author>
      <name>Robin Larson</name>
    </author>
    <author>
      <name>Sam Ringer</name>
    </author>
    <author>
      <name>Scott Johnston</name>
    </author>
    <author>
      <name>Shauna Kravec</name>
    </author>
    <author>
      <name>Sheer El Showk</name>
    </author>
    <author>
      <name>Stanislav Fort</name>
    </author>
    <author>
      <name>Tamera Lanham</name>
    </author>
    <author>
      <name>Timothy Telleen-Lawton</name>
    </author>
    <author>
      <name>Tom Conerly</name>
    </author>
    <author>
      <name>Tom Henighan</name>
    </author>
    <author>
      <name>Tristan Hume</name>
    </author>
    <author>
      <name>Samuel R. Bowman</name>
    </author>
    <author>
      <name>Zac Hatfield-Dodds</name>
    </author>
    <author>
      <name>Ben Mann</name>
    </author>
    <author>
      <name>Dario Amodei</name>
    </author>
    <author>
      <name>Nicholas Joseph</name>
    </author>
    <author>
      <name>Sam McCandlish</name>
    </author>
    <author>
      <name>Tom Brown</name>
    </author>
    <author>
      <name>Jared Kaplan</name>
    </author>
  </entry>
</feed>
