<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/Fnd+9cL+YfQnpFNJfPhSoBdzNtg</id>
  <title>arXiv Query: search_query=&amp;id_list=2508.17580&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-19T21:23:54Z</updated>
  <link href="https://arxiv.org/api/query?search_query=&amp;start=0&amp;max_results=10&amp;id_list=2508.17580" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>1</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2508.17580v1</id>
    <title>UQ: Assessing Language Models on Unsolved Questions</title>
    <updated>2025-08-25T01:07:59Z</updated>
    <link href="https://arxiv.org/abs/2508.17580v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2508.17580v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-08-25T01:07:59Z</published>
    <arxiv:comment>FN, KZL, and NM are project co-leads and contributed equally. Project website: https://uq.stanford.edu</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Fan Nie</name>
    </author>
    <author>
      <name>Ken Ziyu Liu</name>
    </author>
    <author>
      <name>Zihao Wang</name>
    </author>
    <author>
      <name>Rui Sun</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <author>
      <name>Weijia Shi</name>
    </author>
    <author>
      <name>Huaxiu Yao</name>
    </author>
    <author>
      <name>Linjun Zhang</name>
    </author>
    <author>
      <name>Andrew Y. Ng</name>
    </author>
    <author>
      <name>James Zou</name>
    </author>
    <author>
      <name>Sanmi Koyejo</name>
    </author>
    <author>
      <name>Yejin Choi</name>
    </author>
    <author>
      <name>Percy Liang</name>
    </author>
    <author>
      <name>Niklas Muennighoff</name>
    </author>
  </entry>
</feed>
