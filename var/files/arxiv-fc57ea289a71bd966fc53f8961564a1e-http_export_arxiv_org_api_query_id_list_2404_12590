<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/wScqorjfF+1X31v17U9M/hHj61g</id>
  <title>arXiv Query: search_query=&amp;id_list=2404.12590&amp;start=0&amp;max_results=10</title>
  <updated>2025-11-19T17:08:09Z</updated>
  <link href="https://arxiv.org/api/query?search_query=&amp;start=0&amp;max_results=10&amp;id_list=2404.12590" type="application/atom+xml"/>
  <opensearch:itemsPerPage>10</opensearch:itemsPerPage>
  <opensearch:totalResults>1</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2404.12590v7</id>
    <title>The Files are in the Computer: On Copyright, Memorization, and Generative AI</title>
    <updated>2025-09-01T10:08:37Z</updated>
    <link href="https://arxiv.org/abs/2404.12590v7" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2404.12590v7" rel="related" type="application/pdf" title="pdf"/>
    <summary>The New York Times's copyright lawsuit against OpenAI and Microsoft alleges OpenAI's GPT models have "memorized" NYT articles. Other lawsuits make similar claims. But parties, courts, and scholars disagree on what memorization is, whether it is taking place, and what its copyright implications are. These debates are clouded by ambiguities over the nature of "memorization." We attempt to bring clarity to the conversation. We draw on the technical literature to provide a firm foundation for legal discussions, providing a precise definition of memorization: a model has "memorized" a piece of training data when (1) it is possible to reconstruct from the model (2) a near-exact copy of (3) a substantial portion of (4) that piece of training data. We distinguish memorization from "extraction" (user intentionally causes a model to generate a near-exact copy), from "regurgitation" (model generates a near-exact copy, regardless of user intentions), and from "reconstruction" (the near-exact copy can be obtained from the model by any means). Several consequences follow. (1) Not all learning is memorization. (2) Memorization occurs when a model is trained; regurgitation is a symptom not its cause. (3) A model that has memorized training data is a "copy" of that training data in the sense used by copyright. (4) A model is not like a VCR or other general-purpose copying technology; it is better at generating some types of outputs (possibly regurgitated ones) than others. (5) Memorization is not a phenomenon caused by "adversarial" users bent on extraction; it is latent in the model itself. (6) The amount of training data that a model memorizes is a consequence of choices made in training. (7) Whether or not a model that has memorized actually regurgitates depends on overall system design. In a very real sense, memorized training data is in the model--to quote Zoolander, the files are in the computer.</summary>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-04-19T02:37:09Z</published>
    <arxiv:primary_category term="cs.CY"/>
    <arxiv:journal_ref>Chicago-Kent Law Review Vol. 100, 2025</arxiv:journal_ref>
    <author>
      <name>A. Feder Cooper</name>
    </author>
    <author>
      <name>James Grimmelmann</name>
    </author>
  </entry>
</feed>
