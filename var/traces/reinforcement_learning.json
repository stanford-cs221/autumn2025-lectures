{
  "files": {
    "reinforcement_learning.py": "from edtrace import text, image\nfrom typing import Any\nfrom collections import defaultdict\nimport numpy as np\nfrom functools import partial\nfrom typing import Callable\nfrom mdp import FlakyTramMDP, policy_evaluation, value_iteration, tram_if_possible_policy, generate_rollout, MDP, Step, draw_graph, Rollout\n\nPolicy = Callable[[Any], Any]\n\nLEADERBOARD = {}  # method -> value\n\ndef update_leaderboard(method: str, value: float) -> dict[str, float]:\n    LEADERBOARD[method] = value\n    return LEADERBOARD\n\n\ndef main():\n    text(\"Last time: **Markov decision processes**\")\n    review_mdp()\n\n    text(\"This time: **reinforcement learning**\")\n    introduce_rl()\n    introduce_model_based()\n    introduce_model_free_monte_carlo()\n    introduce_sarsa()\n    introduce_q_learning()\n\n    text(\"Summary:\")\n    text(\"- Reinforcement learning: learn the optimal policy from interacting with the environment\")\n    text(\"- Agent (RL algorithm): `get_action` and `incorporate_feedback`\")\n    text(\"- Model-based value iteration: estimate the MDP then compute the optimal policy\")\n    text(\"- Model-free Monte Carlo: estimate Q-values directly from rollouts (on-policy)\")\n    text(\"- SARSA: estimate Q-values of the current policy as you rollout (on-policy, bootstrapping)\")\n    text(\"- Q-learning: estimate Q-values of the optimal policy (off-policy, bootstrapping)\")\n\n    text(\"Next time: how do we deal with huge state spaces?\")\n\n\ndef review_mdp():\n    text(\"MDP: start state, successors (action, probability, reward, next state), is_end, discount\")\n\n    # MDP\n    mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover\n    state = mdp.start_state()  # @inspect state\n    successors = mdp.successors(state)  # @inspect successors\n    is_end = mdp.is_end(successors[0].state)  # @inspect is_end\n    image(draw_graph(mdp).render(\"var/flaky_tram_graph\", format=\"png\"), width=400)  # @stepover\n\n    # Policy\n    text(\"Policy: maps state to action\") # @clear successors is_end\n    policy = partial(tram_if_possible_policy, mdp)\n    action = policy(state)  # @inspect action\n\n    # Value of policy\n    rollout = generate_rollout(mdp, policy)  # @inspect rollout @clear action\n    text(\"Value of policy: expected utility of policy\") # @clear rollout\n    text(\"Policy evaluation: computes value of a given policy\")\n    result = policy_evaluation(mdp, policy)  # @inspect result\n\n    # Optimizing the policy\n    text(\"Value iteration: computes value of the optimal policy\")\n    result = value_iteration(mdp)  # @inspect result\n\n    review_recurrences()\n\n    text(\"But can we find the optimal policy if we don't know the MDP?\")\n\n\ndef review_recurrences():\n    text(\"Policy evaluation recurrence:\")\n    image(\"images/policy_evaluation_recurrence.png\", width=400)\n\n    text(\"Value V_\u03c0(s): value of following policy \u03c0 from state s\")\n    text(\"V_\u03c0(s) = Q_\u03c0(s, \u03c0(s))\")\n\n    text(\"Q-value Q_\u03c0(s, a): value of taking action a in state s, and then following policy \u03c0\")\n    text(\"Q_\u03c0(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V_\u03c0(s'))\")\n\n    text(\"Value iteration recurrence:\")\n    image(\"images/value_iteration_recurrence.png\", width=400)\n\n    text(\"Optimal V-value V`*`(s): value of following the optimal policy from state s\")\n    text(\"V`*`(s) = max_a Q`*`(s, a)\")\n\n    text(\"Optimal Q-value Q`*`(s, a): value of taking action a in state s, and then following the optimal policy\")\n    text(\"Q`*`(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V`*`(s'))\")\n\n    text(\"Optimal policy: take the action with the highest Q-value\")\n    text(\"\u03c0`*`(s) = argmax_a Q`*`(s, a)\")\n\n\ndef introduce_rl():\n    text(\"Reinforcement learning setting:\")\n    image(\"images/rl-framework.png\", width=400)\n    text(\"Repeat:\")\n    text(\"- **Agent** produces action\")\n    text(\"- **Environment** produces reward and observation\")\n\n    text(\"Intuitively:\")\n    text(\"- A good agent should try various actions to find ones that lead to good rewards.\")\n    text(\"- Then it should *learn* to keep doing those actions (those actions are *reinforced*).\")\n\n    text(\"Reinforcement learning really is the metaphor for life.\")\n    text(\"In MDPs, we don't know what outcomes will be, but at least know their probabilities...\")\n    text(\"in RL, we don't even know what the probabilities will be - that's real life!\")\n\n    text(\"In this lecture, assume the environment is backed by an MDP and the observation is the next state.\")\n    text(\"(In real life, you only observe part of the state (partially observable MDPs.)\")\n\n    text(\"Let us define an environment (MDP):\")\n    mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover\n    np.random.seed(1)\n    text(\"...and an agent (RL algorithm):\")\n    policy = partial(tram_if_possible_policy, mdp)\n    rl = StaticAgent(policy=policy)\n    rl.get_action(state=1)\n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n\n    text(\"Now let's simulate the agent and the environment (generating rollouts).\")\n    value = simulate(mdp, rl, num_trials=10)  # @inspect value\n    leaderboard = update_leaderboard(\"tram_if_possible_policy\", value)  # @inspect leaderboard\n    text(\"Simulation yields some estimated value (expected utility).\")\n    text(\"The agent (RL algorithm) doesn't do anything with the feedback.\")\n\n    text(\"Difference between policy and agent:\")\n    text(\"- Policy: maps state to action, doesn't change over time (static)\")\n    text(\"- Agent (RL algorithm): maps state to action, can change over time (dynamic)\")\n\n    text(\"Intuition: RL algorithm uses the feedback to improve its internal policy.\")\n    text(\"How should we perform this update?\")\n\n\nclass RLAlgorithm:\n    \"\"\"\n    Abstract class for an RL algorithm, which does two things:\n    1. Sends actions to the environment\n    2. Incorporates feedback from the environment\n    \"\"\"\n    def get_action(self, state: Any) -> Any:\n        raise NotImplementedError\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:\n        raise NotImplementedError\n\n\nclass StaticAgent(RLAlgorithm):\n    def __init__(self, policy: Policy):\n        self.policy = policy\n\n    def get_action(self, state: Any) -> Any:\n        return self.policy(state)\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:\n        # Do nothing\n        pass\n\n\ndef simulate(mdp: MDP, rl: RLAlgorithm, num_trials: int) -> float:\n    \"\"\"\n    Runs the RL algorithm on the MDP.\n    Return the mean utility of the rollouts.\n    \"\"\"\n    utilities = []  # @inspect utilities\n\n    # Repeat multiple times\n    for trial in range(num_trials):\n        # Environment state\n        state = mdp.start_state()  # @inspect state\n\n        steps = []  # @inspect steps\n        while not mdp.is_end(state):  # @stepover\n            # Agent sends action to environment\n            action = rl.get_action(state)  # @inspect action @stepover\n\n            # Environment sends reward and next state to agent\n            step = sample_transition(mdp, state, action)  # @inspect step @stepover\n            rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover\n            steps.append(step)  # @inspect steps\n            state = step.state  # @inspect state\n\n        # Compute utility of this rollout\n        rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step\n        utilities.append(rollout.utility)  # @inspect utilities\n\n    mean_utility = np.mean(utilities).item()  # @inspect mean_utility\n    return mean_utility\n\n\ndef sample_transition(mdp: MDP, state: Any, action: Any) -> Step:  # @inspect state action\n    \"\"\"\n    Samples a transition from the MDP: samples s' with probability T(s, a, s').\n    Returns:\n    - reward: the reward for the transition\n    - next_state: the next state\n    - is_end: whether the next state is an end state\n    \"\"\"\n    # Get successors given (state, action)\n    successors = [successor for successor in mdp.successors(state) if successor.action == action]  # @inspect successors\n    if len(successors) == 0:\n        raise ValueError(f\"No successors found for state {state} and action {action}\")\n\n    # Sample a successor based on its probabilities\n    probs = [successor.prob for successor in successors]  # @inspect probs\n    choice = np.random.choice(len(successors), p=probs)  # @inspect choice\n    step = successors[choice]  # @inspect step\n    return step\n\n\ndef walk_tram_policy(num_locs: int, state: Any) -> Any:\n    \"\"\"Chooses a random valid action.\"\"\"\n    if 2 * state <= num_locs:\n        # Can take the tram\n        return np.random.choice([\"walk\", \"tram\"]).item()\n    else:\n        # Can only walk\n        return \"walk\"\n\n\ndef introduce_model_based():\n    text(\"What makes RL hard:\")\n    text(\"- We don't know the MDP.\")\n    text(\"- Otherwise, we can use value iteration to compute the optimal policy.\")\n    text(\"Idea: estimate (learn) the MDP from feedback!\")\n\n    text(\"**Model-based value iteration**:\")\n    text(\"1. Exploration: estimate the MDP using an exploration policy (random).\")\n    text(\"2. Compute the optimal policy of the estimated MDP.\")\n    text(\"3. Exploitation: follow this policy.\")\n\n    text(\"Let's define our familiar flaky tram MDP:\")\n    mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover\n    np.random.seed(1)\n\n    text(\"Define an exploration policy that chooses a random valid action:\")\n    exploration_policy = partial(walk_tram_policy, mdp.num_locs)\n    try_out_exploration_policy(exploration_policy)\n    \n    # Define the agent (RL algorithm)\n    rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)\n    try_out_model_based_value_iteration(rl)\n\n    text(\"Stage 1: explore using the exploration policy to estimate the MDP.\")\n    value = simulate(mdp, rl, num_trials=10)  # @inspect value @stepover\n    leaderboard = update_leaderboard(\"model_based_value_iteration.explore\", value)  # @inspect leaderboard @stepover\n    compare_mdps(mdp, rl.mdp)\n\n    text(\"Stage 2: compute the optimal policy of the estimated MDP and follow that.\")\n    rl.run_value_iteration()  # @inspect rl.exploitation_policy\n    compare_policies(value_iteration(mdp), rl.exploitation_policy)\n\n    text(\"Stage 3: run using this estimated policy.\")\n    value = simulate(mdp, rl, num_trials=10)  # @inspect value @stepover\n    leaderboard = update_leaderboard(\"model_based_value_iteration.exploit\", value)  # @inspect leaderboard @stepover\n\n    text(\"Notes:\")\n    text(\"- The utility of the exploration phase is suboptimal, but we're learning!\")\n    text(\"- In practice, we don't need to restrict to two phases\")\n    text(\"- Always continue refining the estimated MDP\")\n    text(\"- Gradually move the policy from full exploration to full exploitation\")\n\n    text(\"Summary:\")\n    text(\"- Model-based RL: estimate the MDP from feedback (explore)\")\n    text(\"- Once have estimated MDP, use value iteration to compute the optimal policy (of estimated MDP)\")\n    text(\"- Once have estimated policy, exploit!\")\n\n    text(\"Can we estimate the optimal policy more directly?\")\n    \n    \nclass ModelBasedValueIteration(RLAlgorithm):\n    \"\"\"\n    Model-based RL algorithm that uses value iteration to estimate the MDP.\n    There are two stages:\n    - explore: follow the `exploration_policy` and estimate the MDP\n    - exploit: use the estimated MDP to choose actions\n    \"\"\"\n    def __init__(self, exploration_policy: Policy, discount: float):\n        self.exploration_policy = exploration_policy\n        self.exploitation_policy = None\n        self.mdp = EstimatedMDP(discount=discount)\n\n    def run_value_iteration(self):\n        # Run value iteration to compute the optimal policy for the estimated MDP\n        result = value_iteration(self.mdp)  # @inspect result @stepover\n        # Use this policy for exploitation\n        self.exploitation_policy = result.pi\n\n    def get_action(self, state: Any) -> Any:\n        # Either follow the exploration policy or the exploitation policy\n        if self.exploitation_policy is None:\n            return self.exploration_policy(state)\n        else:\n            return self.exploitation_policy[state]\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:\n        # Update the estimated MDP with the feedback\n        self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp\n\n\nclass EstimatedMDP(MDP):\n    \"\"\"An MDP whose start state, rewards, transitions, is_end is learned from feedback during RL.\"\"\"\n    def __init__(self, discount: float):\n        self.start_state_ = None\n        self.rewards = defaultdict(float)  # (state, action, state') -> reward\n        self.transitions = defaultdict(lambda : defaultdict(lambda : defaultdict(int)))  # state -> action -> state' -> count\n        self.end_states = set()\n        self.discount_ = discount\n\n    def start_state(self) -> Any:\n        return self.start_state_\n    \n    def successors(self, state: Any) -> list[Step]:\n        \"\"\"Compute successors based on the transition counts and rewards.\"\"\"\n        successors = []  # @inspect successors\n        # For each action...\n        for action, next_state_counts in self.transitions[state].items():  # @inspect action next_state_counts\n            total_count = sum(next_state_counts.values())  # @inspect total_count\n            # For each next state...\n            for next_state, count in next_state_counts.items():\n                # Compute the transition probability, reward\n                prob = count / total_count\n                reward = self.rewards[(state, action, next_state)]\n                step = Step(action=action, prob=prob, reward=reward, state=next_state)\n                successors.append(step)  # @inspect successors\n\n        return successors\n    \n    def is_end(self, state: Any) -> bool:\n        return state in self.end_states\n\n    def discount(self) -> float:\n        return self.discount_\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:\n        \"\"\"Update the MDP based on the feedback.\"\"\"\n        if self.start_state_ is None:\n            self.start_state_ = state\n\n        self.rewards[(state, action, next_state)] = reward\n        self.transitions[state][action][next_state] += 1\n\n        if is_end:\n            self.end_states.add(next_state)\n\n    def asdict(self) -> dict[str, Any]:\n        return {\n            \"start_state_\": self.start_state_,\n            \"rewards\": self.rewards,\n            \"transition_counts\": self.transitions,\n            \"end_states\": list(self.end_states),\n        }\n\n\ndef try_out_model_based_value_iteration(rl: ModelBasedValueIteration):\n    action = rl.get_action(state=1)  # @inspect action\n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    \ndef try_out_exploration_policy(exploration_policy: Policy):\n    text(\"The exploration policy tries all valid actions.\")\n    action = exploration_policy(state=1)  # @inspect action\n    action = exploration_policy(state=1)  # @inspect action @stepover\n    action = exploration_policy(state=1)  # @inspect action @stepover\n    action = exploration_policy(state=6)  # @inspect action\n\n\ndef compare_mdps(true_mdp: MDP, estimated_mdp: MDP):\n    text(\"Let's look at the internals of the estimated MDP.\")  # @inspect estimated_mdp\n\n    text(\"Let us compare the true MDP and the estimated MDP.\")\n    true_start_state = true_mdp.start_state()  # @inspect true_start_state\n    estimated_start_state = estimated_mdp.start_state()  # @inspect estimated_start_state\n\n    true_successors = true_mdp.successors(state=1)  # @inspect true_successors\n    estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors\n\n    true_is_end = true_mdp.is_end(state=9)  # @inspect true_is_end\n    estimated_is_end = estimated_mdp.is_end(state=9)  # @inspect estimated_is_end\n\n    text(\"The more the agent explores, estimated MDP \u2192 true MDP\")\n    text(\"...assuming the exploration policy tries all valid actions.\")\n\n\ndef compare_policies(optimal_policy: dict[Any, Any], estimated_policy: dict[Any, Any]):\n    text(\"We compare:\")\n    text(\"- Optimal policy: the optimal policy for the true MDP (unknown to the agent)\")  # @inspect optimal_policy\n    text(\"- Estimated policy: the optimal policy for the estimated MDP (known to the agent)\")  # @inspect estimated_policy\n\n    text(\"Note the two are similar but not quite the same.\")\n    text(\"The more the agent explores, estimated policy \u2192 optimal policy.\")\n\n\ndef introduce_model_free_monte_carlo():\n    text(\"Previously: model-based value iteration:\")\n    text(\"1. Estimate the MDP first.\")\n    text(\"2. Use value iteration to compute the optimal policy of the estimated MDP.\")\n    \n    image(\"images/value_iteration_recurrence.png\", width=400)\n    text(\"Optimal policy: \u03c0`*`(s) = argmax_a Q`*`(s, a)\")\n    text(\"where Q`*`(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V`*`(s'))\")\n    text(\"Can we estimate Q`*`(s, a) directly?\")\n\n    text(\"Yes!\")\n    text(\"Key idea: just rollout the policy and average the utilities!\")\n\n    example_utility_calculation()\n\n    text(\"Which policy should we use to rollout?\")\n    text(\"- For model-based value iteration, we had a purely random exploration policy (for phase 1).\")\n    text(\"- Let's do something a bit more sophisticated: **epsilon-greedy**.\")\n    text(\"- With probability epsilon, choose a random action according to the exploration policy.\")\n    text(\"- With probability 1 - epsilon, choose the best action according to the current estimated Q-values.\")\n\n    text(\"Now let's define Model-free Monte Carlo:\")\n    mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover\n    np.random.seed(1)\n\n    exploration_policy = partial(walk_tram_policy, mdp.num_locs)\n    rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)\n\n    try_out_model_free_monte_carlo(rl)\n\n    text(\"Let's run it for real now!\")\n    value = simulate(mdp, rl, num_trials=20)  # @inspect value @stepover\n    leaderboard = update_leaderboard(\"model_free_monte_carlo\", value)  # @inspect leaderboard @stepover\n\n    text(\"Summary:\")\n    text(\"- Model-free Monte Carlo: estimate Q-values of the current policy\")\n    text(\"- Directly uses rollouts, bypassing estimating the MDP\")\n    text(\"- Use epsilon-greedy policy to balance exploration and exploitation\")\n\n    text(\"Problem: in life, you only get one rollout.\")\n    text(\"Can we update the Q-values before the rollout is over?\")\n\n\ndef example_utility_calculation():\n    text(\"The utility at each step is the discounted sum of rewards from that point on.\")\n    rollout = [\n        Step(action=\"walk\", prob=1, reward=-1, state=2),\n        Step(action=\"tram\", prob=1, reward=-2, state=2),\n        Step(action=\"tram\", prob=1, reward=-2, state=4),\n    ]\n    discount = 1\n    utilities = [  # @inspect utilities\n        -1 + (discount * -2) + (discount**2 * -2),  # step 0 utility\n        -2 + (discount * -2),                       # step 1 utility\n        -2 + (discount * 0),                        # step 2 utility\n    ]\n\n    text(\"There is a nice recurrence relation between the utilities:\")\n    assert utilities[0] == rollout[0].reward + (discount * utilities[1])\n    assert utilities[1] == rollout[1].reward + (discount * utilities[2])\n\nclass ModelFreeMonteCarlo(RLAlgorithm):\n    def __init__(self, exploration_policy: Policy, epsilon: float, discount: float):\n        self.exploration_policy = exploration_policy\n        self.epsilon = epsilon\n        self.discount = discount\n\n        # Statistics that define the Q-values: Q(s, a) = sum_utilities[s][a] / counts[s][a]\n        self.sum_utilities = defaultdict(lambda : defaultdict(float))  # state -> action -> sum of utility from (state, action)\n        self.counts = defaultdict(lambda : defaultdict(int))  # state -> action -> visitation count\n\n        # Keep track of the current rollout\n        self.start_state = None\n        self.rollout: list[Step] = []\n\n    def get_action(self, state: Any) -> Any:\n        if len(self.counts[state]) == 0:\n            # If no actions have been tried yet, choose a random action\n            return self.exploration_policy(state)  # @stepover\n\n        # Do epsilon-greedy\n        if np.random.random() < self.epsilon:\n            # With probability epsilon, choose a random action according to the exploration policy\n            return self.exploration_policy(state)  # @stepover\n        else:\n            # Otherwise, choose the best action according to the Q-values\n            return self.pi(state)\n\n        return action\n\n    def pi(self, state: Any) -> Any:\n        \"\"\"Return the policy corresponding to the current Q-values.\"\"\"\n        actions = list(self.counts[state].keys())  # @inspect actions\n        q_values = [self.Q(state, action) for action in actions]  # @inspect q_values\n        action = actions[np.argmax(q_values).item()]  # @inspect action\n        return action\n\n\n    def Q(self, state: Any, action: Any) -> float:\n        \"\"\"Compute the estimated Q-values Q(state, action) using the running sums and counts.\"\"\"\n        sum_utility = self.sum_utilities[state][action]  # @inspect sum_utility\n        count = self.counts[state][action]  # @inspect count\n        value = sum_utility / count  # @inspect value\n        return value\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect state action reward next_state is_end\n        # Add this piece of feedback (state, action, reward, next_state) to the history\n        if self.start_state is None:\n            self.start_state = state\n        self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout\n\n        # At the end of the episode, update the statistics needed for computing Q-values\n        if is_end:\n            utilities = [0] * (len(self.rollout) + 1)  # @inspect utilities\n            # Walk backwards and compute the utilities for each step\n            for i, step in reversed(list(enumerate(self.rollout))):  # @inspect i step\n                # Compute utility of step i\n                # state [0] action reward state [1] action reward state [2] action reward state\n                state = self.start_state if i == 0 else self.rollout[i - 1].state  # @inspect state\n                utilities[i] = step.reward + self.discount * utilities[i + 1]  # @inspect utilities\n\n                # Update the running sums\n                self.sum_utilities[state][step.action] += utilities[i]  # @inspect self.sum_utilities\n                self.counts[state][step.action] += 1  # @inspect self.counts\n\n            # Reset history\n            self.start_state = None\n            self.rollout = []\n\n\ndef try_out_model_free_monte_carlo(rl: ModelFreeMonteCarlo):\n    action = rl.get_action(state=1)  # @inspect action\n\n    # Manually simulate some feedback\n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)\n\n    action = rl.get_action(state=1)  # @inspect action\n    action = rl.get_action(state=1)  # @inspect action @stepover\n    action = rl.get_action(state=1)  # @inspect action @stepover\n    action = rl.get_action(state=1)  # @inspect action @stepover\n\n\ndef introduce_sarsa():\n    text(\"Previously: model-free Monte Carlo: estimate Q-values directly from rollouts\")\n    text(\"SARSA: update Q-values as you rollout!\")\n\n    text(\"If we don't rollout completely, how do we get the utility (which requires going towards the end)?\")\n\n    text(\"Key insight: **bootstrapping**!\")\n    text(\"Combine the immediate reward with a model estimate of the future\")\n\n    text(\"Monte Carlo: u = r_0 + \u03b3*r_1 + \u03b3^2*r_2 + ... + \u03b3^n*r_n\")\n    text(\"Bootstrapping (SARSA): u = r_0 + \u03b3*Q_\u03c0(s_1, a_1)\")\n\n    text(\"Perform a gradient update to move Q_\u03c0(s, a) towards u\")\n\n    text(\"Instantiate our favorite flaky tram MDP:\")\n    mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover\n    np.random.seed(1)\n\n    exploration_policy = partial(walk_tram_policy, mdp.num_locs)\n    rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)\n\n    try_out_sarsa(rl)\n\n    text(\"Try it out for real!\")\n    value = simulate(mdp, rl, num_trials=20)  # @inspect value @stepover\n    leaderboard = update_leaderboard(\"sarsa\", value)  # @inspect leaderboard @stepover\n\n    text(\"Summary:\")\n    text(\"- SARSA: estimates Q-values of the current policy Q_\u03c0(s, a) as you rollout\")\n    text(\"- Bootstrapping: estimate utility using model estimate of the future\")\n    text(\"- Gradient update: move Q-values towards the estimated utility\")\n\n    text(\"But we are only computing Q-values of the current policy Q_\u03c0(s, a) (**on-policy**).\")\n    text(\"Can we directly estimate Q-values of the optimal policy Q`*`(s, a)?\")\n\n\nclass SARSA(RLAlgorithm):\n    def __init__(self, exploration_policy: Policy, epsilon: float, discount: float, learning_rate: float):\n        self.exploration_policy = exploration_policy\n        self.epsilon = epsilon\n        self.discount = discount\n        self.learning_rate = learning_rate\n        self.Q = defaultdict(lambda : defaultdict(float))  # state -> action -> Q-value\n\n    def get_action(self, state: Any) -> Any:\n        if len(self.Q[state]) == 0:\n            return self.exploration_policy(state) # @stepover\n        \n        if np.random.random() < self.epsilon:\n            return self.exploration_policy(state) # @stepover\n        else:\n            return self.pi(state) # @stepover\n\n    def pi(self, state: Any) -> Any:\n        \"\"\"Return the policy corresponding to the current Q-values.\"\"\"\n        actions = list(self.Q[state].keys())  # @inspect actions\n        if len(actions) == 0:\n            return None\n        q_values = [self.Q[state][action] for action in actions]  # @inspect q_values\n        action = actions[np.argmax(q_values).item()]  # @inspect action\n        return action\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end\n        # state \u2192 action reward next_state \u2192 next_action ...\n        # Important: use `self.get_action` (not `self.pi`) to get on-policy\n        next_action = self.get_action(next_state)  # @inspect next_action\n        utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility\n        self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q\n\n\ndef try_out_sarsa(rl: SARSA):\n    action = rl.get_action(state=1)  # @inspect action\n    \n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)\n\n    action = rl.get_action(state=1)  # @inspect action\n\n\ndef introduce_q_learning():\n    text(\"SARSA: estimate Q-values of the current policy Q_\u03c0(s, a)\")\n    text(\"Q-learning: estimate Q-values of the optimal policy Q`*`(s, a)\")\n\n    text(\"But we don't know the optimal policy...\")\n    \n    mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover\n    np.random.seed(1)\n\n    exploration_policy = partial(walk_tram_policy, mdp.num_locs)\n    rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)\n\n    try_out_q_learning(rl)\n\n    text(\"Try it out for real!\")\n    value = simulate(mdp, rl, num_trials=20)  # @inspect value @stepover\n    leaderboard = update_leaderboard(\"q_learning\", value)  # @inspect leaderboard @stepover\n\n    text(\"Summary:\")\n    text(\"- Q-learning: estimates Q-values of the optimal policy Q`*`(s, a) (**off-policy**)\")\n    text(\"- Like SARSA, uses bootstrapping and gradient updates\")\n\n\nclass QLearning(SARSA):\n    \"\"\"Q-learning is SARSA, but with an off-policy exploration policy.\"\"\"\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end\n        # state \u2192 action reward next_state \u2192 next_action ...\n        # Important: use `self.pi` (not `self.get_action`) to get off-policy\n        next_action = self.pi(next_state)  # @inspect next_action\n        utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility\n        self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q\n\n\ndef try_out_q_learning(rl: QLearning):\n    action = rl.get_action(state=1)  # @inspect action\n    \n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)\n\n    action = rl.get_action(state=1)  # @inspect action\n\n\nif __name__ == \"__main__\":\n    main()\n"
  },
  "hidden_line_numbers": {
    "reinforcement_learning.py": []
  },
  "steps": [
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 18,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 19,
          "function_name": "main",
          "code": "text(\"Last time: **Markov decision processes**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last time: **Markov decision processes**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 40,
          "function_name": "review_mdp",
          "code": "def review_mdp():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 41,
          "function_name": "review_mdp",
          "code": "text(\"MDP: start state, successors (action, probability, reward, next state), is_end, discount\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "MDP: start state, successors (action, probability, reward, next state), is_end, discount",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 44,
          "function_name": "review_mdp",
          "code": "mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 45,
          "function_name": "review_mdp",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 46,
          "function_name": "review_mdp",
          "code": "successors = mdp.successors(state)  # @inspect successors"
        }
      ],
      "env": {
        "successors": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 47,
          "function_name": "review_mdp",
          "code": "is_end = mdp.is_end(successors[0].state)  # @inspect is_end"
        }
      ],
      "env": {
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 48,
          "function_name": "review_mdp",
          "code": "image(draw_graph(mdp).render(\"var/flaky_tram_graph\", format=\"png\"), width=400)  # @stepover"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/flaky_tram_graph.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 51,
          "function_name": "review_mdp",
          "code": "text(\"Policy: maps state to action\") # @clear successors is_end"
        }
      ],
      "env": {
        "successors": null,
        "is_end": null
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "Policy: maps state to action",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 52,
          "function_name": "review_mdp",
          "code": "policy = partial(tram_if_possible_policy, mdp)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 53,
          "function_name": "review_mdp",
          "code": "action = policy(state)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 56,
          "function_name": "review_mdp",
          "code": "rollout = generate_rollout(mdp, policy)  # @inspect rollout @clear action"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "action": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 57,
          "function_name": "review_mdp",
          "code": "text(\"Value of policy: expected utility of policy\") # @clear rollout"
        }
      ],
      "env": {
        "rollout": null
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "Value of policy: expected utility of policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 58,
          "function_name": "review_mdp",
          "code": "text(\"Policy evaluation: computes value of a given policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Policy evaluation: computes value of a given policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 59,
          "function_name": "review_mdp",
          "code": "result = policy_evaluation(mdp, policy)  # @inspect result"
        }
      ],
      "env": {
        "result": {
          "type": "mdp.PolicyEvaluationResult",
          "contents": {
            "values": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "float",
                  "contents": -12.00000786576193,
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "float",
                  "contents": -8.666667091449938,
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "float",
                  "contents": -7.333333402499241,
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "float",
                  "contents": -5.333333344374658,
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "float",
                  "contents": -3.333333335074725,
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "int",
                  "contents": -4,
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "int",
                  "contents": -3,
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "int",
                  "contents": 0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "distances": {
              "type": "list",
              "contents": [
                {
                  "type": "int",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "int",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "int",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "int",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 58.974399999999996,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 24.818560000000005,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 20.031999999999996,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 13.410304000000004,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 8.065392640000002,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 4.523573248000002,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 2.4151588864000004,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 1.2430606336000007,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.6219051827200008,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.30418728386559835,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.1460661334835205,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.06907139561881515,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.032241916837886464,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.014884262851380115,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.006805564659073227,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.0030857140998499233,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.0013887787059552181,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.000620947817395745,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.00027600930440918603,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 0.00012203805012056534,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 5.370185434472319e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 2.3528556635454834e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 1.0267813085107491e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "float",
                  "contents": 4.464587195940339e-06,
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 62,
          "function_name": "review_mdp",
          "code": "text(\"Value iteration: computes value of the optimal policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Value iteration: computes value of the optimal policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 63,
          "function_name": "review_mdp",
          "code": "result = value_iteration(mdp)  # @inspect result"
        }
      ],
      "env": {
        "result": {
          "type": "mdp.ValueIterationResult",
          "contents": {
            "values": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "numpy.float64",
                  "contents": -7.333349940540211,
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "numpy.float64",
                  "contents": -6.3333399762160845,
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "numpy.float64",
                  "contents": -5.333335990486434,
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "numpy.float64",
                  "contents": -4.333334396194574,
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "numpy.float64",
                  "contents": -3.3333337584778295,
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "numpy.int64",
                  "contents": -4,
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "numpy.int64",
                  "contents": -3,
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "numpy.int64",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "numpy.int64",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "int",
                  "contents": 0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "pi": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "NoneType",
                  "contents": "None",
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "distances": {
              "type": "list",
              "contents": [
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 27.28,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 19.520000000000003,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 9.280000000000003,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 3.7120000000000015,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 1.4848,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.5939199999999998,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.23756800000000045,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.09502719999999965,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.03801087999999986,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.015204352000000476,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.006081740800000013,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.00243269631999965,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.0009730785280002152,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.0003892314111997308,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.00015569256447989233,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 6.22770257923122e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 2.4910810316747245e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 9.964324126698898e-06,
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 70,
          "function_name": "review_recurrences",
          "code": "def review_recurrences():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 71,
          "function_name": "review_recurrences",
          "code": "text(\"Policy evaluation recurrence:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Policy evaluation recurrence:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 72,
          "function_name": "review_recurrences",
          "code": "image(\"images/policy_evaluation_recurrence.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/policy_evaluation_recurrence.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 74,
          "function_name": "review_recurrences",
          "code": "text(\"Value V_\u03c0(s): value of following policy \u03c0 from state s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Value V_\u03c0(s): value of following policy \u03c0 from state s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 75,
          "function_name": "review_recurrences",
          "code": "text(\"V_\u03c0(s) = Q_\u03c0(s, \u03c0(s))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "V_\u03c0(s) = Q_\u03c0(s, \u03c0(s))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 77,
          "function_name": "review_recurrences",
          "code": "text(\"Q-value Q_\u03c0(s, a): value of taking action a in state s, and then following policy \u03c0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Q-value Q_\u03c0(s, a): value of taking action a in state s, and then following policy \u03c0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 78,
          "function_name": "review_recurrences",
          "code": "text(\"Q_\u03c0(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V_\u03c0(s'))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Q_\u03c0(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V_\u03c0(s'))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 80,
          "function_name": "review_recurrences",
          "code": "text(\"Value iteration recurrence:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Value iteration recurrence:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 81,
          "function_name": "review_recurrences",
          "code": "image(\"images/value_iteration_recurrence.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/value_iteration_recurrence.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 83,
          "function_name": "review_recurrences",
          "code": "text(\"Optimal V-value V`*`(s): value of following the optimal policy from state s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Optimal V-value V`*`(s): value of following the optimal policy from state s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 84,
          "function_name": "review_recurrences",
          "code": "text(\"V`*`(s) = max_a Q`*`(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "V`*`(s) = max_a Q`*`(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 86,
          "function_name": "review_recurrences",
          "code": "text(\"Optimal Q-value Q`*`(s, a): value of taking action a in state s, and then following the optimal policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Optimal Q-value Q`*`(s, a): value of taking action a in state s, and then following the optimal policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 87,
          "function_name": "review_recurrences",
          "code": "text(\"Q`*`(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V`*`(s'))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Q`*`(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V`*`(s'))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 89,
          "function_name": "review_recurrences",
          "code": "text(\"Optimal policy: take the action with the highest Q-value\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Optimal policy: take the action with the highest Q-value",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 90,
          "function_name": "review_recurrences",
          "code": "text(\"\u03c0`*`(s) = argmax_a Q`*`(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u03c0`*`(s) = argmax_a Q`*`(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 65,
          "function_name": "review_mdp",
          "code": "review_recurrences()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 67,
          "function_name": "review_mdp",
          "code": "text(\"But can we find the optimal policy if we don't know the MDP?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But can we find the optimal policy if we don't know the MDP?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 20,
          "function_name": "main",
          "code": "review_mdp()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 22,
          "function_name": "main",
          "code": "text(\"This time: **reinforcement learning**\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This time: **reinforcement learning**",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 93,
          "function_name": "introduce_rl",
          "code": "def introduce_rl():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 94,
          "function_name": "introduce_rl",
          "code": "text(\"Reinforcement learning setting:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reinforcement learning setting:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 95,
          "function_name": "introduce_rl",
          "code": "image(\"images/rl-framework.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/rl-framework.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 96,
          "function_name": "introduce_rl",
          "code": "text(\"Repeat:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Repeat:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 97,
          "function_name": "introduce_rl",
          "code": "text(\"- **Agent** produces action\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Agent** produces action",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 98,
          "function_name": "introduce_rl",
          "code": "text(\"- **Environment** produces reward and observation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- **Environment** produces reward and observation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 100,
          "function_name": "introduce_rl",
          "code": "text(\"Intuitively:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuitively:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 101,
          "function_name": "introduce_rl",
          "code": "text(\"- A good agent should try various actions to find ones that lead to good rewards.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- A good agent should try various actions to find ones that lead to good rewards.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 102,
          "function_name": "introduce_rl",
          "code": "text(\"- Then it should *learn* to keep doing those actions (those actions are *reinforced*).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Then it should *learn* to keep doing those actions (those actions are *reinforced*).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 104,
          "function_name": "introduce_rl",
          "code": "text(\"Reinforcement learning really is the metaphor for life.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reinforcement learning really is the metaphor for life.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 105,
          "function_name": "introduce_rl",
          "code": "text(\"In MDPs, we don't know what outcomes will be, but at least know their probabilities...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In MDPs, we don't know what outcomes will be, but at least know their probabilities...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 106,
          "function_name": "introduce_rl",
          "code": "text(\"in RL, we don't even know what the probabilities will be - that's real life!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "in RL, we don't even know what the probabilities will be - that's real life!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 108,
          "function_name": "introduce_rl",
          "code": "text(\"In this lecture, assume the environment is backed by an MDP and the observation is the next state.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In this lecture, assume the environment is backed by an MDP and the observation is the next state.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 109,
          "function_name": "introduce_rl",
          "code": "text(\"(In real life, you only observe part of the state (partially observable MDPs.)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "(In real life, you only observe part of the state (partially observable MDPs.)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 111,
          "function_name": "introduce_rl",
          "code": "text(\"Let us define an environment (MDP):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let us define an environment (MDP):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 112,
          "function_name": "introduce_rl",
          "code": "mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 113,
          "function_name": "introduce_rl",
          "code": "np.random.seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 114,
          "function_name": "introduce_rl",
          "code": "text(\"...and an agent (RL algorithm):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...and an agent (RL algorithm):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 115,
          "function_name": "introduce_rl",
          "code": "policy = partial(tram_if_possible_policy, mdp)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 116,
          "function_name": "introduce_rl",
          "code": "rl = StaticAgent(policy=policy)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 116,
          "function_name": "introduce_rl",
          "code": "rl = StaticAgent(policy=policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 148,
          "function_name": "__init__",
          "code": "def __init__(self, policy: Policy):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 116,
          "function_name": "introduce_rl",
          "code": "rl = StaticAgent(policy=policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 149,
          "function_name": "__init__",
          "code": "self.policy = policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 116,
          "function_name": "introduce_rl",
          "code": "rl = StaticAgent(policy=policy)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 117,
          "function_name": "introduce_rl",
          "code": "rl.get_action(state=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 117,
          "function_name": "introduce_rl",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 151,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 117,
          "function_name": "introduce_rl",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 152,
          "function_name": "get_action",
          "code": "return self.policy(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 117,
          "function_name": "introduce_rl",
          "code": "rl.get_action(state=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 118,
          "function_name": "introduce_rl",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 118,
          "function_name": "introduce_rl",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 154,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 118,
          "function_name": "introduce_rl",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 156,
          "function_name": "incorporate_feedback",
          "code": "pass"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 118,
          "function_name": "introduce_rl",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 120,
          "function_name": "introduce_rl",
          "code": "text(\"Now let's simulate the agent and the environment (generating rollouts).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now let's simulate the agent and the environment (generating rollouts).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 159,
          "function_name": "simulate",
          "code": "def simulate(mdp: MDP, rl: RLAlgorithm, num_trials: int) -> float:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 164,
          "function_name": "simulate",
          "code": "utilities = []  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -20,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -20,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -20,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -20,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.4,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.4,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -20,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 169,
          "function_name": "simulate",
          "code": "state = mdp.start_state()  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 171,
          "function_name": "simulate",
          "code": "steps = []  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "float",
              "contents": 0.6,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 8,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 174,
          "function_name": "simulate",
          "code": "action = rl.get_action(state)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 177,
          "function_name": "simulate",
          "code": "step = sample_transition(mdp, state, action)  # @inspect step @stepover"
        }
      ],
      "env": {
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 10,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 178,
          "function_name": "simulate",
          "code": "rl.incorporate_feedback(state, action, step.reward, step.state, mdp.is_end(step.state)) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 179,
          "function_name": "simulate",
          "code": "steps.append(step)  # @inspect steps"
        }
      ],
      "env": {
        "steps": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 8,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 9,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 180,
          "function_name": "simulate",
          "code": "state = step.state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 10,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 172,
          "function_name": "simulate",
          "code": "while not mdp.is_end(state):  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 183,
          "function_name": "simulate",
          "code": "rollout = Rollout(steps=steps, discount=mdp.discount())  # @inspect rollout @clear steps state action step"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 4,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "float",
                      "contents": 0.6,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 8,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 9,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 10,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "steps": null,
        "state": null,
        "action": null,
        "step": null
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 184,
          "function_name": "simulate",
          "code": "utilities.append(rollout.utility)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -14,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -18,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -12,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -20,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -10,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -8,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 167,
          "function_name": "simulate",
          "code": "for trial in range(num_trials):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 186,
          "function_name": "simulate",
          "code": "mean_utility = np.mean(utilities).item()  # @inspect mean_utility"
        }
      ],
      "env": {
        "mean_utility": {
          "type": "float",
          "contents": -11.6,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 187,
          "function_name": "simulate",
          "code": "return mean_utility"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 121,
          "function_name": "introduce_rl",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -11.6,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 122,
          "function_name": "introduce_rl",
          "code": "leaderboard = update_leaderboard(\"tram_if_possible_policy\", value)  # @inspect leaderboard"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 122,
          "function_name": "introduce_rl",
          "code": "leaderboard = update_leaderboard(\"tram_if_possible_policy\", value)  # @inspect leaderboard"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 13,
          "function_name": "update_leaderboard",
          "code": "def update_leaderboard(method: str, value: float) -> dict[str, float]:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 122,
          "function_name": "introduce_rl",
          "code": "leaderboard = update_leaderboard(\"tram_if_possible_policy\", value)  # @inspect leaderboard"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 14,
          "function_name": "update_leaderboard",
          "code": "LEADERBOARD[method] = value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 122,
          "function_name": "introduce_rl",
          "code": "leaderboard = update_leaderboard(\"tram_if_possible_policy\", value)  # @inspect leaderboard"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 15,
          "function_name": "update_leaderboard",
          "code": "return LEADERBOARD"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 122,
          "function_name": "introduce_rl",
          "code": "leaderboard = update_leaderboard(\"tram_if_possible_policy\", value)  # @inspect leaderboard"
        }
      ],
      "env": {
        "leaderboard": {
          "type": "dict",
          "contents": {
            "tram_if_possible_policy": {
              "type": "float",
              "contents": -11.6,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 123,
          "function_name": "introduce_rl",
          "code": "text(\"Simulation yields some estimated value (expected utility).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Simulation yields some estimated value (expected utility).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 124,
          "function_name": "introduce_rl",
          "code": "text(\"The agent (RL algorithm) doesn't do anything with the feedback.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The agent (RL algorithm) doesn't do anything with the feedback.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 126,
          "function_name": "introduce_rl",
          "code": "text(\"Difference between policy and agent:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Difference between policy and agent:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 127,
          "function_name": "introduce_rl",
          "code": "text(\"- Policy: maps state to action, doesn't change over time (static)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Policy: maps state to action, doesn't change over time (static)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 128,
          "function_name": "introduce_rl",
          "code": "text(\"- Agent (RL algorithm): maps state to action, can change over time (dynamic)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Agent (RL algorithm): maps state to action, can change over time (dynamic)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 130,
          "function_name": "introduce_rl",
          "code": "text(\"Intuition: RL algorithm uses the feedback to improve its internal policy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition: RL algorithm uses the feedback to improve its internal policy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 131,
          "function_name": "introduce_rl",
          "code": "text(\"How should we perform this update?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How should we perform this update?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 23,
          "function_name": "main",
          "code": "introduce_rl()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 220,
          "function_name": "introduce_model_based",
          "code": "def introduce_model_based():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 221,
          "function_name": "introduce_model_based",
          "code": "text(\"What makes RL hard:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What makes RL hard:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 222,
          "function_name": "introduce_model_based",
          "code": "text(\"- We don't know the MDP.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- We don't know the MDP.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 223,
          "function_name": "introduce_model_based",
          "code": "text(\"- Otherwise, we can use value iteration to compute the optimal policy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Otherwise, we can use value iteration to compute the optimal policy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 224,
          "function_name": "introduce_model_based",
          "code": "text(\"Idea: estimate (learn) the MDP from feedback!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Idea: estimate (learn) the MDP from feedback!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 226,
          "function_name": "introduce_model_based",
          "code": "text(\"**Model-based value iteration**:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Model-based value iteration**:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 227,
          "function_name": "introduce_model_based",
          "code": "text(\"1. Exploration: estimate the MDP using an exploration policy (random).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Exploration: estimate the MDP using an exploration policy (random).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 228,
          "function_name": "introduce_model_based",
          "code": "text(\"2. Compute the optimal policy of the estimated MDP.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Compute the optimal policy of the estimated MDP.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 229,
          "function_name": "introduce_model_based",
          "code": "text(\"3. Exploitation: follow this policy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Exploitation: follow this policy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 231,
          "function_name": "introduce_model_based",
          "code": "text(\"Let's define our familiar flaky tram MDP:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's define our familiar flaky tram MDP:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 232,
          "function_name": "introduce_model_based",
          "code": "mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 233,
          "function_name": "introduce_model_based",
          "code": "np.random.seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 235,
          "function_name": "introduce_model_based",
          "code": "text(\"Define an exploration policy that chooses a random valid action:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define an exploration policy that chooses a random valid action:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 236,
          "function_name": "introduce_model_based",
          "code": "exploration_policy = partial(walk_tram_policy, mdp.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 358,
          "function_name": "try_out_exploration_policy",
          "code": "def try_out_exploration_policy(exploration_policy: Policy):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 359,
          "function_name": "try_out_exploration_policy",
          "code": "text(\"The exploration policy tries all valid actions.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The exploration policy tries all valid actions.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 360,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 360,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 210,
          "function_name": "walk_tram_policy",
          "code": "def walk_tram_policy(num_locs: int, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 360,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 212,
          "function_name": "walk_tram_policy",
          "code": "if 2 * state <= num_locs:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 360,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 214,
          "function_name": "walk_tram_policy",
          "code": "return np.random.choice([\"walk\", \"tram\"]).item()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 360,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 361,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 362,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=1)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 363,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=6)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 363,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=6)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 210,
          "function_name": "walk_tram_policy",
          "code": "def walk_tram_policy(num_locs: int, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 363,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=6)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 212,
          "function_name": "walk_tram_policy",
          "code": "if 2 * state <= num_locs:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 363,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=6)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 217,
          "function_name": "walk_tram_policy",
          "code": "return \"walk\""
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 363,
          "function_name": "try_out_exploration_policy",
          "code": "action = exploration_policy(state=6)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 237,
          "function_name": "introduce_model_based",
          "code": "try_out_exploration_policy(exploration_policy)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 277,
          "function_name": "__init__",
          "code": "def __init__(self, exploration_policy: Policy, discount: float):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 278,
          "function_name": "__init__",
          "code": "self.exploration_policy = exploration_policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 279,
          "function_name": "__init__",
          "code": "self.exploitation_policy = None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 302,
          "function_name": "__init__",
          "code": "def __init__(self, discount: float):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 303,
          "function_name": "__init__",
          "code": "self.start_state_ = None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 304,
          "function_name": "__init__",
          "code": "self.rewards = defaultdict(float)  # (state, action, state') -> reward"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 305,
          "function_name": "__init__",
          "code": "self.transitions = defaultdict(lambda : defaultdict(lambda : defaultdict(int)))  # state -> action -> state' -> count"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 306,
          "function_name": "__init__",
          "code": "self.end_states = set()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 307,
          "function_name": "__init__",
          "code": "self.discount_ = discount"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 280,
          "function_name": "__init__",
          "code": "self.mdp = EstimatedMDP(discount=discount)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 240,
          "function_name": "introduce_model_based",
          "code": "rl = ModelBasedValueIteration(exploration_policy=exploration_policy, discount=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 354,
          "function_name": "try_out_model_based_value_iteration",
          "code": "def try_out_model_based_value_iteration(rl: ModelBasedValueIteration):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 288,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 290,
          "function_name": "get_action",
          "code": "if self.exploitation_policy is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 291,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 291,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 210,
          "function_name": "walk_tram_policy",
          "code": "def walk_tram_policy(num_locs: int, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 291,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 212,
          "function_name": "walk_tram_policy",
          "code": "if 2 * state <= num_locs:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 291,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 214,
          "function_name": "walk_tram_policy",
          "code": "return np.random.choice([\"walk\", \"tram\"]).item()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 291,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 355,
          "function_name": "try_out_model_based_value_iteration",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 295,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 334,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 336,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state_ is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 337,
          "function_name": "incorporate_feedback",
          "code": "self.start_state_ = state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 339,
          "function_name": "incorporate_feedback",
          "code": "self.rewards[(state, action, next_state)] = reward"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 340,
          "function_name": "incorporate_feedback",
          "code": "self.transitions[state][action][next_state] += 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 342,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 297,
          "function_name": "incorporate_feedback",
          "code": "self.mdp.incorporate_feedback(state, action, reward, next_state, is_end)  # @inspect self.mdp"
        }
      ],
      "env": {
        "self.mdp": {
          "type": "reinforcement_learning.EstimatedMDP",
          "contents": {
            "start_state_": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "rewards": {
              "type": "collections.defaultdict",
              "contents": {
                "(1, 'walk', 2)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "transition_counts": {
              "type": "collections.defaultdict",
              "contents": {
                "1": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "2": {
                          "type": "int",
                          "contents": 1,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "end_states": {
              "type": "list",
              "contents": [],
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 356,
          "function_name": "try_out_model_based_value_iteration",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 241,
          "function_name": "introduce_model_based",
          "code": "try_out_model_based_value_iteration(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 243,
          "function_name": "introduce_model_based",
          "code": "text(\"Stage 1: explore using the exploration policy to estimate the MDP.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Stage 1: explore using the exploration policy to estimate the MDP.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 244,
          "function_name": "introduce_model_based",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -9.0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 245,
          "function_name": "introduce_model_based",
          "code": "leaderboard = update_leaderboard(\"model_based_value_iteration.explore\", value)  # @inspect leaderboard @stepover"
        }
      ],
      "env": {
        "leaderboard": {
          "type": "dict",
          "contents": {
            "tram_if_possible_policy": {
              "type": "float",
              "contents": -11.6,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.explore": {
              "type": "float",
              "contents": -9.0,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 366,
          "function_name": "compare_mdps",
          "code": "def compare_mdps(true_mdp: MDP, estimated_mdp: MDP):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 367,
          "function_name": "compare_mdps",
          "code": "text(\"Let's look at the internals of the estimated MDP.\")  # @inspect estimated_mdp"
        }
      ],
      "env": {
        "estimated_mdp": {
          "type": "reinforcement_learning.EstimatedMDP",
          "contents": {
            "start_state_": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "rewards": {
              "type": "collections.defaultdict",
              "contents": {
                "(1, 'walk', 2)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(1, 'tram', 2)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(2, 'tram', 4)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(4, 'walk', 5)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(5, 'tram', 10)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(2, 'walk', 3)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(3, 'walk', 4)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(5, 'tram', 5)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(4, 'tram', 8)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(8, 'walk', 9)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(9, 'walk', 10)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(1, 'tram', 1)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(3, 'tram', 3)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(4, 'tram', 4)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(3, 'tram', 6)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(6, 'walk', 7)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(7, 'walk', 8)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "(2, 'tram', 2)": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "(5, 'walk', 6)": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "transition_counts": {
              "type": "collections.defaultdict",
              "contents": {
                "1": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "2": {
                          "type": "int",
                          "contents": 9,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    },
                    "tram": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "2": {
                          "type": "int",
                          "contents": 2,
                          "dtype": null,
                          "shape": null
                        },
                        "1": {
                          "type": "int",
                          "contents": 1,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "tram": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "4": {
                          "type": "int",
                          "contents": 5,
                          "dtype": null,
                          "shape": null
                        },
                        "2": {
                          "type": "int",
                          "contents": 2,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    },
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "3": {
                          "type": "int",
                          "contents": 5,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "5": {
                          "type": "int",
                          "contents": 4,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    },
                    "tram": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "8": {
                          "type": "int",
                          "contents": 5,
                          "dtype": null,
                          "shape": null
                        },
                        "4": {
                          "type": "int",
                          "contents": 3,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "tram": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "10": {
                          "type": "int",
                          "contents": 3,
                          "dtype": null,
                          "shape": null
                        },
                        "5": {
                          "type": "int",
                          "contents": 1,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    },
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "6": {
                          "type": "int",
                          "contents": 1,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "4": {
                          "type": "int",
                          "contents": 4,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    },
                    "tram": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "3": {
                          "type": "int",
                          "contents": 2,
                          "dtype": null,
                          "shape": null
                        },
                        "6": {
                          "type": "int",
                          "contents": 1,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "9": {
                          "type": "int",
                          "contents": 7,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "10": {
                          "type": "int",
                          "contents": 7,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "7": {
                          "type": "int",
                          "contents": 2,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "collections.defaultdict",
                  "contents": {
                    "walk": {
                      "type": "collections.defaultdict",
                      "contents": {
                        "8": {
                          "type": "int",
                          "contents": 2,
                          "dtype": null,
                          "shape": null
                        }
                      },
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "end_states": {
              "type": "list",
              "contents": [
                {
                  "type": "int",
                  "contents": 10,
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's look at the internals of the estimated MDP.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 369,
          "function_name": "compare_mdps",
          "code": "text(\"Let us compare the true MDP and the estimated MDP.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let us compare the true MDP and the estimated MDP.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 370,
          "function_name": "compare_mdps",
          "code": "true_start_state = true_mdp.start_state()  # @inspect true_start_state"
        }
      ],
      "env": {
        "true_start_state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 371,
          "function_name": "compare_mdps",
          "code": "estimated_start_state = estimated_mdp.start_state()  # @inspect estimated_start_state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 371,
          "function_name": "compare_mdps",
          "code": "estimated_start_state = estimated_mdp.start_state()  # @inspect estimated_start_state"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 309,
          "function_name": "start_state",
          "code": "def start_state(self) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 371,
          "function_name": "compare_mdps",
          "code": "estimated_start_state = estimated_mdp.start_state()  # @inspect estimated_start_state"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 310,
          "function_name": "start_state",
          "code": "return self.start_state_"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 371,
          "function_name": "compare_mdps",
          "code": "estimated_start_state = estimated_mdp.start_state()  # @inspect estimated_start_state"
        }
      ],
      "env": {
        "estimated_start_state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 373,
          "function_name": "compare_mdps",
          "code": "true_successors = true_mdp.successors(state=1)  # @inspect true_successors"
        }
      ],
      "env": {
        "true_successors": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.4,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 312,
          "function_name": "successors",
          "code": "def successors(self, state: Any) -> list[Step]:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 314,
          "function_name": "successors",
          "code": "successors = []  # @inspect successors"
        }
      ],
      "env": {
        "successors": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 316,
          "function_name": "successors",
          "code": "for action, next_state_counts in self.transitions[state].items():  # @inspect action next_state_counts"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        },
        "next_state_counts": {
          "type": "collections.defaultdict",
          "contents": {
            "2": {
              "type": "int",
              "contents": 9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 317,
          "function_name": "successors",
          "code": "total_count = sum(next_state_counts.values())  # @inspect total_count"
        }
      ],
      "env": {
        "total_count": {
          "type": "int",
          "contents": 9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 319,
          "function_name": "successors",
          "code": "for next_state, count in next_state_counts.items():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 321,
          "function_name": "successors",
          "code": "prob = count / total_count"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 322,
          "function_name": "successors",
          "code": "reward = self.rewards[(state, action, next_state)]"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 323,
          "function_name": "successors",
          "code": "step = Step(action=action, prob=prob, reward=reward, state=next_state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 324,
          "function_name": "successors",
          "code": "successors.append(step)  # @inspect successors"
        }
      ],
      "env": {
        "successors": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 1.0,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 319,
          "function_name": "successors",
          "code": "for next_state, count in next_state_counts.items():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 316,
          "function_name": "successors",
          "code": "for action, next_state_counts in self.transitions[state].items():  # @inspect action next_state_counts"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "next_state_counts": {
          "type": "collections.defaultdict",
          "contents": {
            "2": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            },
            "1": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 317,
          "function_name": "successors",
          "code": "total_count = sum(next_state_counts.values())  # @inspect total_count"
        }
      ],
      "env": {
        "total_count": {
          "type": "int",
          "contents": 3,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 319,
          "function_name": "successors",
          "code": "for next_state, count in next_state_counts.items():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 321,
          "function_name": "successors",
          "code": "prob = count / total_count"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 322,
          "function_name": "successors",
          "code": "reward = self.rewards[(state, action, next_state)]"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 323,
          "function_name": "successors",
          "code": "step = Step(action=action, prob=prob, reward=reward, state=next_state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 324,
          "function_name": "successors",
          "code": "successors.append(step)  # @inspect successors"
        }
      ],
      "env": {
        "successors": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 1.0,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6666666666666666,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 319,
          "function_name": "successors",
          "code": "for next_state, count in next_state_counts.items():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 321,
          "function_name": "successors",
          "code": "prob = count / total_count"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 322,
          "function_name": "successors",
          "code": "reward = self.rewards[(state, action, next_state)]"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 323,
          "function_name": "successors",
          "code": "step = Step(action=action, prob=prob, reward=reward, state=next_state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 324,
          "function_name": "successors",
          "code": "successors.append(step)  # @inspect successors"
        }
      ],
      "env": {
        "successors": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 1.0,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6666666666666666,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.3333333333333333,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 319,
          "function_name": "successors",
          "code": "for next_state, count in next_state_counts.items():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 316,
          "function_name": "successors",
          "code": "for action, next_state_counts in self.transitions[state].items():  # @inspect action next_state_counts"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "next_state_counts": {
          "type": "collections.defaultdict",
          "contents": {
            "2": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            },
            "1": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 326,
          "function_name": "successors",
          "code": "return successors"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 374,
          "function_name": "compare_mdps",
          "code": "estimated_successors = estimated_mdp.successors(state=1)  # @inspect estimated_successors"
        }
      ],
      "env": {
        "estimated_successors": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 1.0,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.6666666666666666,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "float",
                  "contents": 0.3333333333333333,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 376,
          "function_name": "compare_mdps",
          "code": "true_is_end = true_mdp.is_end(state=9)  # @inspect true_is_end"
        }
      ],
      "env": {
        "true_is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 377,
          "function_name": "compare_mdps",
          "code": "estimated_is_end = estimated_mdp.is_end(state=9)  # @inspect estimated_is_end"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 377,
          "function_name": "compare_mdps",
          "code": "estimated_is_end = estimated_mdp.is_end(state=9)  # @inspect estimated_is_end"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 328,
          "function_name": "is_end",
          "code": "def is_end(self, state: Any) -> bool:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 377,
          "function_name": "compare_mdps",
          "code": "estimated_is_end = estimated_mdp.is_end(state=9)  # @inspect estimated_is_end"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 329,
          "function_name": "is_end",
          "code": "return state in self.end_states"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 377,
          "function_name": "compare_mdps",
          "code": "estimated_is_end = estimated_mdp.is_end(state=9)  # @inspect estimated_is_end"
        }
      ],
      "env": {
        "estimated_is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 379,
          "function_name": "compare_mdps",
          "code": "text(\"The more the agent explores, estimated MDP \u2192 true MDP\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The more the agent explores, estimated MDP \u2192 true MDP",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 380,
          "function_name": "compare_mdps",
          "code": "text(\"...assuming the exploration policy tries all valid actions.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...assuming the exploration policy tries all valid actions.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 246,
          "function_name": "introduce_model_based",
          "code": "compare_mdps(mdp, rl.mdp)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 248,
          "function_name": "introduce_model_based",
          "code": "text(\"Stage 2: compute the optimal policy of the estimated MDP and follow that.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Stage 2: compute the optimal policy of the estimated MDP and follow that.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 249,
          "function_name": "introduce_model_based",
          "code": "rl.run_value_iteration()  # @inspect rl.exploitation_policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 249,
          "function_name": "introduce_model_based",
          "code": "rl.run_value_iteration()  # @inspect rl.exploitation_policy"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 282,
          "function_name": "run_value_iteration",
          "code": "def run_value_iteration(self):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 249,
          "function_name": "introduce_model_based",
          "code": "rl.run_value_iteration()  # @inspect rl.exploitation_policy"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 284,
          "function_name": "run_value_iteration",
          "code": "result = value_iteration(self.mdp)  # @inspect result @stepover"
        }
      ],
      "env": {
        "result": {
          "type": "mdp.ValueIterationResult",
          "contents": {
            "values": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "numpy.float64",
                  "contents": -6.666672468185425,
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "numpy.float64",
                  "contents": -5.666668117046356,
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "numpy.float64",
                  "contents": -4.666667029261589,
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "numpy.float64",
                  "contents": -3.6666667573153973,
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "numpy.float64",
                  "contents": -2.6666666893288493,
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "numpy.float64",
                  "contents": -4.0,
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "numpy.float64",
                  "contents": -3.0,
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "numpy.float64",
                  "contents": -2.0,
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "numpy.float64",
                  "contents": -1.0,
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "int",
                  "contents": 0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "pi": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "NoneType",
                  "contents": "None",
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "distances": {
              "type": "list",
              "contents": [
                {
                  "type": "numpy.float64",
                  "contents": 99.0,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 99.0,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 99.0,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 99.0,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 27.566326530612244,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 11.826530612244895,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 4.5625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 1.140625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.28515625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.0712890625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.017822265625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.00445556640625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.0011138916015625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.000278472900390625,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 6.961822509765625e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 1.7404556274414062e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 4.351139068603516e-06,
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 249,
          "function_name": "introduce_model_based",
          "code": "rl.run_value_iteration()  # @inspect rl.exploitation_policy"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 286,
          "function_name": "run_value_iteration",
          "code": "self.exploitation_policy = result.pi"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 249,
          "function_name": "introduce_model_based",
          "code": "rl.run_value_iteration()  # @inspect rl.exploitation_policy"
        }
      ],
      "env": {
        "rl.exploitation_policy": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "3": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "5": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "6": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "7": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "8": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "9": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "10": {
              "type": "NoneType",
              "contents": "None",
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 383,
          "function_name": "compare_policies",
          "code": "def compare_policies(optimal_policy: dict[Any, Any], estimated_policy: dict[Any, Any]):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 384,
          "function_name": "compare_policies",
          "code": "text(\"We compare:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We compare:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 385,
          "function_name": "compare_policies",
          "code": "text(\"- Optimal policy: the optimal policy for the true MDP (unknown to the agent)\")  # @inspect optimal_policy"
        }
      ],
      "env": {
        "optimal_policy": {
          "type": "mdp.ValueIterationResult",
          "contents": {
            "values": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "numpy.float64",
                  "contents": -7.333349940540211,
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "numpy.float64",
                  "contents": -6.3333399762160845,
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "numpy.float64",
                  "contents": -5.333335990486434,
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "numpy.float64",
                  "contents": -4.333334396194574,
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "numpy.float64",
                  "contents": -3.3333337584778295,
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "numpy.int64",
                  "contents": -4,
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "numpy.int64",
                  "contents": -3,
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "numpy.int64",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "numpy.int64",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "int",
                  "contents": 0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "pi": {
              "type": "dict",
              "contents": {
                "1": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "2": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "3": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "4": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "5": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "6": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "7": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "8": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "9": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "10": {
                  "type": "NoneType",
                  "contents": "None",
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "distances": {
              "type": "list",
              "contents": [
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.int64",
                  "contents": 99,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 27.28,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 19.520000000000003,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 9.280000000000003,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 3.7120000000000015,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 1.4848,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.5939199999999998,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.23756800000000045,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.09502719999999965,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.03801087999999986,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.015204352000000476,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.006081740800000013,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.00243269631999965,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.0009730785280002152,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.0003892314111997308,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 0.00015569256447989233,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 6.22770257923122e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 2.4910810316747245e-05,
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "numpy.float64",
                  "contents": 9.964324126698898e-06,
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "- Optimal policy: the optimal policy for the true MDP (unknown to the agent)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 386,
          "function_name": "compare_policies",
          "code": "text(\"- Estimated policy: the optimal policy for the estimated MDP (known to the agent)\")  # @inspect estimated_policy"
        }
      ],
      "env": {
        "estimated_policy": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "3": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "5": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "6": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "7": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "8": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "9": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "10": {
              "type": "NoneType",
              "contents": "None",
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "- Estimated policy: the optimal policy for the estimated MDP (known to the agent)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 388,
          "function_name": "compare_policies",
          "code": "text(\"Note the two are similar but not quite the same.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Note the two are similar but not quite the same.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 389,
          "function_name": "compare_policies",
          "code": "text(\"The more the agent explores, estimated policy \u2192 optimal policy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The more the agent explores, estimated policy \u2192 optimal policy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 250,
          "function_name": "introduce_model_based",
          "code": "compare_policies(value_iteration(mdp), rl.exploitation_policy)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 252,
          "function_name": "introduce_model_based",
          "code": "text(\"Stage 3: run using this estimated policy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Stage 3: run using this estimated policy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 253,
          "function_name": "introduce_model_based",
          "code": "value = simulate(mdp, rl, num_trials=10)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -6.6,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 254,
          "function_name": "introduce_model_based",
          "code": "leaderboard = update_leaderboard(\"model_based_value_iteration.exploit\", value)  # @inspect leaderboard @stepover"
        }
      ],
      "env": {
        "leaderboard": {
          "type": "dict",
          "contents": {
            "tram_if_possible_policy": {
              "type": "float",
              "contents": -11.6,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.explore": {
              "type": "float",
              "contents": -9.0,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.exploit": {
              "type": "float",
              "contents": -6.6,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 256,
          "function_name": "introduce_model_based",
          "code": "text(\"Notes:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Notes:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 257,
          "function_name": "introduce_model_based",
          "code": "text(\"- The utility of the exploration phase is suboptimal, but we're learning!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- The utility of the exploration phase is suboptimal, but we're learning!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 258,
          "function_name": "introduce_model_based",
          "code": "text(\"- In practice, we don't need to restrict to two phases\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- In practice, we don't need to restrict to two phases",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 259,
          "function_name": "introduce_model_based",
          "code": "text(\"- Always continue refining the estimated MDP\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Always continue refining the estimated MDP",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 260,
          "function_name": "introduce_model_based",
          "code": "text(\"- Gradually move the policy from full exploration to full exploitation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Gradually move the policy from full exploration to full exploitation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 262,
          "function_name": "introduce_model_based",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 263,
          "function_name": "introduce_model_based",
          "code": "text(\"- Model-based RL: estimate the MDP from feedback (explore)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-based RL: estimate the MDP from feedback (explore)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 264,
          "function_name": "introduce_model_based",
          "code": "text(\"- Once have estimated MDP, use value iteration to compute the optimal policy (of estimated MDP)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Once have estimated MDP, use value iteration to compute the optimal policy (of estimated MDP)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 265,
          "function_name": "introduce_model_based",
          "code": "text(\"- Once have estimated policy, exploit!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Once have estimated policy, exploit!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 267,
          "function_name": "introduce_model_based",
          "code": "text(\"Can we estimate the optimal policy more directly?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Can we estimate the optimal policy more directly?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 24,
          "function_name": "main",
          "code": "introduce_model_based()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 392,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "def introduce_model_free_monte_carlo():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 393,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Previously: model-based value iteration:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Previously: model-based value iteration:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 394,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"1. Estimate the MDP first.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Estimate the MDP first.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 395,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"2. Use value iteration to compute the optimal policy of the estimated MDP.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Use value iteration to compute the optimal policy of the estimated MDP.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 397,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "image(\"images/value_iteration_recurrence.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/value_iteration_recurrence.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 398,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Optimal policy: \u03c0`*`(s) = argmax_a Q`*`(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Optimal policy: \u03c0`*`(s) = argmax_a Q`*`(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 399,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"where Q`*`(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V`*`(s'))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "where Q`*`(s, a) = \u03a3_s' T(s, a, s') (R(s, a, s') + \u03b3 V`*`(s'))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 400,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Can we estimate Q`*`(s, a) directly?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Can we estimate Q`*`(s, a) directly?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 402,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Yes!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Yes!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 403,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Key idea: just rollout the policy and average the utilities!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key idea: just rollout the policy and average the utilities!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 435,
          "function_name": "example_utility_calculation",
          "code": "def example_utility_calculation():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 436,
          "function_name": "example_utility_calculation",
          "code": "text(\"The utility at each step is the discounted sum of rewards from that point on.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The utility at each step is the discounted sum of rewards from that point on.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 438,
          "function_name": "example_utility_calculation",
          "code": "Step(action=\"walk\", prob=1, reward=-1, state=2),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 439,
          "function_name": "example_utility_calculation",
          "code": "Step(action=\"tram\", prob=1, reward=-2, state=2),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 440,
          "function_name": "example_utility_calculation",
          "code": "Step(action=\"tram\", prob=1, reward=-2, state=4),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 437,
          "function_name": "example_utility_calculation",
          "code": "rollout = ["
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 442,
          "function_name": "example_utility_calculation",
          "code": "discount = 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 444,
          "function_name": "example_utility_calculation",
          "code": "-1 + (discount * -2) + (discount**2 * -2),  # step 0 utility"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 445,
          "function_name": "example_utility_calculation",
          "code": "-2 + (discount * -2),                       # step 1 utility"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 446,
          "function_name": "example_utility_calculation",
          "code": "-2 + (discount * 0),                        # step 2 utility"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 443,
          "function_name": "example_utility_calculation",
          "code": "utilities = [  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -5,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -4,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 449,
          "function_name": "example_utility_calculation",
          "code": "text(\"There is a nice recurrence relation between the utilities:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "There is a nice recurrence relation between the utilities:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 450,
          "function_name": "example_utility_calculation",
          "code": "assert utilities[0] == rollout[0].reward + (discount * utilities[1])"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 451,
          "function_name": "example_utility_calculation",
          "code": "assert utilities[1] == rollout[1].reward + (discount * utilities[2])"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 405,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "example_utility_calculation()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 407,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Which policy should we use to rollout?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Which policy should we use to rollout?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 408,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- For model-based value iteration, we had a purely random exploration policy (for phase 1).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- For model-based value iteration, we had a purely random exploration policy (for phase 1).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 409,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- Let's do something a bit more sophisticated: **epsilon-greedy**.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Let's do something a bit more sophisticated: **epsilon-greedy**.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 410,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- With probability epsilon, choose a random action according to the exploration policy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- With probability epsilon, choose a random action according to the exploration policy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 411,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- With probability 1 - epsilon, choose the best action according to the current estimated Q-values.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- With probability 1 - epsilon, choose the best action according to the current estimated Q-values.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 413,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Now let's define Model-free Monte Carlo:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now let's define Model-free Monte Carlo:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 414,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 415,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "np.random.seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 417,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "exploration_policy = partial(walk_tram_policy, mdp.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 454,
          "function_name": "__init__",
          "code": "def __init__(self, exploration_policy: Policy, epsilon: float, discount: float):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 455,
          "function_name": "__init__",
          "code": "self.exploration_policy = exploration_policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 456,
          "function_name": "__init__",
          "code": "self.epsilon = epsilon"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 457,
          "function_name": "__init__",
          "code": "self.discount = discount"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 460,
          "function_name": "__init__",
          "code": "self.sum_utilities = defaultdict(lambda : defaultdict(float))  # state -> action -> sum of utility from (state, action)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 461,
          "function_name": "__init__",
          "code": "self.counts = defaultdict(lambda : defaultdict(int))  # state -> action -> visitation count"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 464,
          "function_name": "__init__",
          "code": "self.start_state = None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 465,
          "function_name": "__init__",
          "code": "self.rollout: list[Step] = []"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 418,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "rl = ModelFreeMonteCarlo(exploration_policy=exploration_policy, epsilon=0.4, discount=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 522,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "def try_out_model_free_monte_carlo(rl: ModelFreeMonteCarlo):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 523,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 523,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 467,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 523,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 468,
          "function_name": "get_action",
          "code": "if len(self.counts[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 523,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 470,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 523,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 497,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect state action reward next_state is_end"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -1,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 499,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 500,
          "function_name": "incorporate_feedback",
          "code": "self.start_state = state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 501,
          "function_name": "incorporate_feedback",
          "code": "self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout"
        }
      ],
      "env": {
        "self.rollout": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 504,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 526,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 527,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 527,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 497,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect state action reward next_state is_end"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 527,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 499,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 527,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 501,
          "function_name": "incorporate_feedback",
          "code": "self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout"
        }
      ],
      "env": {
        "self.rollout": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 527,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 504,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 527,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 497,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect state action reward next_state is_end"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": true,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 499,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 501,
          "function_name": "incorporate_feedback",
          "code": "self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout"
        }
      ],
      "env": {
        "self.rollout": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 504,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 505,
          "function_name": "incorporate_feedback",
          "code": "utilities = [0] * (len(self.rollout) + 1)  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 507,
          "function_name": "incorporate_feedback",
          "code": "for i, step in reversed(list(enumerate(self.rollout))):  # @inspect i step"
        }
      ],
      "env": {
        "i": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 510,
          "function_name": "incorporate_feedback",
          "code": "state = self.start_state if i == 0 else self.rollout[i - 1].state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 511,
          "function_name": "incorporate_feedback",
          "code": "utilities[i] = step.reward + self.discount * utilities[i + 1]  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 514,
          "function_name": "incorporate_feedback",
          "code": "self.sum_utilities[state][step.action] += utilities[i]  # @inspect self.sum_utilities"
        }
      ],
      "env": {
        "self.sum_utilities": {
          "type": "collections.defaultdict",
          "contents": {
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -2.0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 515,
          "function_name": "incorporate_feedback",
          "code": "self.counts[state][step.action] += 1  # @inspect self.counts"
        }
      ],
      "env": {
        "self.counts": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 507,
          "function_name": "incorporate_feedback",
          "code": "for i, step in reversed(list(enumerate(self.rollout))):  # @inspect i step"
        }
      ],
      "env": {
        "i": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        },
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 510,
          "function_name": "incorporate_feedback",
          "code": "state = self.start_state if i == 0 else self.rollout[i - 1].state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 511,
          "function_name": "incorporate_feedback",
          "code": "utilities[i] = step.reward + self.discount * utilities[i + 1]  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -4,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 514,
          "function_name": "incorporate_feedback",
          "code": "self.sum_utilities[state][step.action] += utilities[i]  # @inspect self.sum_utilities"
        }
      ],
      "env": {
        "self.sum_utilities": {
          "type": "collections.defaultdict",
          "contents": {
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -6.0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 515,
          "function_name": "incorporate_feedback",
          "code": "self.counts[state][step.action] += 1  # @inspect self.counts"
        }
      ],
      "env": {
        "self.counts": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 507,
          "function_name": "incorporate_feedback",
          "code": "for i, step in reversed(list(enumerate(self.rollout))):  # @inspect i step"
        }
      ],
      "env": {
        "i": {
          "type": "int",
          "contents": 0,
          "dtype": null,
          "shape": null
        },
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 510,
          "function_name": "incorporate_feedback",
          "code": "state = self.start_state if i == 0 else self.rollout[i - 1].state  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 511,
          "function_name": "incorporate_feedback",
          "code": "utilities[i] = step.reward + self.discount * utilities[i + 1]  # @inspect utilities"
        }
      ],
      "env": {
        "utilities": {
          "type": "list",
          "contents": [
            {
              "type": "int",
              "contents": -5,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -4,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 514,
          "function_name": "incorporate_feedback",
          "code": "self.sum_utilities[state][step.action] += utilities[i]  # @inspect self.sum_utilities"
        }
      ],
      "env": {
        "self.sum_utilities": {
          "type": "collections.defaultdict",
          "contents": {
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -6.0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -5.0,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 515,
          "function_name": "incorporate_feedback",
          "code": "self.counts[state][step.action] += 1  # @inspect self.counts"
        }
      ],
      "env": {
        "self.counts": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 507,
          "function_name": "incorporate_feedback",
          "code": "for i, step in reversed(list(enumerate(self.rollout))):  # @inspect i step"
        }
      ],
      "env": {
        "i": {
          "type": "int",
          "contents": 0,
          "dtype": null,
          "shape": null
        },
        "step": {
          "type": "mdp.Step",
          "contents": {
            "action": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "prob": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "reward": {
              "type": "int",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "state": {
              "type": "int",
              "contents": 2,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 518,
          "function_name": "incorporate_feedback",
          "code": "self.start_state = None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 519,
          "function_name": "incorporate_feedback",
          "code": "self.rollout = []"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 528,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 467,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 468,
          "function_name": "get_action",
          "code": "if len(self.counts[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 473,
          "function_name": "get_action",
          "code": "if np.random.random() < self.epsilon:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 482,
          "function_name": "pi",
          "code": "def pi(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 484,
          "function_name": "pi",
          "code": "actions = list(self.counts[state].keys())  # @inspect actions"
        }
      ],
      "env": {
        "actions": {
          "type": "list",
          "contents": [
            {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "<listcomp>",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 490,
          "function_name": "Q",
          "code": "def Q(self, state: Any, action: Any) -> float:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "<listcomp>",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 492,
          "function_name": "Q",
          "code": "sum_utility = self.sum_utilities[state][action]  # @inspect sum_utility"
        }
      ],
      "env": {
        "sum_utility": {
          "type": "float",
          "contents": -5.0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "<listcomp>",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 493,
          "function_name": "Q",
          "code": "count = self.counts[state][action]  # @inspect count"
        }
      ],
      "env": {
        "count": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "<listcomp>",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 494,
          "function_name": "Q",
          "code": "value = sum_utility / count  # @inspect value"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -5.0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "<listcomp>",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 495,
          "function_name": "Q",
          "code": "return value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 485,
          "function_name": "pi",
          "code": "q_values = [self.Q(state, action) for action in actions]  # @inspect q_values"
        }
      ],
      "env": {
        "q_values": {
          "type": "list",
          "contents": [
            {
              "type": "float",
              "contents": -5.0,
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 486,
          "function_name": "pi",
          "code": "action = actions[np.argmax(q_values).item()]  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 487,
          "function_name": "pi",
          "code": "return action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 478,
          "function_name": "get_action",
          "code": "return self.pi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 530,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 531,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 532,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 533,
          "function_name": "try_out_model_free_monte_carlo",
          "code": "action = rl.get_action(state=1)  # @inspect action @stepover"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 420,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "try_out_model_free_monte_carlo(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 422,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Let's run it for real now!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's run it for real now!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 423,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "value = simulate(mdp, rl, num_trials=20)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -9.45,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 424,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "leaderboard = update_leaderboard(\"model_free_monte_carlo\", value)  # @inspect leaderboard @stepover"
        }
      ],
      "env": {
        "leaderboard": {
          "type": "dict",
          "contents": {
            "tram_if_possible_policy": {
              "type": "float",
              "contents": -11.6,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.explore": {
              "type": "float",
              "contents": -9.0,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.exploit": {
              "type": "float",
              "contents": -6.6,
              "dtype": null,
              "shape": null
            },
            "model_free_monte_carlo": {
              "type": "float",
              "contents": -9.45,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 426,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 427,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- Model-free Monte Carlo: estimate Q-values of the current policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-free Monte Carlo: estimate Q-values of the current policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 428,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- Directly uses rollouts, bypassing estimating the MDP\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Directly uses rollouts, bypassing estimating the MDP",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 429,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"- Use epsilon-greedy policy to balance exploration and exploitation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Use epsilon-greedy policy to balance exploration and exploitation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 431,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Problem: in life, you only get one rollout.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Problem: in life, you only get one rollout.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 432,
          "function_name": "introduce_model_free_monte_carlo",
          "code": "text(\"Can we update the Q-values before the rollout is over?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Can we update the Q-values before the rollout is over?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 25,
          "function_name": "main",
          "code": "introduce_model_free_monte_carlo()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 536,
          "function_name": "introduce_sarsa",
          "code": "def introduce_sarsa():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 537,
          "function_name": "introduce_sarsa",
          "code": "text(\"Previously: model-free Monte Carlo: estimate Q-values directly from rollouts\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Previously: model-free Monte Carlo: estimate Q-values directly from rollouts",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 538,
          "function_name": "introduce_sarsa",
          "code": "text(\"SARSA: update Q-values as you rollout!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "SARSA: update Q-values as you rollout!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 540,
          "function_name": "introduce_sarsa",
          "code": "text(\"If we don't rollout completely, how do we get the utility (which requires going towards the end)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If we don't rollout completely, how do we get the utility (which requires going towards the end)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 542,
          "function_name": "introduce_sarsa",
          "code": "text(\"Key insight: **bootstrapping**!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key insight: **bootstrapping**!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 543,
          "function_name": "introduce_sarsa",
          "code": "text(\"Combine the immediate reward with a model estimate of the future\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Combine the immediate reward with a model estimate of the future",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 545,
          "function_name": "introduce_sarsa",
          "code": "text(\"Monte Carlo: u = r_0 + \u03b3*r_1 + \u03b3^2*r_2 + ... + \u03b3^n*r_n\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Monte Carlo: u = r_0 + \u03b3*r_1 + \u03b3^2*r_2 + ... + \u03b3^n*r_n",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 546,
          "function_name": "introduce_sarsa",
          "code": "text(\"Bootstrapping (SARSA): u = r_0 + \u03b3*Q_\u03c0(s_1, a_1)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Bootstrapping (SARSA): u = r_0 + \u03b3*Q_\u03c0(s_1, a_1)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 548,
          "function_name": "introduce_sarsa",
          "code": "text(\"Perform a gradient update to move Q_\u03c0(s, a) towards u\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Perform a gradient update to move Q_\u03c0(s, a) towards u",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 550,
          "function_name": "introduce_sarsa",
          "code": "text(\"Instantiate our favorite flaky tram MDP:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Instantiate our favorite flaky tram MDP:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 551,
          "function_name": "introduce_sarsa",
          "code": "mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 552,
          "function_name": "introduce_sarsa",
          "code": "np.random.seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 554,
          "function_name": "introduce_sarsa",
          "code": "exploration_policy = partial(walk_tram_policy, mdp.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 573,
          "function_name": "__init__",
          "code": "def __init__(self, exploration_policy: Policy, epsilon: float, discount: float, learning_rate: float):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 574,
          "function_name": "__init__",
          "code": "self.exploration_policy = exploration_policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 575,
          "function_name": "__init__",
          "code": "self.epsilon = epsilon"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 576,
          "function_name": "__init__",
          "code": "self.discount = discount"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 577,
          "function_name": "__init__",
          "code": "self.learning_rate = learning_rate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 578,
          "function_name": "__init__",
          "code": "self.Q = defaultdict(lambda : defaultdict(float))  # state -> action -> Q-value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 555,
          "function_name": "introduce_sarsa",
          "code": "rl = SARSA(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 606,
          "function_name": "try_out_sarsa",
          "code": "def try_out_sarsa(rl: SARSA):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 607,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 607,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 607,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 607,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 607,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 598,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -1,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 210,
          "function_name": "walk_tram_policy",
          "code": "def walk_tram_policy(num_locs: int, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 212,
          "function_name": "walk_tram_policy",
          "code": "if 2 * state <= num_locs:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 214,
          "function_name": "walk_tram_policy",
          "code": "return np.random.choice([\"walk\", \"tram\"]).item()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        }
      ],
      "env": {
        "next_action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 602,
          "function_name": "incorporate_feedback",
          "code": "utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility"
        }
      ],
      "env": {
        "utility": {
          "type": "int",
          "contents": -1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 603,
          "function_name": "incorporate_feedback",
          "code": "self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 609,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 598,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        }
      ],
      "env": {
        "next_action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 602,
          "function_name": "incorporate_feedback",
          "code": "utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility"
        }
      ],
      "env": {
        "utility": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 603,
          "function_name": "incorporate_feedback",
          "code": "self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -0.2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 610,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 598,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -0.2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": true,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 210,
          "function_name": "walk_tram_policy",
          "code": "def walk_tram_policy(num_locs: int, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 212,
          "function_name": "walk_tram_policy",
          "code": "if 2 * state <= num_locs:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 214,
          "function_name": "walk_tram_policy",
          "code": "return np.random.choice([\"walk\", \"tram\"]).item()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 601,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.get_action(next_state)  # @inspect next_action"
        }
      ],
      "env": {
        "next_action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 602,
          "function_name": "incorporate_feedback",
          "code": "utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility"
        }
      ],
      "env": {
        "utility": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 603,
          "function_name": "incorporate_feedback",
          "code": "self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -0.38,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 611,
          "function_name": "try_out_sarsa",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 613,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 613,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 613,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 613,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 584,
          "function_name": "get_action",
          "code": "if np.random.random() < self.epsilon:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 613,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 585,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 613,
          "function_name": "try_out_sarsa",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 557,
          "function_name": "introduce_sarsa",
          "code": "try_out_sarsa(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 559,
          "function_name": "introduce_sarsa",
          "code": "text(\"Try it out for real!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Try it out for real!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 560,
          "function_name": "introduce_sarsa",
          "code": "value = simulate(mdp, rl, num_trials=20)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -9.6,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 561,
          "function_name": "introduce_sarsa",
          "code": "leaderboard = update_leaderboard(\"sarsa\", value)  # @inspect leaderboard @stepover"
        }
      ],
      "env": {
        "leaderboard": {
          "type": "dict",
          "contents": {
            "tram_if_possible_policy": {
              "type": "float",
              "contents": -11.6,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.explore": {
              "type": "float",
              "contents": -9.0,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.exploit": {
              "type": "float",
              "contents": -6.6,
              "dtype": null,
              "shape": null
            },
            "model_free_monte_carlo": {
              "type": "float",
              "contents": -9.45,
              "dtype": null,
              "shape": null
            },
            "sarsa": {
              "type": "float",
              "contents": -9.6,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 563,
          "function_name": "introduce_sarsa",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 564,
          "function_name": "introduce_sarsa",
          "code": "text(\"- SARSA: estimates Q-values of the current policy Q_\u03c0(s, a) as you rollout\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- SARSA: estimates Q-values of the current policy Q_\u03c0(s, a) as you rollout",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 565,
          "function_name": "introduce_sarsa",
          "code": "text(\"- Bootstrapping: estimate utility using model estimate of the future\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Bootstrapping: estimate utility using model estimate of the future",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 566,
          "function_name": "introduce_sarsa",
          "code": "text(\"- Gradient update: move Q-values towards the estimated utility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Gradient update: move Q-values towards the estimated utility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 568,
          "function_name": "introduce_sarsa",
          "code": "text(\"But we are only computing Q-values of the current policy Q_\u03c0(s, a) (**on-policy**).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But we are only computing Q-values of the current policy Q_\u03c0(s, a) (**on-policy**).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 569,
          "function_name": "introduce_sarsa",
          "code": "text(\"Can we directly estimate Q-values of the optimal policy Q`*`(s, a)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Can we directly estimate Q-values of the optimal policy Q`*`(s, a)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 26,
          "function_name": "main",
          "code": "introduce_sarsa()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 616,
          "function_name": "introduce_q_learning",
          "code": "def introduce_q_learning():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 617,
          "function_name": "introduce_q_learning",
          "code": "text(\"SARSA: estimate Q-values of the current policy Q_\u03c0(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "SARSA: estimate Q-values of the current policy Q_\u03c0(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 618,
          "function_name": "introduce_q_learning",
          "code": "text(\"Q-learning: estimate Q-values of the optimal policy Q`*`(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Q-learning: estimate Q-values of the optimal policy Q`*`(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 620,
          "function_name": "introduce_q_learning",
          "code": "text(\"But we don't know the optimal policy...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But we don't know the optimal policy...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 622,
          "function_name": "introduce_q_learning",
          "code": "mdp = FlakyTramMDP(num_locs=10, failure_prob=0.4)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 623,
          "function_name": "introduce_q_learning",
          "code": "np.random.seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 625,
          "function_name": "introduce_q_learning",
          "code": "exploration_policy = partial(walk_tram_policy, mdp.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 573,
          "function_name": "__init__",
          "code": "def __init__(self, exploration_policy: Policy, epsilon: float, discount: float, learning_rate: float):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 574,
          "function_name": "__init__",
          "code": "self.exploration_policy = exploration_policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 575,
          "function_name": "__init__",
          "code": "self.epsilon = epsilon"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 576,
          "function_name": "__init__",
          "code": "self.discount = discount"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 577,
          "function_name": "__init__",
          "code": "self.learning_rate = learning_rate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 578,
          "function_name": "__init__",
          "code": "self.Q = defaultdict(lambda : defaultdict(float))  # state -> action -> Q-value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 626,
          "function_name": "introduce_q_learning",
          "code": "rl = QLearning(exploration_policy=exploration_policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 649,
          "function_name": "try_out_q_learning",
          "code": "def try_out_q_learning(rl: QLearning):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 650,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 650,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 650,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 650,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 582,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 650,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 641,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -1,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 589,
          "function_name": "pi",
          "code": "def pi(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 591,
          "function_name": "pi",
          "code": "actions = list(self.Q[state].keys())  # @inspect actions"
        }
      ],
      "env": {
        "actions": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 592,
          "function_name": "pi",
          "code": "if len(actions) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 593,
          "function_name": "pi",
          "code": "return None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        }
      ],
      "env": {
        "next_action": {
          "type": "NoneType",
          "contents": "None",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 645,
          "function_name": "incorporate_feedback",
          "code": "utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility"
        }
      ],
      "env": {
        "utility": {
          "type": "int",
          "contents": -1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 646,
          "function_name": "incorporate_feedback",
          "code": "self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 652,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 641,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": false,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 589,
          "function_name": "pi",
          "code": "def pi(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 591,
          "function_name": "pi",
          "code": "actions = list(self.Q[state].keys())  # @inspect actions"
        }
      ],
      "env": {
        "actions": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 592,
          "function_name": "pi",
          "code": "if len(actions) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 593,
          "function_name": "pi",
          "code": "return None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        }
      ],
      "env": {
        "next_action": {
          "type": "NoneType",
          "contents": "None",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 645,
          "function_name": "incorporate_feedback",
          "code": "utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility"
        }
      ],
      "env": {
        "utility": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 646,
          "function_name": "incorporate_feedback",
          "code": "self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -0.2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 653,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 641,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:  # @inspect self.Q state action reward next_state is_end"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -0.2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        },
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        },
        "reward": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        },
        "next_state": {
          "type": "int",
          "contents": 4,
          "dtype": null,
          "shape": null
        },
        "is_end": {
          "type": "bool",
          "contents": true,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 589,
          "function_name": "pi",
          "code": "def pi(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 591,
          "function_name": "pi",
          "code": "actions = list(self.Q[state].keys())  # @inspect actions"
        }
      ],
      "env": {
        "actions": {
          "type": "list",
          "contents": [],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 592,
          "function_name": "pi",
          "code": "if len(actions) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 593,
          "function_name": "pi",
          "code": "return None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 644,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action"
        }
      ],
      "env": {
        "next_action": {
          "type": "NoneType",
          "contents": "None",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 645,
          "function_name": "incorporate_feedback",
          "code": "utility = reward + self.discount * self.Q[next_state].get(next_action, 0)  # @inspect utility"
        }
      ],
      "env": {
        "utility": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 646,
          "function_name": "incorporate_feedback",
          "code": "self.Q[state][action] += self.learning_rate * (utility - self.Q[state][action])  # @inspect self.Q"
        }
      ],
      "env": {
        "self.Q": {
          "type": "collections.defaultdict",
          "contents": {
            "1": {
              "type": "collections.defaultdict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": -0.1,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "collections.defaultdict",
              "contents": {
                "tram": {
                  "type": "float",
                  "contents": -0.38,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "collections.defaultdict",
              "contents": {},
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 654,
          "function_name": "try_out_q_learning",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 656,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 656,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 580,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 656,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 581,
          "function_name": "get_action",
          "code": "if len(self.Q[state]) == 0:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 656,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 584,
          "function_name": "get_action",
          "code": "if np.random.random() < self.epsilon:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 656,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 587,
          "function_name": "get_action",
          "code": "return self.pi(state) # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 656,
          "function_name": "try_out_q_learning",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 628,
          "function_name": "introduce_q_learning",
          "code": "try_out_q_learning(rl)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 630,
          "function_name": "introduce_q_learning",
          "code": "text(\"Try it out for real!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Try it out for real!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 631,
          "function_name": "introduce_q_learning",
          "code": "value = simulate(mdp, rl, num_trials=20)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -9.9,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 632,
          "function_name": "introduce_q_learning",
          "code": "leaderboard = update_leaderboard(\"q_learning\", value)  # @inspect leaderboard @stepover"
        }
      ],
      "env": {
        "leaderboard": {
          "type": "dict",
          "contents": {
            "tram_if_possible_policy": {
              "type": "float",
              "contents": -11.6,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.explore": {
              "type": "float",
              "contents": -9.0,
              "dtype": null,
              "shape": null
            },
            "model_based_value_iteration.exploit": {
              "type": "float",
              "contents": -6.6,
              "dtype": null,
              "shape": null
            },
            "model_free_monte_carlo": {
              "type": "float",
              "contents": -9.45,
              "dtype": null,
              "shape": null
            },
            "sarsa": {
              "type": "float",
              "contents": -9.6,
              "dtype": null,
              "shape": null
            },
            "q_learning": {
              "type": "float",
              "contents": -9.9,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 634,
          "function_name": "introduce_q_learning",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 635,
          "function_name": "introduce_q_learning",
          "code": "text(\"- Q-learning: estimates Q-values of the optimal policy Q`*`(s, a) (**off-policy**)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q-learning: estimates Q-values of the optimal policy Q`*`(s, a) (**off-policy**)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        },
        {
          "path": "reinforcement_learning.py",
          "line_number": 636,
          "function_name": "introduce_q_learning",
          "code": "text(\"- Like SARSA, uses bootstrapping and gradient updates\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Like SARSA, uses bootstrapping and gradient updates",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 27,
          "function_name": "main",
          "code": "introduce_q_learning()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 29,
          "function_name": "main",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 30,
          "function_name": "main",
          "code": "text(\"- Reinforcement learning: learn the optimal policy from interacting with the environment\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Reinforcement learning: learn the optimal policy from interacting with the environment",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 31,
          "function_name": "main",
          "code": "text(\"- Agent (RL algorithm): `get_action` and `incorporate_feedback`\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Agent (RL algorithm): `get_action` and `incorporate_feedback`",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 32,
          "function_name": "main",
          "code": "text(\"- Model-based value iteration: estimate the MDP then compute the optimal policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-based value iteration: estimate the MDP then compute the optimal policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 33,
          "function_name": "main",
          "code": "text(\"- Model-free Monte Carlo: estimate Q-values directly from rollouts (on-policy)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-free Monte Carlo: estimate Q-values directly from rollouts (on-policy)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 34,
          "function_name": "main",
          "code": "text(\"- SARSA: estimate Q-values of the current policy as you rollout (on-policy, bootstrapping)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- SARSA: estimate Q-values of the current policy as you rollout (on-policy, bootstrapping)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 35,
          "function_name": "main",
          "code": "text(\"- Q-learning: estimate Q-values of the optimal policy (off-policy, bootstrapping)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q-learning: estimate Q-values of the optimal policy (off-policy, bootstrapping)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "reinforcement_learning.py",
          "line_number": 37,
          "function_name": "main",
          "code": "text(\"Next time: how do we deal with huge state spaces?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Next time: how do we deal with huge state spaces?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    }
  ]
}