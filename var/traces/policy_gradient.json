{
  "files": {
    "policy_gradient.py": "from edtrace import text, image, link\nfrom typing import Any\nfrom functools import partial\nfrom typing import Callable\nfrom mdp import FlakyTramMDP, value_iteration\nfrom reinforcement_learning import QLearning, walk_tram_policy, simulate, RLAlgorithm, Step, Rollout\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom util import set_random_seed, one_hot\n\nPolicy = Callable[[Any], Any]\n\ndef main():\n    text(\"Last time: reinforcement learning\")\n    text(\"This time: generalize to large state spaces, learn policy directly\")\n\n    review_rl()\n\n    function_approximation()\n    policy_gradient()\n\n    text(\"Summary:\")\n    text(\"- Model-based: estimate the MDP\")\n    text(\"- Value-based: estimate Q-values\")\n    text(\"- Policy-based: estimate policy\")\n\n    text(\"Form of algorithm\")\n    text(\"1. Generate rollouts from exploration policy\")\n    text(\"2. Form some loss function (parameters of MDP, Q-values, or policy)\")\n    text(\"3. Update parameters using gradient step\")\n\n    text(\"In MDPs and RL, we're maximizing expected utility.\")\n    text(\"Next time: what if there are adversaries in the environment?\")\n\n\ndef review_rl():\n    text(\"Reinforcement learning (RL) setting:\")\n    image(\"images/rl-framework.png\", width=400)\n\n    text(\"**Environment**: Markov decision process (MDP)\")\n    mdp = FlakyTramMDP(num_locs=6, failure_prob=0.1)  # @stepover\n    set_random_seed(1)\n\n    text(\"**Agent**: RL algorithm\")\n    policy = partial(walk_tram_policy, mdp.num_locs)\n    rl = QLearning(exploration_policy=policy, epsilon=0.4, discount=1, learning_rate=0.1)\n\n    text(\"An example interaction (what the agent sees):\")\n    action = rl.get_action(state=1)  # @inspect action\n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    action = rl.get_action(state=2)  # @inspect action\n    rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)\n    action = rl.get_action(state=3)  # @inspect action\n    rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)\n    text(\"Each rollout generates a utility (discounted sum of rewards).\")  # @clear action\n\n    text(\"Generate rollouts, value is the mean utility.\")\n    value = simulate(mdp, rl, num_trials=20)  # @inspect value\n\n    text(\"Various flavors of values (expected utility):\")\n    text(\"- V_\u03c0(s): value of following policy \u03c0 from state s\")\n    text(\"- Q_\u03c0(s, a): value of taking action a in state s, and then following policy \u03c0\")\n    text(\"- V`*`(s): value of following the optimal policy from state s\")\n    text(\"- Q`*`(s, a): value of taking action a in state s, and then following the optimal policy\")\n\n    text(\"To model or not to model?\")\n    text(\"- Model-based: estimate the MDP, then compute the optimal policy\")\n    text(\"- Value-based (model-free): don't estimate the MDP, just estimate Q-values directly\")\n    \n    text(\"General form for model-free methods (for model-free Monte Carlo, SARSA, Q-learning):\")\n    text(\"- Current model estimate: Q(s, a)\")\n    text(\"- Target (estimate of utility)\")\n    text(\"Update: Q(s, a) \u2190 Q(s, a) + learning_rate * (target - Q(s, a))\")\n\n    text(\"On-policy (Q_\u03c0) vs. off-policy (Q`*`):\")\n    text(\"Follow exploration policy \u03c0 to generate rollouts\")\n    text(\"...to estimate Q-values of an estimation policy\")\n    text(\"- On-policy: exploration policy = estimation policy\")\n    text(\"- Off-policy: exploration policy \u2260 estimation policy\")\n\n    text(\"What to use as targets?\")\n    text(\"- Full rollouts: use actual utility (discounted sum of rewards)\")\n    text(\"- Bootstrapping: immediate reward + estimate of future reward\")\n\n    text(\"Algorithms:\")\n    text(\"- Model-based value iteration: estimate the MDP then compute the optimal policy\")\n    text(\"- Model-free Monte Carlo: estimate Q_\u03c0(s, a) from full rollouts (on-policy): target = utility\")\n    text(\"- SARSA: estimate Q_\u03c0(s, a) as you rollout (on-policy, bootstrapping): target = r + \u03b3 * Q(s', a'), a' = \u03c0(s')\")\n    text(f\"- Q-learning: estimate Q`*`(s, a) (off-policy, bootstrapping): target = r + \u03b3 * max_a' Q`*`(s', a')\")\n\n    text(\"So far, we are estimating a Q-value for each state/action (s, a) pair.\")\n    text(\"This is the **tabular** setting (we keep a lookup table for each state/action pair).\")\n\n    text(\"Examples of states in real-world settings:\")\n    text(\"- State is an image (e.g., for learning a robot policy)\")\n    image(\"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v1/berkeley/toykitchen2/put_potato_in_pot_cardboard_fence/2022-01-28_11-25-48/raw/traj_group0/traj13/images0/im_0.jpg\", width=200)\n    text(\"- State is a sentence (e.g., for theorem proving)\")\n    text(\"*Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations...*\")\n\n    text(\"How do we handle these huge state spaces (all possible images or sentences)?\")\n\n\ndef function_approximation():\n    text(\"Tabular setting: lookup table for Q(s, a)\")\n    text(\"Function approximation: Q_\u03b8(s, a) is a function with parameters \u03b8\")\n\n    text(\"Design decisions:\")\n    text(\"1. What are the possible functions Q_\u03b8(s, a)?\")\n    text(\"2. What do we determine whether Q_\u03b8(s, a) is good? (loss function)\")\n    text(\"3. How do we reduce the loss function? (optimization algorithm)\")\n    text(\"Should seem familiar from the machine learning lecture!\")\n\n    text(\"1. What are the possible functions Q_\u03b8(s, a)?\")\n    text(\"- First, map the state and action to a vector of features \u03c6(s, a)\")\n    text(\"- Then, we could use a linear function: Q_\u03b8(s, a) = \u03c6(s, a) * \u03b8\")\n    text(\"- Or a multi-layer perceptron (MLP): Q_\u03b8(s, a) = MLP_\u03b8(\u03c6(s, a))\")\n\n    text(\"Let's define the same environment as before:\")\n    mdp = FlakyTramMDP(num_locs=6, failure_prob=0.1)\n    set_random_seed(1)\n\n    text(\"For the agent, use a parameterized Q-value function (but for simplicity, simulate the tabular setting):\")\n    policy = partial(walk_tram_policy, mdp.num_locs)\n    rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],\n                       exploration_policy=policy, epsilon=0.4, discount=1, learning_rate=0.1)\n\n    text(\"Define feature vector \u03c6(s, a) for each state and action:\")\n    phi = rl.phi(state=1, action=\"walk\")  # @inspect phi\n    phi = rl.phi(state=1, action=\"tram\")  # @inspect phi @stepover\n    phi = rl.phi(state=2, action=\"walk\")  # @inspect phi @stepover\n    phi = rl.phi(state=2, action=\"tram\")  # @inspect phi @stepover\n    phi = rl.phi(state=6, action=\"tram\")  # @inspect phi @stepover\n\n    text(\"Define Q-value function Q_\u03b8(s, a) = \u03c6(s, a) * \u03b8:\")\n    value = rl.Q(state=1, action=\"walk\")  # @inspect value\n\n    text(\"Compute policy \u03c0(s) = argmax_a Q_\u03b8(s, a):\")\n    action = rl.pi(state=1)  # @inspect action\n \n    text(\"Using these pieces, the agent interacts with the environment:\")  # @clear phi action value\n    rl.get_action(state=1)\n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    rl.get_action(state=2)\n    rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)\n    rl.get_action(state=3)\n    rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)\n\n    text(\"Now let's run this agent for multiple rollouts:\")\n    value = simulate(mdp, rl, num_trials=100)  # @inspect value rl @stepover\n\n    text(\"We can extract the current optimal policy \u03c0_\u03b8(s) of the agent:\")\n    states = range(1, mdp.num_locs + 1)\n    pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover\n    text(\"And the corresponding values V_\u03b8(s) = Q_\u03b8(s, \u03c0_\u03b8(s)):\")\n    V = {state: rl.Q(state, pi[state]) for state in states}  # @inspect V @stepover\n    \n    text(\"Let's compare with the true values by solving the MDP:\")\n    result = value_iteration(mdp)  # @inspect result.values result.pi\n    text(\"It's in the ballpark, and more accurate for more visited states.\")\n\n    text(\"Summary:\")\n    text(\"- Function approximation: parameterize Q_\u03b8(s, a) by some \u03b8\")\n    text(\"- Map states and actions into a vector of features \u03c6(s, a)\")\n    text(\"- Define either a linear function or an MLP to map \u03c6(s, a) \u2192 Q_\u03b8(s, a)\")\n    text(\"- Define squared loss between Q_\u03b8(s, a) and target (r + \u03b3 * max_a' Q_\u03b8(s', a'))\")\n    text(\"- Q-learning: take a gradient of the loss with respect to \u03b8 and take a step\")\n\n\nclass ParameterizedQLearning(RLAlgorithm):\n    def __init__(self, num_locs: int, actions: list[str],\n                 exploration_policy: Policy, epsilon: float, discount: float, learning_rate: float):\n        self.num_locs = num_locs\n        self.actions = actions\n        self.num_features = num_locs * len(actions)\n\n        self.exploration_policy = exploration_policy\n        self.epsilon = epsilon\n        self.discount = discount\n        self.learning_rate = learning_rate\n        self.model = nn.Linear(self.num_features, 1)\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n\n\n    def phi(self, state: int, action: str) -> torch.Tensor:\n        # Map (state, action) to an integer 0 ... |states|*|actions|-1\n        index = (state - 1) * len(self.actions) + self.actions.index(action)  # @inspect index\n        # Create a one-hot vector of length self.num_features\n        vector = one_hot(index, self.num_features)  # @inspect vector\n        return vector\n\n\n    def Q(self, state: int, action: str) -> torch.Tensor:\n        \"\"\"Compute the Q-value for a given state and action.\"\"\"\n        # Map (state, action) to feature vector\n        phi = self.phi(state, action)  # @inspect phi @stepover\n\n        # Pass it through the model\n        value = self.model(phi)  # @inspect value\n\n        return value\n\n\n    def pi(self, state: int) -> str:\n        \"\"\"Compute the policy for a given state.\"\"\"\n        # Compute Q-values for all actions\n        q_values = {action: self.Q(state, action) for action in self.actions}  # @inspect q_values @stepover\n\n        # Choose the action with the highest Q-value\n        action = max(q_values.keys(), key=lambda k: q_values[k].item())  # @inspect action\n\n        return action\n\n\n    def get_action(self, state: Any) -> Any:\n        # Run epsilon-greedy\n        if torch.rand(1).item() < self.epsilon:\n            # Explore\n            return self.exploration_policy(state)\n        else:\n            # Exploit\n            return self.pi(state)  # @stepover\n\n\n    def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:\n        # Compute target (immediate reward + bootstrapped discounted future reward)\n        if is_end:\n            target = reward  # @inspect target\n        else:   # Bootstrapping\n            next_action = self.pi(next_state)  # @inspect next_action @stepover\n            target = reward + self.discount * self.Q(next_state, next_action)  # @inspect target @stepover\n\n        # Compute predicted value\n        value = self.Q(state, action)  # @inspect value @stepover\n\n        # Compute loss (standard squared loss)\n        loss = (value - target) ** 2  # @inspect loss\n\n        # Update model with a gradient step\n        self.optimizer.zero_grad()\n        loss.backward()  # Compute gradients\n        self.optimizer.step()  # Update the parameters @inspect self\n\n    def asdict(self):\n        return dict(self.model.named_parameters())\n\n\ndef policy_gradient():\n    text(\"Model-based: estimate the MDP \u2192 policy \u03c0(s)\")\n    text(\"Value-based: estimate Q-values Q(s, a) \u2192 policy \u03c0(s)\")\n\n    text(\"Why not **directly** estimate the policy \u03c0(s)?\")\n    text(\"Think of the policy as a classifier (input s \u2192 output a)\")\n\n    text(\"Recall that a classifier is a hard function to optimize directly\")\n    text(\"...so we optimize over a probabilistic classifier \u03c0(a | s).\")\n\n    imitation_learning()\n\n    policy_gradient_math()\n    policy_gradient_implementation()\n    policy_gradient_enhancements()\n\n\ndef imitation_learning():\n    text(\"If we had demonstrations of the policy, it'd be easy.\")\n    \n    text(\"Suppose we have a rollout \ud835\udf0f:\")\n    rollout = Rollout(steps=[\n        Step(action=\"walk\", prob=1, reward=-1, state=2),\n        Step(action=\"walk\", prob=1, reward=-1, state=3),\n        Step(action=\"tram\", prob=1, reward=-2, state=6),\n    ], discount=1)\n    text(\"We create the following examples:\")\n    examples = [\n        Example(input=1, output=\"walk\"),\n        Example(input=2, output=\"walk\"),\n        Example(input=3, output=\"tram\"),\n    ]\n    text(\"J(\u03b8) = \u03a3_i log \u03c0_\u03b8(a_i | s_i)\")\n    text(\"This is called **imitation learning**.\")\n\n    text(\"Examples:\")\n    text(\"- Robotics: teleoperation\")\n    image(\"https://news.stanford.edu/__data/assets/image/0028/134857/mobilealoha_2.jpg\", width=200)\n    text(\"- Math: human-written solution to a math problem\")\n\n\n@dataclass(frozen=True)\nclass Example:\n    input: Any\n    output: Any\n\n\ndef policy_gradient_math():\n    text(\"But in reinforcement learning, we don't have demonstrations.\")\n    text(\"We have a reward function...\")\n    text(\"So what do we do?\")\n\n    text(\"Suppose we have a rollout (trajectory, episode):\")\n    text(\"\ud835\udf0f = (s_0, a_1, r_1, s_1, a_2, r_2, s_2, a_3, r_3, s_3):\")\n    rollout = Rollout(steps=[  # @inspect rollout\n        Step(action=\"walk\", prob=1, reward=-1, state=2),\n        Step(action=\"walk\", prob=1, reward=-1, state=3),\n        Step(action=\"tram\", prob=1, reward=-2, state=6),\n    ], discount=1)\n    text(\"Each rollout produces a utility.\")\n\n    text(\"What is the probability of a rollout?\")\n    text(\"p(\ud835\udf0f) = p(s_0) * \u03c0_\u03b8(a_1 | s_0) * T(s_0, a_1, s_1) * \u03c0_\u03b8(a_2 | s_1) * T(s_1, a_2, s_2) * \u03c0_\u03b8(a_3 | s_2) * T(s_2, a_3, s_3)\")\n    text(\"Components:\")\n    text(\"- p(s_0): probability of starting in state s_0\")\n    text(\"- \u03c0_\u03b8(a_t | s_{t-1}): policy\")\n    text(\"- T(s_{t-1}, a_t, s_t): transition distribution\")\n\n    text(\"Our goal is to maximize the expected utility of a rollout:\")\n    text(\"V(\u03b8) = E_\u03b8[utility(\ud835\udf0f)] = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n\n    text(\"Let's just take the gradient of V(\u03b8) with respect to \u03b8:\")\n    text(\"\u2207_\u03b8 V(\u03b8) = \u2207_\u03b8 E_\u03b8[utility(\ud835\udf0f)]\")\n    text(\"\u2207_\u03b8 V(\u03b8) = \u2207_\u03b8 \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n    text(\"\u2207_\u03b8 V(\u03b8) = \u03a3_\ud835\udf0f \u2207_\u03b8 p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n    text(\"\u2207_\u03b8 V(\u03b8) = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * \u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n    text(\"\u2207_\u03b8 V(\u03b8) = E_\u03b8[\u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]\")\n    text(\"This is the **policy gradient theorem (identity)**.\")\n\n    text(\"Whenever you see E_\u03b8[...], you can replace it with a sample \ud835\udf0f ~ p_\u03b8(\ud835\udf0f)\")\n    text(\"...define the objective function J(\u03b8, \ud835\udf0f) = log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n    text(\"...and update \u03b8 by taking a step in the direction of \u2207_\u03b8 J(\u03b8, \ud835\udf0f).\")\n\n    text(\"Breaking down the gradient:\")\n    text(\"\u2207_\u03b8 J(\u03b8, \ud835\udf0f) = utility(\ud835\udf0f) * \u03a3_t \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_{t-1})\")\n    \n    text(\"This is the REINFORCE algorithm \"), link(\"https://link.springer.com/article/10.1007/BF00992696\", title=\"[Williams, 1992]\")\n    \n    text(\"Intuition:\")\n    text(\"- Just performing imitation learning on demonstrations from own policy weighted by utility.\")\n    text(\"- If utility(\ud835\udf0f) \u2208 {0, 1} (success/failure), then this is just imitation learning on own successful demonstrations.\")\n\n    text(\"Summary:\")\n    text(\"- Objective: optimize policy directly to maximize expected utility\")\n    text(\"- Due to policy gradient theorem, can compute unbiased gradient\")\n    text(\"- Algorithm: sample a rollout (on-policy), update using (state, action) in the rollout \ud835\udf0f, weighted by utility(\ud835\udf0f)\")\n\n\ndef policy_gradient_implementation():\n    mdp = FlakyTramMDP(num_locs=6, failure_prob=0.1)\n    set_random_seed(1)\n    rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)\n    text(\"Note that we don't need an explicit exploration policy\")\n    text(\"...because the stochastic policy should give us exploration.\")\n\n    text(\"Let's try a rollout:\")\n    rl.get_action(state=1)\n    rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)\n    rl.get_action(state=2)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)\n    rl.get_action(state=2)\n    rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)\n\n    text(\"Now let's run this agent for multiple rollouts:\")\n    value = simulate(mdp, rl, num_trials=100)  # @inspect value rl @stepover\n    text(\"We can extract the current optimal policy \u03c0_\u03b8(s) of the agent:\")\n    states = range(1, mdp.num_locs + 1)\n    pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover\n\n    text(\"Summary:\")\n    text(\"- `get_action`: sample from the policy \u03c0_\u03b8(a | s)\")\n    text(\"- `incorporate_feedback`: update parameters on (state, action) in the rollout \ud835\udf0f, weighted by utility(\ud835\udf0f)\")\n\n\nclass Reinforce(RLAlgorithm):\n    def __init__(self, num_locs: int, actions: list[str], discount: float, learning_rate: float):\n        self.num_locs = num_locs\n        self.actions = actions\n        self.discount = discount\n        self.learning_rate = learning_rate\n        self.model = nn.Linear(self.num_locs, len(actions))\n        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)\n\n        # Keep track of the current rollout (like model-free Monte Carlo)\n        self.start_state = None\n        self.rollout: list[Step] = []\n        self.utility = 0\n\n    def phi(self, state: int) -> torch.Tensor:\n        index = state - 1\n        return one_hot(index, self.num_locs)\n\n    def pi(self, state: int) -> dict[str, float]:\n        \"\"\"Return a distribution over actions.\"\"\"\n        phi = self.phi(state)\n        logits = self.model(phi)\n        probs = torch.softmax(logits, dim=0)\n        return dict(zip(self.actions, probs.tolist()))\n\n    def get_action(self, state: int) -> str:  # @inspect state\n        \"\"\"Sample an action from the policy.\"\"\"\n        # Compute logits under the model\n        phi = self.phi(state)  # @inspect phi @stepover\n        logits = self.model(phi)  # @inspect logits\n\n        # Sample from the distribution given by the logits\n        probs = torch.softmax(logits, dim=0)  # @inspect probs\n        index = torch.multinomial(probs, num_samples=1).item()  # @inspect index\n\n        # Return the action\n        action = self.actions[index]  # @inspect action\n        return action\n\n\n    def incorporate_feedback(self, state: int, action: str, reward: float, next_state: int, is_end: bool) -> None:\n        \"\"\"Update the policy parameters.\"\"\"\n        # Remember the rollout\n        if self.start_state is None:\n            self.start_state = state\n        self.utility += reward * self.discount ** len(self.rollout)  # @inspect self.utility\n        self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout\n\n        if is_end:\n            # Compute the loss\n            loss = 0  # @inspect loss\n            for i, step in enumerate(self.rollout):\n                state = self.start_state if i == 0 else self.rollout[i - 1].state\n                action = step.action\n\n                # Compute cross-entropy loss for state -> action\n                phi = self.phi(state)  # @inspect phi @stepover\n                logits = self.model(phi) # @inspect logits\n                cross_entropy = nn.CrossEntropyLoss()\n                target = one_hot(self.actions.index(action), len(self.actions))  # @inspect target\n                loss += cross_entropy(logits, target)  # @inspect loss\n\n            # Update parameters\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()  # @inspect self\n\n            # Reset the rollout\n            self.start_state = None\n            self.rollout = []\n            self.utility = 0\n\n    def asdict(self):\n        return dict(self.model.named_parameters())\n\n\ndef policy_gradient_enhancements():\n    text(\"Recall the objective: maximize expected utility\")\n    text(\"V(\u03b8) = E_\u03b8[utility(\ud835\udf0f)] = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n\n    text(\"Policy gradient theorem: \u2207_\u03b8 V(\u03b8) = E_\u03b8[\u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]\")\n\n    text(\"REINFORCE is one particular (unbiased) estimate of \u2207_\u03b8 V(\u03b8):\")\n    text(\"Sample \ud835\udf0f ~ p_\u03b8(\ud835\udf0f) and compute \u2207_\u03b8 J(\u03b8, \ud835\udf0f) = \u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")\n    text(\"However, can we get a better estimate?\")\n\n    variance_reduction()\n\n    text(\"So how do we find an offset(s, a) that E[offset(s, a)] = 0?\")\n\n    text(\"Key identity:\")\n    text(\"Let b(s) be **any** function that only depends on state s (not action a).\")\n    text(\"Then E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(a | s) * b(s)] = 0 for all s, and a ~ \u03c0_\u03b8(a | s).\")\n    text(\"Analogy: heuristics in A* search (use domain knowledge to improve algorithm)\")\n\n    text(\"Proof:\")\n    text(\"- E_\u03b8[b(s)] = constant\")\n    text(\"- \u2207_\u03b8 E_\u03b8[b(s)] = 0\")\n    text(\"- \u2207_\u03b8 \u03a3_a \u03c0_\u03b8(a | s) b(s) = 0\")\n    text(\"- \u03a3_a \u2207_\u03b8 \u03c0_\u03b8(a | s) b(s) = 0\")\n    text(\"- \u03a3_a \u03c0_\u03b8(a | s) \u2207_\u03b8 log \u03c0_\u03b8(a | s) b(s) = 0\")\n    text(\"- E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(a | s) * b(s)] = 0\")\n    text(\"This is basically the reverse direction of the policy gradient theorem.\")\n\n    text(\"Enhancements to reduce variance:\")\n    text(\"1. Baselines: subtract off a baseline b(s) from the utility\")\n    text(\"J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (utility(\ud835\udf0f) - b(s_{t-1}))\")\n    text(\"2. Use returns-to-go instead of utility\")\n    text(\"J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (r_t + \u03b3 r_{t+1} + \u03b3^2 r_{t+2} + ... - b(s_{t-1}))\")\n    text(\"3. Use a **biased** estimate of the value function Q(s, a) instead of utility(\ud835\udf0f) (bootstrapping).\")\n    text(\"J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (Q(s_t, a_t) - b(s_{t-1}))\")\n\n    text(\"Summary:\")\n    text(\"- The game: find a low-bias, low-variance estimate of E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]\")\n    text(\"- Baselines: a general-purpose strategy to reduce variance, still unbiased\")\n    text(\"- Bootstrapping: reduce variance but introduce a bit of bias\")\n\n\ndef variance_reduction():\n    text(\"Consider the simple mean estimation problem:\")\n    text(\"\u03bc = E[f(i)] = \u03a3_i p(i) * f(i)\")\n    probs = torch.tensor([0.1, 0.1, 0.1, 0.1])  # p(0), p(1), ... @inspect probs\n    points = torch.tensor([-4., -6., 6., 8.])  # f(0), f(1), ... @inspect points\n    \n    text(\"This is the true mean we want to estimate (is unknown):\")\n    mu = probs @ points  # @inspect mu\n\n    text(\"An **estimator** is any random variable that tries to get close to \u03bc.\")\n    text(\"Each estimator has a **bias** and a **variance** (and a cost).\")\n\n    text(\"The simplest estimate of \u03bc:\")\n    text(\"Sample a single i ~ p and return f(i).\")\n    def estimator1():\n        index = torch.multinomial(probs, num_samples=1)  # @inspect index\n        estimate = points[index]  # @inspect estimate\n        return estimate\n    estimate = estimator1()  # @inspect estimate\n    estimate = estimator1()  # @inspect estimate @stepover\n    estimate = estimator1()  # @inspect estimate @stepover\n    result1 = evaluate_estimator(estimator1)  # @inspect result1\n\n    text(\"A more expensive estimator is to sample 2 points and average:\")\n    def estimator2():\n        indices = torch.multinomial(probs, num_samples=2)  # @inspect indices\n        values = points[indices]  # @inspect values\n        estimate = torch.mean(values)  # @inspect estimate\n        return estimate\n    estimate = estimator2()  # @inspect estimate\n    estimate = estimator2()  # @inspect estimate @stepover\n    estimate = estimator2()  # @inspect estimate @stepover\n    result2 = evaluate_estimator(estimator2)  # @inspect result2 @stepover\n\n    text(\"A worse estimator is to add noise, but it's still unbiased:\")\n    def estimator3():\n        # Add noise\n        index = torch.multinomial(probs, num_samples=1)  # @inspect index\n        noise = torch.randn(1)  # @inspect noise\n        estimate = points[index] + noise  # @inspect estimate\n        return estimate\n    estimate = estimator3()  # @inspect estimate\n    estimate = estimator3()  # @inspect estimate @stepover\n    estimate = estimator3()  # @inspect estimate @stepover\n    result3 = evaluate_estimator(estimator3)  # @inspect result3 @stepover\n\n    text(\"Let's suppose we have a magic function offset(i) that has mean 0.\")\n    text(\"Then E[f(i) - offset(i)]\")\n    text(\"= E[f(i)] - E[offset(i)]\")\n    text(\"= E[f(i)]\")\n    def estimator4():\n        offsets = torch.tensor([-6., -6., 6., 6.])\n        index = torch.multinomial(probs, num_samples=1)  # @inspect index\n        estimate = points[index] - offsets[index]  # @inspect estimate\n        return estimate\n    estimate = estimator4()  # @inspect estimate\n    estimate = estimator4()  # @inspect estimate @stepover\n    estimate = estimator4()  # @inspect estimate @stepover\n    result4 = evaluate_estimator(estimator4)  # @inspect result4 @stepover\n\n\ndef evaluate_estimator(estimator):\n    # Try 1000 samples\n    num_samples = 1000\n    samples = torch.tensor([estimator() for _ in range(num_samples)])  # @inspect samples @stepover\n    mean = torch.mean(samples)  # @inspect mean\n    variance = torch.var(samples)  # @inspect variance\n    return {\n        \"mean\": mean,\n        \"variance\": variance,\n    }\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  "hidden_line_numbers": {
    "policy_gradient.py": []
  },
  "steps": [
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 14,
          "function_name": "main",
          "code": "def main():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 15,
          "function_name": "main",
          "code": "text(\"Last time: reinforcement learning\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Last time: reinforcement learning",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 16,
          "function_name": "main",
          "code": "text(\"This time: generalize to large state spaces, learn policy directly\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This time: generalize to large state spaces, learn policy directly",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 37,
          "function_name": "review_rl",
          "code": "def review_rl():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 38,
          "function_name": "review_rl",
          "code": "text(\"Reinforcement learning (RL) setting:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Reinforcement learning (RL) setting:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 39,
          "function_name": "review_rl",
          "code": "image(\"images/rl-framework.png\", width=400)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "images/rl-framework.png",
          "style": {
            "width": 400
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 41,
          "function_name": "review_rl",
          "code": "text(\"**Environment**: Markov decision process (MDP)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Environment**: Markov decision process (MDP)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 42,
          "function_name": "review_rl",
          "code": "mdp = FlakyTramMDP(num_locs=6, failure_prob=0.1)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 43,
          "function_name": "review_rl",
          "code": "set_random_seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 45,
          "function_name": "review_rl",
          "code": "text(\"**Agent**: RL algorithm\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "**Agent**: RL algorithm",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 46,
          "function_name": "review_rl",
          "code": "policy = partial(walk_tram_policy, mdp.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 47,
          "function_name": "review_rl",
          "code": "rl = QLearning(exploration_policy=policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 49,
          "function_name": "review_rl",
          "code": "text(\"An example interaction (what the agent sees):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "An example interaction (what the agent sees):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 50,
          "function_name": "review_rl",
          "code": "action = rl.get_action(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 51,
          "function_name": "review_rl",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 52,
          "function_name": "review_rl",
          "code": "action = rl.get_action(state=2)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 53,
          "function_name": "review_rl",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 54,
          "function_name": "review_rl",
          "code": "action = rl.get_action(state=3)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 55,
          "function_name": "review_rl",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 56,
          "function_name": "review_rl",
          "code": "text(\"Each rollout generates a utility (discounted sum of rewards).\")  # @clear action"
        }
      ],
      "env": {
        "action": null
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "Each rollout generates a utility (discounted sum of rewards).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 58,
          "function_name": "review_rl",
          "code": "text(\"Generate rollouts, value is the mean utility.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Generate rollouts, value is the mean utility.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 59,
          "function_name": "review_rl",
          "code": "value = simulate(mdp, rl, num_trials=20)  # @inspect value"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -5.2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 61,
          "function_name": "review_rl",
          "code": "text(\"Various flavors of values (expected utility):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Various flavors of values (expected utility):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 62,
          "function_name": "review_rl",
          "code": "text(\"- V_\u03c0(s): value of following policy \u03c0 from state s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- V_\u03c0(s): value of following policy \u03c0 from state s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 63,
          "function_name": "review_rl",
          "code": "text(\"- Q_\u03c0(s, a): value of taking action a in state s, and then following policy \u03c0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q_\u03c0(s, a): value of taking action a in state s, and then following policy \u03c0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 64,
          "function_name": "review_rl",
          "code": "text(\"- V`*`(s): value of following the optimal policy from state s\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- V`*`(s): value of following the optimal policy from state s",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 65,
          "function_name": "review_rl",
          "code": "text(\"- Q`*`(s, a): value of taking action a in state s, and then following the optimal policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q`*`(s, a): value of taking action a in state s, and then following the optimal policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 67,
          "function_name": "review_rl",
          "code": "text(\"To model or not to model?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "To model or not to model?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 68,
          "function_name": "review_rl",
          "code": "text(\"- Model-based: estimate the MDP, then compute the optimal policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-based: estimate the MDP, then compute the optimal policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 69,
          "function_name": "review_rl",
          "code": "text(\"- Value-based (model-free): don't estimate the MDP, just estimate Q-values directly\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Value-based (model-free): don't estimate the MDP, just estimate Q-values directly",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 71,
          "function_name": "review_rl",
          "code": "text(\"General form for model-free methods (for model-free Monte Carlo, SARSA, Q-learning):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "General form for model-free methods (for model-free Monte Carlo, SARSA, Q-learning):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 72,
          "function_name": "review_rl",
          "code": "text(\"- Current model estimate: Q(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Current model estimate: Q(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 73,
          "function_name": "review_rl",
          "code": "text(\"- Target (estimate of utility)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Target (estimate of utility)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 74,
          "function_name": "review_rl",
          "code": "text(\"Update: Q(s, a) \u2190 Q(s, a) + learning_rate * (target - Q(s, a))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Update: Q(s, a) \u2190 Q(s, a) + learning_rate * (target - Q(s, a))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 76,
          "function_name": "review_rl",
          "code": "text(\"On-policy (Q_\u03c0) vs. off-policy (Q`*`):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "On-policy (Q_\u03c0) vs. off-policy (Q`*`):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 77,
          "function_name": "review_rl",
          "code": "text(\"Follow exploration policy \u03c0 to generate rollouts\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Follow exploration policy \u03c0 to generate rollouts",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 78,
          "function_name": "review_rl",
          "code": "text(\"...to estimate Q-values of an estimation policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...to estimate Q-values of an estimation policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 79,
          "function_name": "review_rl",
          "code": "text(\"- On-policy: exploration policy = estimation policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- On-policy: exploration policy = estimation policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 80,
          "function_name": "review_rl",
          "code": "text(\"- Off-policy: exploration policy \u2260 estimation policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Off-policy: exploration policy \u2260 estimation policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 82,
          "function_name": "review_rl",
          "code": "text(\"What to use as targets?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What to use as targets?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 83,
          "function_name": "review_rl",
          "code": "text(\"- Full rollouts: use actual utility (discounted sum of rewards)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Full rollouts: use actual utility (discounted sum of rewards)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 84,
          "function_name": "review_rl",
          "code": "text(\"- Bootstrapping: immediate reward + estimate of future reward\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Bootstrapping: immediate reward + estimate of future reward",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 86,
          "function_name": "review_rl",
          "code": "text(\"Algorithms:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Algorithms:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 87,
          "function_name": "review_rl",
          "code": "text(\"- Model-based value iteration: estimate the MDP then compute the optimal policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-based value iteration: estimate the MDP then compute the optimal policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 88,
          "function_name": "review_rl",
          "code": "text(\"- Model-free Monte Carlo: estimate Q_\u03c0(s, a) from full rollouts (on-policy): target = utility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-free Monte Carlo: estimate Q_\u03c0(s, a) from full rollouts (on-policy): target = utility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 89,
          "function_name": "review_rl",
          "code": "text(\"- SARSA: estimate Q_\u03c0(s, a) as you rollout (on-policy, bootstrapping): target = r + \u03b3 * Q(s', a'), a' = \u03c0(s')\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- SARSA: estimate Q_\u03c0(s, a) as you rollout (on-policy, bootstrapping): target = r + \u03b3 * Q(s', a'), a' = \u03c0(s')",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 90,
          "function_name": "review_rl",
          "code": "text(f\"- Q-learning: estimate Q`*`(s, a) (off-policy, bootstrapping): target = r + \u03b3 * max_a' Q`*`(s', a')\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q-learning: estimate Q`*`(s, a) (off-policy, bootstrapping): target = r + \u03b3 * max_a' Q`*`(s', a')",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 92,
          "function_name": "review_rl",
          "code": "text(\"So far, we are estimating a Q-value for each state/action (s, a) pair.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So far, we are estimating a Q-value for each state/action (s, a) pair.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 93,
          "function_name": "review_rl",
          "code": "text(\"This is the **tabular** setting (we keep a lookup table for each state/action pair).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is the **tabular** setting (we keep a lookup table for each state/action pair).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 95,
          "function_name": "review_rl",
          "code": "text(\"Examples of states in real-world settings:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Examples of states in real-world settings:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 96,
          "function_name": "review_rl",
          "code": "text(\"- State is an image (e.g., for learning a robot policy)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- State is an image (e.g., for learning a robot policy)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 97,
          "function_name": "review_rl",
          "code": "image(\"https://rail.eecs.berkeley.edu/datasets/bridge_release/raw/bridge_data_v1/berkeley/toykitchen2/put_potato_in_pot_cardboard_fence/2022-01-28_11-25-48/raw/traj_group0/traj13/images0/im_0.jpg\", width=200)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-61641e1a53e519f559d90fbd7d5bca73-https_rail_eecs_berkeley_edu_datasets_bridge_release_raw_bridge_data_v1_berkeley_toykitchen2_put_potato_in_pot_cardboard_fence_2022-01-28_11-25-48_raw_traj_group0_traj13_images0_im_0_jpg",
          "style": {
            "width": 200
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 98,
          "function_name": "review_rl",
          "code": "text(\"- State is a sentence (e.g., for theorem proving)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- State is a sentence (e.g., for theorem proving)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 99,
          "function_name": "review_rl",
          "code": "text(\"*Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations...*\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "*Let $x,y$ and $z$ be positive real numbers that satisfy the following system of equations...*",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 101,
          "function_name": "review_rl",
          "code": "text(\"How do we handle these huge state spaces (all possible images or sentences)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "How do we handle these huge state spaces (all possible images or sentences)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 18,
          "function_name": "main",
          "code": "review_rl()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 104,
          "function_name": "function_approximation",
          "code": "def function_approximation():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 105,
          "function_name": "function_approximation",
          "code": "text(\"Tabular setting: lookup table for Q(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Tabular setting: lookup table for Q(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 106,
          "function_name": "function_approximation",
          "code": "text(\"Function approximation: Q_\u03b8(s, a) is a function with parameters \u03b8\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Function approximation: Q_\u03b8(s, a) is a function with parameters \u03b8",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 108,
          "function_name": "function_approximation",
          "code": "text(\"Design decisions:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Design decisions:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 109,
          "function_name": "function_approximation",
          "code": "text(\"1. What are the possible functions Q_\u03b8(s, a)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. What are the possible functions Q_\u03b8(s, a)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 110,
          "function_name": "function_approximation",
          "code": "text(\"2. What do we determine whether Q_\u03b8(s, a) is good? (loss function)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. What do we determine whether Q_\u03b8(s, a) is good? (loss function)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 111,
          "function_name": "function_approximation",
          "code": "text(\"3. How do we reduce the loss function? (optimization algorithm)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. How do we reduce the loss function? (optimization algorithm)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 112,
          "function_name": "function_approximation",
          "code": "text(\"Should seem familiar from the machine learning lecture!\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Should seem familiar from the machine learning lecture!",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 114,
          "function_name": "function_approximation",
          "code": "text(\"1. What are the possible functions Q_\u03b8(s, a)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. What are the possible functions Q_\u03b8(s, a)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 115,
          "function_name": "function_approximation",
          "code": "text(\"- First, map the state and action to a vector of features \u03c6(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- First, map the state and action to a vector of features \u03c6(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 116,
          "function_name": "function_approximation",
          "code": "text(\"- Then, we could use a linear function: Q_\u03b8(s, a) = \u03c6(s, a) * \u03b8\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Then, we could use a linear function: Q_\u03b8(s, a) = \u03c6(s, a) * \u03b8",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 117,
          "function_name": "function_approximation",
          "code": "text(\"- Or a multi-layer perceptron (MLP): Q_\u03b8(s, a) = MLP_\u03b8(\u03c6(s, a))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Or a multi-layer perceptron (MLP): Q_\u03b8(s, a) = MLP_\u03b8(\u03c6(s, a))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 119,
          "function_name": "function_approximation",
          "code": "text(\"Let's define the same environment as before:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's define the same environment as before:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 120,
          "function_name": "function_approximation",
          "code": "mdp = FlakyTramMDP(num_locs=6, failure_prob=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 121,
          "function_name": "function_approximation",
          "code": "set_random_seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 123,
          "function_name": "function_approximation",
          "code": "text(\"For the agent, use a parameterized Q-value function (but for simplicity, simulate the tabular setting):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "For the agent, use a parameterized Q-value function (but for simplicity, simulate the tabular setting):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 124,
          "function_name": "function_approximation",
          "code": "policy = partial(walk_tram_policy, mdp.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 126,
          "function_name": "function_approximation",
          "code": "exploration_policy=policy, epsilon=0.4, discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 171,
          "function_name": "__init__",
          "code": "def __init__(self, num_locs: int, actions: list[str],"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 173,
          "function_name": "__init__",
          "code": "self.num_locs = num_locs"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 174,
          "function_name": "__init__",
          "code": "self.actions = actions"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 175,
          "function_name": "__init__",
          "code": "self.num_features = num_locs * len(actions)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 177,
          "function_name": "__init__",
          "code": "self.exploration_policy = exploration_policy"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 178,
          "function_name": "__init__",
          "code": "self.epsilon = epsilon"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 179,
          "function_name": "__init__",
          "code": "self.discount = discount"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 180,
          "function_name": "__init__",
          "code": "self.learning_rate = learning_rate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 181,
          "function_name": "__init__",
          "code": "self.model = nn.Linear(self.num_features, 1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 182,
          "function_name": "__init__",
          "code": "self.optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 125,
          "function_name": "function_approximation",
          "code": "rl = ParameterizedQLearning(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"],"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 128,
          "function_name": "function_approximation",
          "code": "text(\"Define feature vector \u03c6(s, a) for each state and action:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define feature vector \u03c6(s, a) for each state and action:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 129,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"walk\")  # @inspect phi"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 129,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"walk\")  # @inspect phi"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 185,
          "function_name": "phi",
          "code": "def phi(self, state: int, action: str) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 129,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"walk\")  # @inspect phi"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 187,
          "function_name": "phi",
          "code": "index = (state - 1) * len(self.actions) + self.actions.index(action)  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "int",
          "contents": 0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 129,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"walk\")  # @inspect phi"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 189,
          "function_name": "phi",
          "code": "vector = one_hot(index, self.num_features)  # @inspect vector"
        }
      ],
      "env": {
        "vector": {
          "type": "torch.Tensor",
          "contents": [
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 129,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"walk\")  # @inspect phi"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 190,
          "function_name": "phi",
          "code": "return vector"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 129,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"walk\")  # @inspect phi"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 130,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=1, action=\"tram\")  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 131,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=2, action=\"walk\")  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 132,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=2, action=\"tram\")  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            0.0,
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 133,
          "function_name": "function_approximation",
          "code": "phi = rl.phi(state=6, action=\"tram\")  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            1.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 135,
          "function_name": "function_approximation",
          "code": "text(\"Define Q-value function Q_\u03b8(s, a) = \u03c6(s, a) * \u03b8:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Define Q-value function Q_\u03b8(s, a) = \u03c6(s, a) * \u03b8:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 136,
          "function_name": "function_approximation",
          "code": "value = rl.Q(state=1, action=\"walk\")  # @inspect value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 136,
          "function_name": "function_approximation",
          "code": "value = rl.Q(state=1, action=\"walk\")  # @inspect value"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 193,
          "function_name": "Q",
          "code": "def Q(self, state: int, action: str) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 136,
          "function_name": "function_approximation",
          "code": "value = rl.Q(state=1, action=\"walk\")  # @inspect value"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 196,
          "function_name": "Q",
          "code": "phi = self.phi(state, action)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            12
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 136,
          "function_name": "function_approximation",
          "code": "value = rl.Q(state=1, action=\"walk\")  # @inspect value"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 199,
          "function_name": "Q",
          "code": "value = self.model(phi)  # @inspect value"
        }
      ],
      "env": {
        "value": {
          "type": "torch.Tensor",
          "contents": [
            0.25417596101760864
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 136,
          "function_name": "function_approximation",
          "code": "value = rl.Q(state=1, action=\"walk\")  # @inspect value"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 201,
          "function_name": "Q",
          "code": "return value"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 136,
          "function_name": "function_approximation",
          "code": "value = rl.Q(state=1, action=\"walk\")  # @inspect value"
        }
      ],
      "env": {
        "value": {
          "type": "torch.Tensor",
          "contents": [
            0.25417596101760864
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 138,
          "function_name": "function_approximation",
          "code": "text(\"Compute policy \u03c0(s) = argmax_a Q_\u03b8(s, a):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Compute policy \u03c0(s) = argmax_a Q_\u03b8(s, a):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 204,
          "function_name": "pi",
          "code": "def pi(self, state: int) -> str:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 207,
          "function_name": "pi",
          "code": "q_values = {action: self.Q(state, action) for action in self.actions}  # @inspect q_values @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 207,
          "function_name": "pi",
          "code": "q_values = {action: self.Q(state, action) for action in self.actions}  # @inspect q_values @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 207,
          "function_name": "<dictcomp>",
          "code": "q_values = {action: self.Q(state, action) for action in self.actions}  # @inspect q_values @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 207,
          "function_name": "pi",
          "code": "q_values = {action: self.Q(state, action) for action in self.actions}  # @inspect q_values @stepover"
        }
      ],
      "env": {
        "q_values": {
          "type": "dict",
          "contents": {
            "walk": {
              "type": "torch.Tensor",
              "contents": [
                0.25417596101760864
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            },
            "tram": {
              "type": "torch.Tensor",
              "contents": [
                -0.021982617676258087
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 210,
          "function_name": "pi",
          "code": "action = max(q_values.keys(), key=lambda k: q_values[k].item())  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 212,
          "function_name": "pi",
          "code": "return action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 139,
          "function_name": "function_approximation",
          "code": "action = rl.pi(state=1)  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 141,
          "function_name": "function_approximation",
          "code": "text(\"Using these pieces, the agent interacts with the environment:\")  # @clear phi action value"
        }
      ],
      "env": {
        "phi": null,
        "action": null,
        "value": null
      },
      "renderings": [
        {
          "type": "markdown",
          "data": "Using these pieces, the agent interacts with the environment:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 142,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 142,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 215,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 142,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 217,
          "function_name": "get_action",
          "code": "if torch.rand(1).item() < self.epsilon:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 142,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 219,
          "function_name": "get_action",
          "code": "return self.exploration_policy(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 142,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 225,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 227,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 230,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action @stepover"
        }
      ],
      "env": {
        "next_action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 231,
          "function_name": "incorporate_feedback",
          "code": "target = reward + self.discount * self.Q(next_state, next_action)  # @inspect target @stepover"
        }
      ],
      "env": {
        "target": {
          "type": "torch.Tensor",
          "contents": [
            -0.7590725421905518
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 234,
          "function_name": "incorporate_feedback",
          "code": "value = self.Q(state, action)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "torch.Tensor",
          "contents": [
            0.25417596101760864
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 237,
          "function_name": "incorporate_feedback",
          "code": "loss = (value - target) ** 2  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "torch.Tensor",
          "contents": [
            1.02667236328125
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 240,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.zero_grad()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 241,
          "function_name": "incorporate_feedback",
          "code": "loss.backward()  # Compute gradients"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 242,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.step()  # Update the parameters @inspect self"
        }
      ],
      "env": {
        "self": {
          "type": "policy_gradient.ParameterizedQLearning",
          "contents": {
            "weight": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                [
                  -0.05390601605176926,
                  -0.12741491198539734,
                  -0.05596299096941948,
                  0.33814483880996704,
                  -0.2717694044113159,
                  0.17312346398830414,
                  -0.05938778072595596,
                  0.1468617022037506,
                  0.040130745619535446,
                  -0.035346582531929016,
                  0.08006719499826431,
                  0.01424085907638073
                ]
              ],
              "dtype": "torch.float32",
              "shape": [
                1,
                12
              ]
            },
            "bias": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                0.10543229430913925
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 143,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 144,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=2)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 144,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 215,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 144,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 217,
          "function_name": "get_action",
          "code": "if torch.rand(1).item() < self.epsilon:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 144,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 222,
          "function_name": "get_action",
          "code": "return self.pi(state)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 144,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=2)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 225,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 227,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 230,
          "function_name": "incorporate_feedback",
          "code": "next_action = self.pi(next_state)  # @inspect next_action @stepover"
        }
      ],
      "env": {
        "next_action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 231,
          "function_name": "incorporate_feedback",
          "code": "target = reward + self.discount * self.Q(next_state, next_action)  # @inspect target @stepover"
        }
      ],
      "env": {
        "target": {
          "type": "torch.Tensor",
          "contents": [
            -0.7214442491531372
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 234,
          "function_name": "incorporate_feedback",
          "code": "value = self.Q(state, action)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "torch.Tensor",
          "contents": [
            0.04946930333971977
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 237,
          "function_name": "incorporate_feedback",
          "code": "loss = (value - target) ** 2  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "torch.Tensor",
          "contents": [
            0.5943076610565186
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 240,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.zero_grad()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 241,
          "function_name": "incorporate_feedback",
          "code": "loss.backward()  # Compute gradients"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 242,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.step()  # Update the parameters @inspect self"
        }
      ],
      "env": {
        "self": {
          "type": "policy_gradient.ParameterizedQLearning",
          "contents": {
            "weight": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                [
                  -0.05390601605176926,
                  -0.12741491198539734,
                  -0.21014569699764252,
                  0.33814483880996704,
                  -0.2717694044113159,
                  0.32730618119239807,
                  -0.05938778072595596,
                  0.1468617022037506,
                  0.040130745619535446,
                  -0.035346582531929016,
                  0.08006719499826431,
                  0.01424085907638073
                ]
              ],
              "dtype": "torch.float32",
              "shape": [
                1,
                12
              ]
            },
            "bias": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                0.10543229430913925
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 145,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=2, action=\"walk\", reward=-1, next_state=3, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 146,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=3)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 146,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=3)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 215,
          "function_name": "get_action",
          "code": "def get_action(self, state: Any) -> Any:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 146,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=3)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 217,
          "function_name": "get_action",
          "code": "if torch.rand(1).item() < self.epsilon:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 146,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=3)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 222,
          "function_name": "get_action",
          "code": "return self.pi(state)  # @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 146,
          "function_name": "function_approximation",
          "code": "rl.get_action(state=3)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 225,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: Any, action: Any, reward: Any, next_state: Any, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 227,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 228,
          "function_name": "incorporate_feedback",
          "code": "target = reward  # @inspect target"
        }
      ],
      "env": {
        "target": {
          "type": "int",
          "contents": -2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 234,
          "function_name": "incorporate_feedback",
          "code": "value = self.Q(state, action)  # @inspect value @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "torch.Tensor",
          "contents": [
            0.4327384829521179
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 237,
          "function_name": "incorporate_feedback",
          "code": "loss = (value - target) ** 2  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "torch.Tensor",
          "contents": [
            5.918216705322266
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 240,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.zero_grad()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 241,
          "function_name": "incorporate_feedback",
          "code": "loss.backward()  # Compute gradients"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 242,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.step()  # Update the parameters @inspect self"
        }
      ],
      "env": {
        "self": {
          "type": "policy_gradient.ParameterizedQLearning",
          "contents": {
            "weight": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                [
                  -0.05390601605176926,
                  -0.12741491198539734,
                  -0.21014569699764252,
                  0.33814483880996704,
                  -0.2717694044113159,
                  -0.15924152731895447,
                  -0.05938778072595596,
                  0.1468617022037506,
                  0.040130745619535446,
                  -0.035346582531929016,
                  0.08006719499826431,
                  0.01424085907638073
                ]
              ],
              "dtype": "torch.float32",
              "shape": [
                1,
                12
              ]
            },
            "bias": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                -0.3811154067516327
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 147,
          "function_name": "function_approximation",
          "code": "rl.incorporate_feedback(state=3, action=\"tram\", reward=-2, next_state=6, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 149,
          "function_name": "function_approximation",
          "code": "text(\"Now let's run this agent for multiple rollouts:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now let's run this agent for multiple rollouts:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 150,
          "function_name": "function_approximation",
          "code": "value = simulate(mdp, rl, num_trials=100)  # @inspect value rl @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -7.81,
          "dtype": null,
          "shape": null
        },
        "rl": {
          "type": "policy_gradient.ParameterizedQLearning",
          "contents": {
            "weight": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                [
                  -1.0049307346343994,
                  -1.8772765398025513,
                  -0.4960387349128723,
                  -0.027683349326252937,
                  0.6914286017417908,
                  -0.5949492454528809,
                  -0.5191930532455444,
                  1.891168236732483,
                  0.480899453163147,
                  -0.035346582531929016,
                  0.08006719499826431,
                  0.01424085907638073
                ]
              ],
              "dtype": "torch.float32",
              "shape": [
                1,
                12
              ]
            },
            "bias": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                -1.4809625148773193
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 152,
          "function_name": "function_approximation",
          "code": "text(\"We can extract the current optimal policy \u03c0_\u03b8(s) of the agent:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We can extract the current optimal policy \u03c0_\u03b8(s) of the agent:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 153,
          "function_name": "function_approximation",
          "code": "states = range(1, mdp.num_locs + 1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 154,
          "function_name": "function_approximation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 154,
          "function_name": "function_approximation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 154,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 154,
          "function_name": "function_approximation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {
        "pi": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "3": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "5": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "6": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 155,
          "function_name": "function_approximation",
          "code": "text(\"And the corresponding values V_\u03b8(s) = Q_\u03b8(s, \u03c0_\u03b8(s)):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "And the corresponding values V_\u03b8(s) = Q_\u03b8(s, \u03c0_\u03b8(s)):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 156,
          "function_name": "function_approximation",
          "code": "V = {state: rl.Q(state, pi[state]) for state in states}  # @inspect V @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 156,
          "function_name": "function_approximation",
          "code": "V = {state: rl.Q(state, pi[state]) for state in states}  # @inspect V @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 156,
          "function_name": "<dictcomp>",
          "code": "V = {state: rl.Q(state, pi[state]) for state in states}  # @inspect V @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 156,
          "function_name": "function_approximation",
          "code": "V = {state: rl.Q(state, pi[state]) for state in states}  # @inspect V @stepover"
        }
      ],
      "env": {
        "V": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "torch.Tensor",
              "contents": [
                -2.4858932495117188
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            },
            "2": {
              "type": "torch.Tensor",
              "contents": [
                -1.5086458921432495
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            },
            "3": {
              "type": "torch.Tensor",
              "contents": [
                -0.7895339131355286
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            },
            "4": {
              "type": "torch.Tensor",
              "contents": [
                0.4102057218551636
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            },
            "5": {
              "type": "torch.Tensor",
              "contents": [
                -1.0000630617141724
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            },
            "6": {
              "type": "torch.Tensor",
              "contents": [
                -1.400895357131958
              ],
              "dtype": "torch.float32",
              "shape": [
                1
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 158,
          "function_name": "function_approximation",
          "code": "text(\"Let's compare with the true values by solving the MDP:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's compare with the true values by solving the MDP:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 159,
          "function_name": "function_approximation",
          "code": "result = value_iteration(mdp)  # @inspect result.values result.pi"
        }
      ],
      "env": {
        "result.values": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "numpy.float64",
              "contents": -4.222232,
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "numpy.float64",
              "contents": -3.2222232,
              "dtype": null,
              "shape": null
            },
            "3": {
              "type": "numpy.float64",
              "contents": -2.22222232,
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "numpy.int64",
              "contents": -2,
              "dtype": null,
              "shape": null
            },
            "5": {
              "type": "numpy.int64",
              "contents": -1,
              "dtype": null,
              "shape": null
            },
            "6": {
              "type": "int",
              "contents": 0,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        },
        "result.pi": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "3": {
              "type": "str",
              "contents": "tram",
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "5": {
              "type": "str",
              "contents": "walk",
              "dtype": null,
              "shape": null
            },
            "6": {
              "type": "NoneType",
              "contents": "None",
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 160,
          "function_name": "function_approximation",
          "code": "text(\"It's in the ballpark, and more accurate for more visited states.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "It's in the ballpark, and more accurate for more visited states.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 162,
          "function_name": "function_approximation",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 163,
          "function_name": "function_approximation",
          "code": "text(\"- Function approximation: parameterize Q_\u03b8(s, a) by some \u03b8\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Function approximation: parameterize Q_\u03b8(s, a) by some \u03b8",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 164,
          "function_name": "function_approximation",
          "code": "text(\"- Map states and actions into a vector of features \u03c6(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Map states and actions into a vector of features \u03c6(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 165,
          "function_name": "function_approximation",
          "code": "text(\"- Define either a linear function or an MLP to map \u03c6(s, a) \u2192 Q_\u03b8(s, a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Define either a linear function or an MLP to map \u03c6(s, a) \u2192 Q_\u03b8(s, a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 166,
          "function_name": "function_approximation",
          "code": "text(\"- Define squared loss between Q_\u03b8(s, a) and target (r + \u03b3 * max_a' Q_\u03b8(s', a'))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Define squared loss between Q_\u03b8(s, a) and target (r + \u03b3 * max_a' Q_\u03b8(s', a'))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 167,
          "function_name": "function_approximation",
          "code": "text(\"- Q-learning: take a gradient of the loss with respect to \u03b8 and take a step\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Q-learning: take a gradient of the loss with respect to \u03b8 and take a step",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 20,
          "function_name": "main",
          "code": "function_approximation()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 248,
          "function_name": "policy_gradient",
          "code": "def policy_gradient():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 249,
          "function_name": "policy_gradient",
          "code": "text(\"Model-based: estimate the MDP \u2192 policy \u03c0(s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Model-based: estimate the MDP \u2192 policy \u03c0(s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 250,
          "function_name": "policy_gradient",
          "code": "text(\"Value-based: estimate Q-values Q(s, a) \u2192 policy \u03c0(s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Value-based: estimate Q-values Q(s, a) \u2192 policy \u03c0(s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 252,
          "function_name": "policy_gradient",
          "code": "text(\"Why not **directly** estimate the policy \u03c0(s)?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Why not **directly** estimate the policy \u03c0(s)?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 253,
          "function_name": "policy_gradient",
          "code": "text(\"Think of the policy as a classifier (input s \u2192 output a)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Think of the policy as a classifier (input s \u2192 output a)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 255,
          "function_name": "policy_gradient",
          "code": "text(\"Recall that a classifier is a hard function to optimize directly\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall that a classifier is a hard function to optimize directly",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 256,
          "function_name": "policy_gradient",
          "code": "text(\"...so we optimize over a probabilistic classifier \u03c0(a | s).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...so we optimize over a probabilistic classifier \u03c0(a | s).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 265,
          "function_name": "imitation_learning",
          "code": "def imitation_learning():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 266,
          "function_name": "imitation_learning",
          "code": "text(\"If we had demonstrations of the policy, it'd be easy.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "If we had demonstrations of the policy, it'd be easy.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 268,
          "function_name": "imitation_learning",
          "code": "text(\"Suppose we have a rollout \ud835\udf0f:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Suppose we have a rollout \ud835\udf0f:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 269,
          "function_name": "imitation_learning",
          "code": "rollout = Rollout(steps=["
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 270,
          "function_name": "imitation_learning",
          "code": "Step(action=\"walk\", prob=1, reward=-1, state=2),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 271,
          "function_name": "imitation_learning",
          "code": "Step(action=\"walk\", prob=1, reward=-1, state=3),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 272,
          "function_name": "imitation_learning",
          "code": "Step(action=\"tram\", prob=1, reward=-2, state=6),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 269,
          "function_name": "imitation_learning",
          "code": "rollout = Rollout(steps=["
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 273,
          "function_name": "imitation_learning",
          "code": "], discount=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 269,
          "function_name": "imitation_learning",
          "code": "rollout = Rollout(steps=["
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 274,
          "function_name": "imitation_learning",
          "code": "text(\"We create the following examples:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We create the following examples:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 276,
          "function_name": "imitation_learning",
          "code": "Example(input=1, output=\"walk\"),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 277,
          "function_name": "imitation_learning",
          "code": "Example(input=2, output=\"walk\"),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 278,
          "function_name": "imitation_learning",
          "code": "Example(input=3, output=\"tram\"),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 275,
          "function_name": "imitation_learning",
          "code": "examples = ["
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 280,
          "function_name": "imitation_learning",
          "code": "text(\"J(\u03b8) = \u03a3_i log \u03c0_\u03b8(a_i | s_i)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "J(\u03b8) = \u03a3_i log \u03c0_\u03b8(a_i | s_i)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 281,
          "function_name": "imitation_learning",
          "code": "text(\"This is called **imitation learning**.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is called **imitation learning**.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 283,
          "function_name": "imitation_learning",
          "code": "text(\"Examples:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Examples:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 284,
          "function_name": "imitation_learning",
          "code": "text(\"- Robotics: teleoperation\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Robotics: teleoperation",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 285,
          "function_name": "imitation_learning",
          "code": "image(\"https://news.stanford.edu/__data/assets/image/0028/134857/mobilealoha_2.jpg\", width=200)"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "image",
          "data": "var/files/image-9dd7e08003008418614edeb931b6ab96-https_news_stanford_edu___data_assets_image_0028_134857_mobilealoha_2_jpg",
          "style": {
            "width": 200
          },
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 286,
          "function_name": "imitation_learning",
          "code": "text(\"- Math: human-written solution to a math problem\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Math: human-written solution to a math problem",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 258,
          "function_name": "policy_gradient",
          "code": "imitation_learning()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 295,
          "function_name": "policy_gradient_math",
          "code": "def policy_gradient_math():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 296,
          "function_name": "policy_gradient_math",
          "code": "text(\"But in reinforcement learning, we don't have demonstrations.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "But in reinforcement learning, we don't have demonstrations.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 297,
          "function_name": "policy_gradient_math",
          "code": "text(\"We have a reward function...\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We have a reward function...",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 298,
          "function_name": "policy_gradient_math",
          "code": "text(\"So what do we do?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So what do we do?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 300,
          "function_name": "policy_gradient_math",
          "code": "text(\"Suppose we have a rollout (trajectory, episode):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Suppose we have a rollout (trajectory, episode):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 301,
          "function_name": "policy_gradient_math",
          "code": "text(\"\ud835\udf0f = (s_0, a_1, r_1, s_1, a_2, r_2, s_2, a_3, r_3, s_3):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\ud835\udf0f = (s_0, a_1, r_1, s_1, a_2, r_2, s_2, a_3, r_3, s_3):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 302,
          "function_name": "policy_gradient_math",
          "code": "rollout = Rollout(steps=[  # @inspect rollout"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 303,
          "function_name": "policy_gradient_math",
          "code": "Step(action=\"walk\", prob=1, reward=-1, state=2),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 304,
          "function_name": "policy_gradient_math",
          "code": "Step(action=\"walk\", prob=1, reward=-1, state=3),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 305,
          "function_name": "policy_gradient_math",
          "code": "Step(action=\"tram\", prob=1, reward=-2, state=6),"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 302,
          "function_name": "policy_gradient_math",
          "code": "rollout = Rollout(steps=[  # @inspect rollout"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 306,
          "function_name": "policy_gradient_math",
          "code": "], discount=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 302,
          "function_name": "policy_gradient_math",
          "code": "rollout = Rollout(steps=[  # @inspect rollout"
        }
      ],
      "env": {
        "rollout": {
          "type": "mdp.Rollout",
          "contents": {
            "steps": {
              "type": "list",
              "contents": [
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 2,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "walk",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -1,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 3,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                },
                {
                  "type": "mdp.Step",
                  "contents": {
                    "action": {
                      "type": "str",
                      "contents": "tram",
                      "dtype": null,
                      "shape": null
                    },
                    "prob": {
                      "type": "int",
                      "contents": 1,
                      "dtype": null,
                      "shape": null
                    },
                    "reward": {
                      "type": "int",
                      "contents": -2,
                      "dtype": null,
                      "shape": null
                    },
                    "state": {
                      "type": "int",
                      "contents": 6,
                      "dtype": null,
                      "shape": null
                    }
                  },
                  "dtype": null,
                  "shape": null
                }
              ],
              "dtype": null,
              "shape": null
            },
            "discount": {
              "type": "int",
              "contents": 1,
              "dtype": null,
              "shape": null
            },
            "utility": {
              "type": "int",
              "contents": -4,
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 307,
          "function_name": "policy_gradient_math",
          "code": "text(\"Each rollout produces a utility.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Each rollout produces a utility.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 309,
          "function_name": "policy_gradient_math",
          "code": "text(\"What is the probability of a rollout?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "What is the probability of a rollout?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 310,
          "function_name": "policy_gradient_math",
          "code": "text(\"p(\ud835\udf0f) = p(s_0) * \u03c0_\u03b8(a_1 | s_0) * T(s_0, a_1, s_1) * \u03c0_\u03b8(a_2 | s_1) * T(s_1, a_2, s_2) * \u03c0_\u03b8(a_3 | s_2) * T(s_2, a_3, s_3)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "p(\ud835\udf0f) = p(s_0) * \u03c0_\u03b8(a_1 | s_0) * T(s_0, a_1, s_1) * \u03c0_\u03b8(a_2 | s_1) * T(s_1, a_2, s_2) * \u03c0_\u03b8(a_3 | s_2) * T(s_2, a_3, s_3)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 311,
          "function_name": "policy_gradient_math",
          "code": "text(\"Components:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Components:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 312,
          "function_name": "policy_gradient_math",
          "code": "text(\"- p(s_0): probability of starting in state s_0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- p(s_0): probability of starting in state s_0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 313,
          "function_name": "policy_gradient_math",
          "code": "text(\"- \u03c0_\u03b8(a_t | s_{t-1}): policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- \u03c0_\u03b8(a_t | s_{t-1}): policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 314,
          "function_name": "policy_gradient_math",
          "code": "text(\"- T(s_{t-1}, a_t, s_t): transition distribution\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- T(s_{t-1}, a_t, s_t): transition distribution",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 316,
          "function_name": "policy_gradient_math",
          "code": "text(\"Our goal is to maximize the expected utility of a rollout:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Our goal is to maximize the expected utility of a rollout:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 317,
          "function_name": "policy_gradient_math",
          "code": "text(\"V(\u03b8) = E_\u03b8[utility(\ud835\udf0f)] = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "V(\u03b8) = E_\u03b8[utility(\ud835\udf0f)] = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 319,
          "function_name": "policy_gradient_math",
          "code": "text(\"Let's just take the gradient of V(\u03b8) with respect to \u03b8:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's just take the gradient of V(\u03b8) with respect to \u03b8:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 320,
          "function_name": "policy_gradient_math",
          "code": "text(\"\u2207_\u03b8 V(\u03b8) = \u2207_\u03b8 E_\u03b8[utility(\ud835\udf0f)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207_\u03b8 V(\u03b8) = \u2207_\u03b8 E_\u03b8[utility(\ud835\udf0f)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 321,
          "function_name": "policy_gradient_math",
          "code": "text(\"\u2207_\u03b8 V(\u03b8) = \u2207_\u03b8 \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207_\u03b8 V(\u03b8) = \u2207_\u03b8 \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 322,
          "function_name": "policy_gradient_math",
          "code": "text(\"\u2207_\u03b8 V(\u03b8) = \u03a3_\ud835\udf0f \u2207_\u03b8 p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207_\u03b8 V(\u03b8) = \u03a3_\ud835\udf0f \u2207_\u03b8 p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 323,
          "function_name": "policy_gradient_math",
          "code": "text(\"\u2207_\u03b8 V(\u03b8) = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * \u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207_\u03b8 V(\u03b8) = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * \u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 324,
          "function_name": "policy_gradient_math",
          "code": "text(\"\u2207_\u03b8 V(\u03b8) = E_\u03b8[\u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207_\u03b8 V(\u03b8) = E_\u03b8[\u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 325,
          "function_name": "policy_gradient_math",
          "code": "text(\"This is the **policy gradient theorem (identity)**.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is the **policy gradient theorem (identity)**.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 327,
          "function_name": "policy_gradient_math",
          "code": "text(\"Whenever you see E_\u03b8[...], you can replace it with a sample \ud835\udf0f ~ p_\u03b8(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Whenever you see E_\u03b8[...], you can replace it with a sample \ud835\udf0f ~ p_\u03b8(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 328,
          "function_name": "policy_gradient_math",
          "code": "text(\"...define the objective function J(\u03b8, \ud835\udf0f) = log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...define the objective function J(\u03b8, \ud835\udf0f) = log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 329,
          "function_name": "policy_gradient_math",
          "code": "text(\"...and update \u03b8 by taking a step in the direction of \u2207_\u03b8 J(\u03b8, \ud835\udf0f).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...and update \u03b8 by taking a step in the direction of \u2207_\u03b8 J(\u03b8, \ud835\udf0f).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 331,
          "function_name": "policy_gradient_math",
          "code": "text(\"Breaking down the gradient:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Breaking down the gradient:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 332,
          "function_name": "policy_gradient_math",
          "code": "text(\"\u2207_\u03b8 J(\u03b8, \ud835\udf0f) = utility(\ud835\udf0f) * \u03a3_t \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_{t-1})\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u2207_\u03b8 J(\u03b8, \ud835\udf0f) = utility(\ud835\udf0f) * \u03a3_t \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_{t-1})",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 334,
          "function_name": "policy_gradient_math",
          "code": "text(\"This is the REINFORCE algorithm \"), link(\"https://link.springer.com/article/10.1007/BF00992696\", title=\"[Williams, 1992]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is the REINFORCE algorithm ",
          "style": {},
          "external_link": null,
          "internal_link": null
        },
        {
          "type": "link",
          "data": null,
          "style": {},
          "external_link": {
            "title": "[Williams, 1992]",
            "authors": null,
            "organization": null,
            "date": null,
            "url": "https://link.springer.com/article/10.1007/BF00992696",
            "description": null,
            "notes": null
          },
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 336,
          "function_name": "policy_gradient_math",
          "code": "text(\"Intuition:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Intuition:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 337,
          "function_name": "policy_gradient_math",
          "code": "text(\"- Just performing imitation learning on demonstrations from own policy weighted by utility.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Just performing imitation learning on demonstrations from own policy weighted by utility.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 338,
          "function_name": "policy_gradient_math",
          "code": "text(\"- If utility(\ud835\udf0f) \u2208 {0, 1} (success/failure), then this is just imitation learning on own successful demonstrations.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- If utility(\ud835\udf0f) \u2208 {0, 1} (success/failure), then this is just imitation learning on own successful demonstrations.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 340,
          "function_name": "policy_gradient_math",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 341,
          "function_name": "policy_gradient_math",
          "code": "text(\"- Objective: optimize policy directly to maximize expected utility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Objective: optimize policy directly to maximize expected utility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 342,
          "function_name": "policy_gradient_math",
          "code": "text(\"- Due to policy gradient theorem, can compute unbiased gradient\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Due to policy gradient theorem, can compute unbiased gradient",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 343,
          "function_name": "policy_gradient_math",
          "code": "text(\"- Algorithm: sample a rollout (on-policy), update using (state, action) in the rollout \ud835\udf0f, weighted by utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Algorithm: sample a rollout (on-policy), update using (state, action) in the rollout \ud835\udf0f, weighted by utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 260,
          "function_name": "policy_gradient",
          "code": "policy_gradient_math()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 346,
          "function_name": "policy_gradient_implementation",
          "code": "def policy_gradient_implementation():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 347,
          "function_name": "policy_gradient_implementation",
          "code": "mdp = FlakyTramMDP(num_locs=6, failure_prob=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 348,
          "function_name": "policy_gradient_implementation",
          "code": "set_random_seed(1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 373,
          "function_name": "__init__",
          "code": "def __init__(self, num_locs: int, actions: list[str], discount: float, learning_rate: float):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 374,
          "function_name": "__init__",
          "code": "self.num_locs = num_locs"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 375,
          "function_name": "__init__",
          "code": "self.actions = actions"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 376,
          "function_name": "__init__",
          "code": "self.discount = discount"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 377,
          "function_name": "__init__",
          "code": "self.learning_rate = learning_rate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 378,
          "function_name": "__init__",
          "code": "self.model = nn.Linear(self.num_locs, len(actions))"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 379,
          "function_name": "__init__",
          "code": "self.optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 382,
          "function_name": "__init__",
          "code": "self.start_state = None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 383,
          "function_name": "__init__",
          "code": "self.rollout: list[Step] = []"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 384,
          "function_name": "__init__",
          "code": "self.utility = 0"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 349,
          "function_name": "policy_gradient_implementation",
          "code": "rl = Reinforce(num_locs=mdp.num_locs, actions=[\"walk\", \"tram\"], discount=1, learning_rate=0.1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 350,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"Note that we don't need an explicit exploration policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Note that we don't need an explicit exploration policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 351,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"...because the stochastic policy should give us exploration.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "...because the stochastic policy should give us exploration.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 353,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"Let's try a rollout:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's try a rollout:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 397,
          "function_name": "get_action",
          "code": "def get_action(self, state: int) -> str:  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            6
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 401,
          "function_name": "get_action",
          "code": "logits = self.model(phi)  # @inspect logits"
        }
      ],
      "env": {
        "logits": {
          "type": "torch.Tensor",
          "contents": [
            0.3594591021537781,
            -0.24308179318904877
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 404,
          "function_name": "get_action",
          "code": "probs = torch.softmax(logits, dim=0)  # @inspect probs"
        }
      ],
      "env": {
        "probs": {
          "type": "torch.Tensor",
          "contents": [
            0.6462373733520508,
            0.35376259684562683
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 405,
          "function_name": "get_action",
          "code": "index = torch.multinomial(probs, num_samples=1).item()  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "int",
          "contents": 0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 408,
          "function_name": "get_action",
          "code": "action = self.actions[index]  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 409,
          "function_name": "get_action",
          "code": "return action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 354,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 412,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: int, action: str, reward: float, next_state: int, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 415,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 416,
          "function_name": "incorporate_feedback",
          "code": "self.start_state = state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 417,
          "function_name": "incorporate_feedback",
          "code": "self.utility += reward * self.discount ** len(self.rollout)  # @inspect self.utility"
        }
      ],
      "env": {
        "self.utility": {
          "type": "int",
          "contents": -1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 418,
          "function_name": "incorporate_feedback",
          "code": "self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout"
        }
      ],
      "env": {
        "self.rollout": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 420,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 355,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=1, action=\"walk\", reward=-1, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 397,
          "function_name": "get_action",
          "code": "def get_action(self, state: int) -> str:  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 386,
          "function_name": "phi",
          "code": "def phi(self, state: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 387,
          "function_name": "phi",
          "code": "index = state - 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 388,
          "function_name": "phi",
          "code": "return one_hot(index, self.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            6
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 401,
          "function_name": "get_action",
          "code": "logits = self.model(phi)  # @inspect logits"
        }
      ],
      "env": {
        "logits": {
          "type": "torch.Tensor",
          "contents": [
            -0.031088128685951233,
            0.04859904944896698
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 404,
          "function_name": "get_action",
          "code": "probs = torch.softmax(logits, dim=0)  # @inspect probs"
        }
      ],
      "env": {
        "probs": {
          "type": "torch.Tensor",
          "contents": [
            0.4800887107849121,
            0.5199112296104431
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 405,
          "function_name": "get_action",
          "code": "index = torch.multinomial(probs, num_samples=1).item()  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "int",
          "contents": 1,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 408,
          "function_name": "get_action",
          "code": "action = self.actions[index]  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "tram",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 409,
          "function_name": "get_action",
          "code": "return action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 356,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 412,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: int, action: str, reward: float, next_state: int, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 415,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 417,
          "function_name": "incorporate_feedback",
          "code": "self.utility += reward * self.discount ** len(self.rollout)  # @inspect self.utility"
        }
      ],
      "env": {
        "self.utility": {
          "type": "int",
          "contents": -3,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 418,
          "function_name": "incorporate_feedback",
          "code": "self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout"
        }
      ],
      "env": {
        "self.rollout": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 420,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 357,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=2, is_end=False)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 397,
          "function_name": "get_action",
          "code": "def get_action(self, state: int) -> str:  # @inspect state"
        }
      ],
      "env": {
        "state": {
          "type": "int",
          "contents": 2,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 400,
          "function_name": "get_action",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            6
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 401,
          "function_name": "get_action",
          "code": "logits = self.model(phi)  # @inspect logits"
        }
      ],
      "env": {
        "logits": {
          "type": "torch.Tensor",
          "contents": [
            -0.031088128685951233,
            0.04859904944896698
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 404,
          "function_name": "get_action",
          "code": "probs = torch.softmax(logits, dim=0)  # @inspect probs"
        }
      ],
      "env": {
        "probs": {
          "type": "torch.Tensor",
          "contents": [
            0.4800887107849121,
            0.5199112296104431
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 405,
          "function_name": "get_action",
          "code": "index = torch.multinomial(probs, num_samples=1).item()  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "int",
          "contents": 0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 408,
          "function_name": "get_action",
          "code": "action = self.actions[index]  # @inspect action"
        }
      ],
      "env": {
        "action": {
          "type": "str",
          "contents": "walk",
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 409,
          "function_name": "get_action",
          "code": "return action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 358,
          "function_name": "policy_gradient_implementation",
          "code": "rl.get_action(state=2)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 412,
          "function_name": "incorporate_feedback",
          "code": "def incorporate_feedback(self, state: int, action: str, reward: float, next_state: int, is_end: bool) -> None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 415,
          "function_name": "incorporate_feedback",
          "code": "if self.start_state is None:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 417,
          "function_name": "incorporate_feedback",
          "code": "self.utility += reward * self.discount ** len(self.rollout)  # @inspect self.utility"
        }
      ],
      "env": {
        "self.utility": {
          "type": "int",
          "contents": -5,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 418,
          "function_name": "incorporate_feedback",
          "code": "self.rollout.append(Step(action=action, prob=1, reward=reward, state=next_state))  # @inspect self.rollout"
        }
      ],
      "env": {
        "self.rollout": {
          "type": "list",
          "contents": [
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "walk",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -1,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 2,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            {
              "type": "mdp.Step",
              "contents": {
                "action": {
                  "type": "str",
                  "contents": "tram",
                  "dtype": null,
                  "shape": null
                },
                "prob": {
                  "type": "int",
                  "contents": 1,
                  "dtype": null,
                  "shape": null
                },
                "reward": {
                  "type": "int",
                  "contents": -2,
                  "dtype": null,
                  "shape": null
                },
                "state": {
                  "type": "int",
                  "contents": 4,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          ],
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 420,
          "function_name": "incorporate_feedback",
          "code": "if is_end:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 422,
          "function_name": "incorporate_feedback",
          "code": "loss = 0  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "int",
          "contents": 0,
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 423,
          "function_name": "incorporate_feedback",
          "code": "for i, step in enumerate(self.rollout):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 424,
          "function_name": "incorporate_feedback",
          "code": "state = self.start_state if i == 0 else self.rollout[i - 1].state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 425,
          "function_name": "incorporate_feedback",
          "code": "action = step.action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            1.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            6
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 429,
          "function_name": "incorporate_feedback",
          "code": "logits = self.model(phi) # @inspect logits"
        }
      ],
      "env": {
        "logits": {
          "type": "torch.Tensor",
          "contents": [
            0.3594591021537781,
            -0.24308179318904877
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 430,
          "function_name": "incorporate_feedback",
          "code": "cross_entropy = nn.CrossEntropyLoss()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 431,
          "function_name": "incorporate_feedback",
          "code": "target = one_hot(self.actions.index(action), len(self.actions))  # @inspect target"
        }
      ],
      "env": {
        "target": {
          "type": "torch.Tensor",
          "contents": [
            1.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 432,
          "function_name": "incorporate_feedback",
          "code": "loss += cross_entropy(logits, target)  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "torch.Tensor",
          "contents": 0.4365883469581604,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 423,
          "function_name": "incorporate_feedback",
          "code": "for i, step in enumerate(self.rollout):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 424,
          "function_name": "incorporate_feedback",
          "code": "state = self.start_state if i == 0 else self.rollout[i - 1].state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 425,
          "function_name": "incorporate_feedback",
          "code": "action = step.action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 386,
          "function_name": "phi",
          "code": "def phi(self, state: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 387,
          "function_name": "phi",
          "code": "index = state - 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 388,
          "function_name": "phi",
          "code": "return one_hot(index, self.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            6
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 429,
          "function_name": "incorporate_feedback",
          "code": "logits = self.model(phi) # @inspect logits"
        }
      ],
      "env": {
        "logits": {
          "type": "torch.Tensor",
          "contents": [
            -0.031088128685951233,
            0.04859904944896698
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 430,
          "function_name": "incorporate_feedback",
          "code": "cross_entropy = nn.CrossEntropyLoss()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 431,
          "function_name": "incorporate_feedback",
          "code": "target = one_hot(self.actions.index(action), len(self.actions))  # @inspect target"
        }
      ],
      "env": {
        "target": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 432,
          "function_name": "incorporate_feedback",
          "code": "loss += cross_entropy(logits, target)  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "torch.Tensor",
          "contents": 1.090685486793518,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 423,
          "function_name": "incorporate_feedback",
          "code": "for i, step in enumerate(self.rollout):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 424,
          "function_name": "incorporate_feedback",
          "code": "state = self.start_state if i == 0 else self.rollout[i - 1].state"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 425,
          "function_name": "incorporate_feedback",
          "code": "action = step.action"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 428,
          "function_name": "incorporate_feedback",
          "code": "phi = self.phi(state)  # @inspect phi @stepover"
        }
      ],
      "env": {
        "phi": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            6
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 429,
          "function_name": "incorporate_feedback",
          "code": "logits = self.model(phi) # @inspect logits"
        }
      ],
      "env": {
        "logits": {
          "type": "torch.Tensor",
          "contents": [
            -0.031088128685951233,
            0.04859904944896698
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 430,
          "function_name": "incorporate_feedback",
          "code": "cross_entropy = nn.CrossEntropyLoss()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 431,
          "function_name": "incorporate_feedback",
          "code": "target = one_hot(self.actions.index(action), len(self.actions))  # @inspect target"
        }
      ],
      "env": {
        "target": {
          "type": "torch.Tensor",
          "contents": [
            0.0,
            1.0
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 432,
          "function_name": "incorporate_feedback",
          "code": "loss += cross_entropy(logits, target)  # @inspect loss"
        }
      ],
      "env": {
        "loss": {
          "type": "torch.Tensor",
          "contents": 1.7447826862335205,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 423,
          "function_name": "incorporate_feedback",
          "code": "for i, step in enumerate(self.rollout):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 435,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.zero_grad()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 436,
          "function_name": "incorporate_feedback",
          "code": "loss.backward()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 437,
          "function_name": "incorporate_feedback",
          "code": "self.optimizer.step()  # @inspect self"
        }
      ],
      "env": {
        "self": {
          "type": "policy_gradient.Reinforce",
          "contents": {
            "weight": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                [
                  0.24573159217834473,
                  -0.27620968222618103,
                  -0.07914362102746964,
                  0.19161906838417053,
                  -0.3843400180339813,
                  0.2448335587978363
                ],
                [
                  -0.11936327069997787,
                  0.30371159315109253,
                  0.05675344914197922,
                  -0.04998761788010597,
                  0.11323212087154388,
                  0.02013961784541607
                ]
              ],
              "dtype": "torch.float32",
              "shape": [
                2,
                6
              ]
            },
            "bias": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                0.08846230059862137,
                -0.09845328330993652
              ],
              "dtype": "torch.float32",
              "shape": [
                2
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 440,
          "function_name": "incorporate_feedback",
          "code": "self.start_state = None"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 441,
          "function_name": "incorporate_feedback",
          "code": "self.rollout = []"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 442,
          "function_name": "incorporate_feedback",
          "code": "self.utility = 0"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 359,
          "function_name": "policy_gradient_implementation",
          "code": "rl.incorporate_feedback(state=2, action=\"tram\", reward=-2, next_state=4, is_end=True)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 361,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"Now let's run this agent for multiple rollouts:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Now let's run this agent for multiple rollouts:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 362,
          "function_name": "policy_gradient_implementation",
          "code": "value = simulate(mdp, rl, num_trials=100)  # @inspect value rl @stepover"
        }
      ],
      "env": {
        "value": {
          "type": "float",
          "contents": -5.83,
          "dtype": null,
          "shape": null
        },
        "rl": {
          "type": "policy_gradient.Reinforce",
          "contents": {
            "weight": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                [
                  0.48215335607528687,
                  0.1446770280599594,
                  -0.847517192363739,
                  0.8425390720367432,
                  -0.46491876244544983,
                  0.2448335587978363
                ],
                [
                  -0.3557850420475006,
                  -0.1171751394867897,
                  0.8251269459724426,
                  -0.7009077072143555,
                  0.19381068646907806,
                  0.02013961784541607
                ]
              ],
              "dtype": "torch.float32",
              "shape": [
                2,
                6
              ]
            },
            "bias": {
              "type": "torch.nn.parameter.Parameter",
              "contents": [
                0.5477389693260193,
                -0.5577297806739807
              ],
              "dtype": "torch.float32",
              "shape": [
                2
              ]
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 363,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"We can extract the current optimal policy \u03c0_\u03b8(s) of the agent:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "We can extract the current optimal policy \u03c0_\u03b8(s) of the agent:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 364,
          "function_name": "policy_gradient_implementation",
          "code": "states = range(1, mdp.num_locs + 1)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 390,
          "function_name": "pi",
          "code": "def pi(self, state: int) -> dict[str, float]:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 386,
          "function_name": "phi",
          "code": "def phi(self, state: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 387,
          "function_name": "phi",
          "code": "index = state - 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 388,
          "function_name": "phi",
          "code": "return one_hot(index, self.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 393,
          "function_name": "pi",
          "code": "logits = self.model(phi)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 394,
          "function_name": "pi",
          "code": "probs = torch.softmax(logits, dim=0)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 395,
          "function_name": "pi",
          "code": "return dict(zip(self.actions, probs.tolist()))"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 390,
          "function_name": "pi",
          "code": "def pi(self, state: int) -> dict[str, float]:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 386,
          "function_name": "phi",
          "code": "def phi(self, state: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 387,
          "function_name": "phi",
          "code": "index = state - 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 388,
          "function_name": "phi",
          "code": "return one_hot(index, self.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 393,
          "function_name": "pi",
          "code": "logits = self.model(phi)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 394,
          "function_name": "pi",
          "code": "probs = torch.softmax(logits, dim=0)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 395,
          "function_name": "pi",
          "code": "return dict(zip(self.actions, probs.tolist()))"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 390,
          "function_name": "pi",
          "code": "def pi(self, state: int) -> dict[str, float]:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 386,
          "function_name": "phi",
          "code": "def phi(self, state: int) -> torch.Tensor:"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 387,
          "function_name": "phi",
          "code": "index = state - 1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 388,
          "function_name": "phi",
          "code": "return one_hot(index, self.num_locs)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 392,
          "function_name": "pi",
          "code": "phi = self.phi(state)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 393,
          "function_name": "pi",
          "code": "logits = self.model(phi)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 394,
          "function_name": "pi",
          "code": "probs = torch.softmax(logits, dim=0)"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 395,
          "function_name": "pi",
          "code": "return dict(zip(self.actions, probs.tolist()))"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "<dictcomp>",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 365,
          "function_name": "policy_gradient_implementation",
          "code": "pi = {state: rl.pi(state) for state in states}  # @inspect pi @stepover"
        }
      ],
      "env": {
        "pi": {
          "type": "dict",
          "contents": {
            "1": {
              "type": "dict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": 0.8747259378433228,
                  "dtype": null,
                  "shape": null
                },
                "tram": {
                  "type": "float",
                  "contents": 0.12527401745319366,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "2": {
              "type": "dict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": 0.7969469428062439,
                  "dtype": null,
                  "shape": null
                },
                "tram": {
                  "type": "float",
                  "contents": 0.20305299758911133,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "3": {
              "type": "dict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": 0.3618888556957245,
                  "dtype": null,
                  "shape": null
                },
                "tram": {
                  "type": "float",
                  "contents": 0.6381111741065979,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "4": {
              "type": "dict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": 0.9339441657066345,
                  "dtype": null,
                  "shape": null
                },
                "tram": {
                  "type": "float",
                  "contents": 0.06605587899684906,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "5": {
              "type": "dict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": 0.6098636984825134,
                  "dtype": null,
                  "shape": null
                },
                "tram": {
                  "type": "float",
                  "contents": 0.3901363015174866,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            },
            "6": {
              "type": "dict",
              "contents": {
                "walk": {
                  "type": "float",
                  "contents": 0.7908675670623779,
                  "dtype": null,
                  "shape": null
                },
                "tram": {
                  "type": "float",
                  "contents": 0.20913247764110565,
                  "dtype": null,
                  "shape": null
                }
              },
              "dtype": null,
              "shape": null
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 367,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 368,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"- `get_action`: sample from the policy \u03c0_\u03b8(a | s)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- `get_action`: sample from the policy \u03c0_\u03b8(a | s)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 369,
          "function_name": "policy_gradient_implementation",
          "code": "text(\"- `incorporate_feedback`: update parameters on (state, action) in the rollout \ud835\udf0f, weighted by utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- `incorporate_feedback`: update parameters on (state, action) in the rollout \ud835\udf0f, weighted by utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 261,
          "function_name": "policy_gradient",
          "code": "policy_gradient_implementation()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 448,
          "function_name": "policy_gradient_enhancements",
          "code": "def policy_gradient_enhancements():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 449,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Recall the objective: maximize expected utility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Recall the objective: maximize expected utility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 450,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"V(\u03b8) = E_\u03b8[utility(\ud835\udf0f)] = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "V(\u03b8) = E_\u03b8[utility(\ud835\udf0f)] = \u03a3_\ud835\udf0f p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 452,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Policy gradient theorem: \u2207_\u03b8 V(\u03b8) = E_\u03b8[\u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Policy gradient theorem: \u2207_\u03b8 V(\u03b8) = E_\u03b8[\u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 454,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"REINFORCE is one particular (unbiased) estimate of \u2207_\u03b8 V(\u03b8):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "REINFORCE is one particular (unbiased) estimate of \u2207_\u03b8 V(\u03b8):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 455,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Sample \ud835\udf0f ~ p_\u03b8(\ud835\udf0f) and compute \u2207_\u03b8 J(\u03b8, \ud835\udf0f) = \u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sample \ud835\udf0f ~ p_\u03b8(\ud835\udf0f) and compute \u2207_\u03b8 J(\u03b8, \ud835\udf0f) = \u2207_\u03b8 log p_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 456,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"However, can we get a better estimate?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "However, can we get a better estimate?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 490,
          "function_name": "variance_reduction",
          "code": "def variance_reduction():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 491,
          "function_name": "variance_reduction",
          "code": "text(\"Consider the simple mean estimation problem:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Consider the simple mean estimation problem:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 492,
          "function_name": "variance_reduction",
          "code": "text(\"\u03bc = E[f(i)] = \u03a3_i p(i) * f(i)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "\u03bc = E[f(i)] = \u03a3_i p(i) * f(i)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 493,
          "function_name": "variance_reduction",
          "code": "probs = torch.tensor([0.1, 0.1, 0.1, 0.1])  # p(0), p(1), ... @inspect probs"
        }
      ],
      "env": {
        "probs": {
          "type": "torch.Tensor",
          "contents": [
            0.10000000149011612,
            0.10000000149011612,
            0.10000000149011612,
            0.10000000149011612
          ],
          "dtype": "torch.float32",
          "shape": [
            4
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 494,
          "function_name": "variance_reduction",
          "code": "points = torch.tensor([-4., -6., 6., 8.])  # f(0), f(1), ... @inspect points"
        }
      ],
      "env": {
        "points": {
          "type": "torch.Tensor",
          "contents": [
            -4.0,
            -6.0,
            6.0,
            8.0
          ],
          "dtype": "torch.float32",
          "shape": [
            4
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 496,
          "function_name": "variance_reduction",
          "code": "text(\"This is the true mean we want to estimate (is unknown):\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is the true mean we want to estimate (is unknown):",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 497,
          "function_name": "variance_reduction",
          "code": "mu = probs @ points  # @inspect mu"
        }
      ],
      "env": {
        "mu": {
          "type": "torch.Tensor",
          "contents": 0.40000009536743164,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 499,
          "function_name": "variance_reduction",
          "code": "text(\"An **estimator** is any random variable that tries to get close to \u03bc.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "An **estimator** is any random variable that tries to get close to \u03bc.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 500,
          "function_name": "variance_reduction",
          "code": "text(\"Each estimator has a **bias** and a **variance** (and a cost).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Each estimator has a **bias** and a **variance** (and a cost).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 502,
          "function_name": "variance_reduction",
          "code": "text(\"The simplest estimate of \u03bc:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "The simplest estimate of \u03bc:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 503,
          "function_name": "variance_reduction",
          "code": "text(\"Sample a single i ~ p and return f(i).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Sample a single i ~ p and return f(i).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 504,
          "function_name": "variance_reduction",
          "code": "def estimator1():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 508,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 508,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 504,
          "function_name": "estimator1",
          "code": "def estimator1():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 508,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 505,
          "function_name": "estimator1",
          "code": "index = torch.multinomial(probs, num_samples=1)  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "torch.Tensor",
          "contents": [
            2
          ],
          "dtype": "torch.int64",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 508,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 506,
          "function_name": "estimator1",
          "code": "estimate = points[index]  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            6.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 508,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 507,
          "function_name": "estimator1",
          "code": "return estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 508,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            6.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 509,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            8.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 510,
          "function_name": "variance_reduction",
          "code": "estimate = estimator1()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            6.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 551,
          "function_name": "evaluate_estimator",
          "code": "def evaluate_estimator(estimator):"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 553,
          "function_name": "evaluate_estimator",
          "code": "num_samples = 1000"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 554,
          "function_name": "evaluate_estimator",
          "code": "samples = torch.tensor([estimator() for _ in range(num_samples)])  # @inspect samples @stepover"
        }
      ],
      "env": {
        "samples": {
          "type": "torch.Tensor",
          "contents": [
            -6.0,
            -4.0,
            -4.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            -6.0,
            -4.0,
            6.0,
            -4.0,
            8.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            -6.0,
            -4.0,
            -6.0,
            -4.0,
            8.0,
            6.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            -6.0,
            8.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            -6.0,
            -6.0,
            8.0,
            -6.0,
            -6.0,
            8.0,
            -4.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            -6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            8.0,
            -4.0,
            -4.0,
            -4.0,
            -4.0,
            6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            8.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            8.0,
            -4.0,
            -4.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            6.0,
            -4.0,
            8.0,
            6.0,
            -6.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            -4.0,
            -4.0,
            -4.0,
            6.0,
            8.0,
            -6.0,
            6.0,
            8.0,
            -6.0,
            8.0,
            6.0,
            8.0,
            8.0,
            8.0,
            -4.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            6.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            -4.0,
            8.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            -4.0,
            -4.0,
            8.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -4.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            8.0,
            6.0,
            8.0,
            -4.0,
            -4.0,
            -6.0,
            6.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            8.0,
            -6.0,
            6.0,
            6.0,
            6.0,
            6.0,
            8.0,
            6.0,
            8.0,
            -6.0,
            8.0,
            -4.0,
            -4.0,
            -6.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            8.0,
            6.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            6.0,
            6.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            -6.0,
            6.0,
            -4.0,
            6.0,
            -4.0,
            6.0,
            6.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            -6.0,
            -4.0,
            -6.0,
            -4.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            -4.0,
            8.0,
            -4.0,
            6.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            -4.0,
            6.0,
            8.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            6.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            8.0,
            8.0,
            -6.0,
            -4.0,
            -4.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            6.0,
            8.0,
            -6.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            -4.0,
            8.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            -4.0,
            -6.0,
            -4.0,
            -6.0,
            8.0,
            -6.0,
            6.0,
            -6.0,
            -6.0,
            6.0,
            -6.0,
            6.0,
            8.0,
            8.0,
            6.0,
            6.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            -6.0,
            8.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            -6.0,
            -6.0,
            8.0,
            6.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            8.0,
            -4.0,
            -4.0,
            -4.0,
            -6.0,
            -6.0,
            -6.0,
            -4.0,
            8.0,
            6.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            8.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            -4.0,
            -4.0,
            8.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            -6.0,
            6.0,
            8.0,
            6.0,
            -6.0,
            6.0,
            8.0,
            6.0,
            -6.0,
            8.0,
            6.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            8.0,
            6.0,
            8.0,
            6.0,
            -6.0,
            -4.0,
            -4.0,
            6.0,
            8.0,
            8.0,
            -4.0,
            6.0,
            -4.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            -4.0,
            6.0,
            6.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            8.0,
            -6.0,
            -4.0,
            8.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            6.0,
            6.0,
            -6.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            6.0,
            8.0,
            8.0,
            8.0,
            -6.0,
            6.0,
            -6.0,
            8.0,
            -6.0,
            8.0,
            6.0,
            8.0,
            -4.0,
            -4.0,
            -4.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            8.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            -6.0,
            -4.0,
            8.0,
            -4.0,
            -4.0,
            8.0,
            -6.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            -4.0,
            -6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            8.0,
            -6.0,
            8.0,
            6.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            -4.0,
            -4.0,
            -4.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            8.0,
            6.0,
            -4.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            6.0,
            6.0,
            -6.0,
            -4.0,
            8.0,
            -6.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            8.0,
            8.0,
            8.0,
            -6.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            -6.0,
            -4.0,
            -6.0,
            8.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            -6.0,
            -4.0,
            -4.0,
            8.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            -4.0,
            -6.0,
            -6.0,
            -4.0,
            -4.0,
            8.0,
            -6.0,
            -4.0,
            8.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            8.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            6.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            8.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            6.0,
            8.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            8.0,
            -4.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            -4.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            8.0,
            8.0,
            -4.0,
            8.0,
            8.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            8.0,
            -6.0,
            8.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            -4.0,
            -4.0,
            -6.0,
            -6.0,
            6.0,
            -6.0,
            6.0,
            -6.0,
            8.0,
            -6.0,
            6.0,
            -6.0,
            -4.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            6.0,
            8.0,
            6.0,
            -6.0,
            8.0,
            6.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            -4.0,
            -4.0,
            6.0,
            -4.0,
            -6.0,
            6.0,
            -4.0,
            -4.0,
            -4.0,
            -4.0,
            6.0,
            -4.0,
            -6.0,
            8.0,
            -6.0,
            6.0,
            8.0,
            -4.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            6.0,
            -4.0,
            -4.0,
            6.0,
            -6.0,
            8.0,
            -4.0,
            -4.0,
            6.0,
            -4.0,
            6.0,
            -4.0,
            8.0,
            8.0,
            -4.0,
            6.0,
            -4.0,
            -4.0,
            6.0,
            8.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            8.0,
            8.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            8.0,
            -6.0,
            6.0,
            8.0,
            -6.0,
            6.0,
            6.0,
            -4.0,
            8.0,
            6.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            -4.0,
            8.0,
            8.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            8.0,
            8.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            -6.0,
            6.0,
            6.0,
            -4.0,
            8.0,
            6.0,
            6.0,
            6.0,
            -6.0,
            6.0,
            8.0,
            6.0,
            6.0,
            6.0,
            -4.0,
            8.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            8.0,
            6.0,
            8.0,
            8.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            8.0,
            -6.0,
            -4.0,
            6.0,
            -6.0,
            -6.0,
            -6.0,
            -4.0,
            -4.0,
            6.0,
            8.0,
            6.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            -4.0,
            6.0,
            -4.0,
            -4.0,
            -6.0,
            -6.0,
            -4.0,
            -6.0,
            8.0,
            6.0,
            -6.0,
            8.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            -4.0,
            -4.0,
            -4.0,
            8.0,
            -4.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            -4.0,
            6.0,
            -4.0,
            6.0,
            -6.0,
            -4.0,
            -4.0,
            8.0,
            6.0,
            8.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            -6.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            6.0,
            8.0,
            8.0,
            6.0,
            8.0,
            -6.0,
            8.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            -6.0,
            -4.0,
            -6.0,
            6.0,
            6.0,
            -4.0,
            -6.0,
            8.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            -4.0,
            -4.0,
            8.0,
            -4.0,
            6.0,
            6.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            8.0,
            8.0,
            -4.0,
            -4.0,
            8.0,
            -4.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            8.0,
            8.0,
            6.0,
            -4.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            8.0,
            8.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            8.0,
            -6.0,
            -6.0,
            8.0,
            6.0,
            8.0,
            -4.0,
            8.0,
            8.0,
            -4.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            6.0,
            6.0,
            -4.0,
            8.0,
            -6.0,
            -6.0,
            -6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            6.0,
            -4.0,
            -6.0,
            -4.0,
            -6.0,
            8.0,
            -6.0,
            6.0,
            -4.0,
            6.0,
            6.0,
            8.0,
            6.0,
            -6.0,
            6.0,
            6.0,
            6.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            -6.0,
            6.0,
            6.0,
            8.0,
            8.0,
            -4.0,
            8.0,
            6.0,
            6.0,
            6.0,
            6.0,
            8.0,
            -4.0,
            -6.0,
            -6.0,
            8.0,
            8.0,
            8.0,
            8.0,
            6.0,
            -4.0,
            -4.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            -4.0,
            6.0,
            -6.0,
            -4.0,
            8.0,
            -4.0,
            -6.0,
            8.0,
            -4.0,
            -4.0,
            8.0,
            -6.0,
            -4.0,
            -4.0,
            6.0,
            -6.0,
            8.0,
            -4.0,
            -6.0,
            6.0,
            -6.0,
            -6.0,
            -4.0,
            -4.0,
            -6.0,
            8.0,
            -6.0,
            8.0,
            -6.0,
            -6.0,
            6.0,
            8.0,
            8.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1000
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 555,
          "function_name": "evaluate_estimator",
          "code": "mean = torch.mean(samples)  # @inspect mean"
        }
      ],
      "env": {
        "mean": {
          "type": "torch.Tensor",
          "contents": 1.0499999523162842,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 556,
          "function_name": "evaluate_estimator",
          "code": "variance = torch.var(samples)  # @inspect variance"
        }
      ],
      "env": {
        "variance": {
          "type": "torch.Tensor",
          "contents": 36.554054260253906,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 558,
          "function_name": "evaluate_estimator",
          "code": "\"mean\": mean,"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 559,
          "function_name": "evaluate_estimator",
          "code": "\"variance\": variance,"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 557,
          "function_name": "evaluate_estimator",
          "code": "return {"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 511,
          "function_name": "variance_reduction",
          "code": "result1 = evaluate_estimator(estimator1)  # @inspect result1"
        }
      ],
      "env": {
        "result1": {
          "type": "dict",
          "contents": {
            "mean": {
              "type": "torch.Tensor",
              "contents": 1.0499999523162842,
              "dtype": "torch.float32",
              "shape": []
            },
            "variance": {
              "type": "torch.Tensor",
              "contents": 36.554054260253906,
              "dtype": "torch.float32",
              "shape": []
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 513,
          "function_name": "variance_reduction",
          "code": "text(\"A more expensive estimator is to sample 2 points and average:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "A more expensive estimator is to sample 2 points and average:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 514,
          "function_name": "variance_reduction",
          "code": "def estimator2():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 514,
          "function_name": "estimator2",
          "code": "def estimator2():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 515,
          "function_name": "estimator2",
          "code": "indices = torch.multinomial(probs, num_samples=2)  # @inspect indices"
        }
      ],
      "env": {
        "indices": {
          "type": "torch.Tensor",
          "contents": [
            3,
            1
          ],
          "dtype": "torch.int64",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 516,
          "function_name": "estimator2",
          "code": "values = points[indices]  # @inspect values"
        }
      ],
      "env": {
        "values": {
          "type": "torch.Tensor",
          "contents": [
            8.0,
            -6.0
          ],
          "dtype": "torch.float32",
          "shape": [
            2
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 517,
          "function_name": "estimator2",
          "code": "estimate = torch.mean(values)  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": 1.0,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 518,
          "function_name": "estimator2",
          "code": "return estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 519,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": 1.0,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 520,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": -5.0,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 521,
          "function_name": "variance_reduction",
          "code": "estimate = estimator2()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": 7.0,
          "dtype": "torch.float32",
          "shape": []
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 522,
          "function_name": "variance_reduction",
          "code": "result2 = evaluate_estimator(estimator2)  # @inspect result2 @stepover"
        }
      ],
      "env": {
        "result2": {
          "type": "dict",
          "contents": {
            "mean": {
              "type": "torch.Tensor",
              "contents": 0.9610000252723694,
              "dtype": "torch.float32",
              "shape": []
            },
            "variance": {
              "type": "torch.Tensor",
              "contents": 12.41789722442627,
              "dtype": "torch.float32",
              "shape": []
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 524,
          "function_name": "variance_reduction",
          "code": "text(\"A worse estimator is to add noise, but it's still unbiased:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "A worse estimator is to add noise, but it's still unbiased:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 525,
          "function_name": "variance_reduction",
          "code": "def estimator3():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 525,
          "function_name": "estimator3",
          "code": "def estimator3():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 527,
          "function_name": "estimator3",
          "code": "index = torch.multinomial(probs, num_samples=1)  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "torch.Tensor",
          "contents": [
            2
          ],
          "dtype": "torch.int64",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 528,
          "function_name": "estimator3",
          "code": "noise = torch.randn(1)  # @inspect noise"
        }
      ],
      "env": {
        "noise": {
          "type": "torch.Tensor",
          "contents": [
            1.3690204620361328
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 529,
          "function_name": "estimator3",
          "code": "estimate = points[index] + noise  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            7.369020462036133
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 530,
          "function_name": "estimator3",
          "code": "return estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 531,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            7.369020462036133
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 532,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            -5.008062839508057
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 533,
          "function_name": "variance_reduction",
          "code": "estimate = estimator3()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            6.180088520050049
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 534,
          "function_name": "variance_reduction",
          "code": "result3 = evaluate_estimator(estimator3)  # @inspect result3 @stepover"
        }
      ],
      "env": {
        "result3": {
          "type": "dict",
          "contents": {
            "mean": {
              "type": "torch.Tensor",
              "contents": 1.2687219381332397,
              "dtype": "torch.float32",
              "shape": []
            },
            "variance": {
              "type": "torch.Tensor",
              "contents": 37.58057403564453,
              "dtype": "torch.float32",
              "shape": []
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 536,
          "function_name": "variance_reduction",
          "code": "text(\"Let's suppose we have a magic function offset(i) that has mean 0.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let's suppose we have a magic function offset(i) that has mean 0.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 537,
          "function_name": "variance_reduction",
          "code": "text(\"Then E[f(i) - offset(i)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Then E[f(i) - offset(i)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 538,
          "function_name": "variance_reduction",
          "code": "text(\"= E[f(i)] - E[offset(i)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "= E[f(i)] - E[offset(i)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 539,
          "function_name": "variance_reduction",
          "code": "text(\"= E[f(i)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "= E[f(i)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 540,
          "function_name": "variance_reduction",
          "code": "def estimator4():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 540,
          "function_name": "estimator4",
          "code": "def estimator4():"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 541,
          "function_name": "estimator4",
          "code": "offsets = torch.tensor([-6., -6., 6., 6.])"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 542,
          "function_name": "estimator4",
          "code": "index = torch.multinomial(probs, num_samples=1)  # @inspect index"
        }
      ],
      "env": {
        "index": {
          "type": "torch.Tensor",
          "contents": [
            3
          ],
          "dtype": "torch.int64",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 543,
          "function_name": "estimator4",
          "code": "estimate = points[index] - offsets[index]  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            2.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 544,
          "function_name": "estimator4",
          "code": "return estimate"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 545,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            2.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 546,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            2.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 547,
          "function_name": "variance_reduction",
          "code": "estimate = estimator4()  # @inspect estimate @stepover"
        }
      ],
      "env": {
        "estimate": {
          "type": "torch.Tensor",
          "contents": [
            0.0
          ],
          "dtype": "torch.float32",
          "shape": [
            1
          ]
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 548,
          "function_name": "variance_reduction",
          "code": "result4 = evaluate_estimator(estimator4)  # @inspect result4 @stepover"
        }
      ],
      "env": {
        "result4": {
          "type": "dict",
          "contents": {
            "mean": {
              "type": "torch.Tensor",
              "contents": 1.0299999713897705,
              "dtype": "torch.float32",
              "shape": []
            },
            "variance": {
              "type": "torch.Tensor",
              "contents": 1.0001001358032227,
              "dtype": "torch.float32",
              "shape": []
            }
          },
          "dtype": null,
          "shape": null
        }
      },
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 458,
          "function_name": "policy_gradient_enhancements",
          "code": "variance_reduction()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 460,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"So how do we find an offset(s, a) that E[offset(s, a)] = 0?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "So how do we find an offset(s, a) that E[offset(s, a)] = 0?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 462,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Key identity:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Key identity:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 463,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Let b(s) be **any** function that only depends on state s (not action a).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Let b(s) be **any** function that only depends on state s (not action a).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 464,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Then E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(a | s) * b(s)] = 0 for all s, and a ~ \u03c0_\u03b8(a | s).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Then E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(a | s) * b(s)] = 0 for all s, and a ~ \u03c0_\u03b8(a | s).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 465,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Analogy: heuristics in A* search (use domain knowledge to improve algorithm)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Analogy: heuristics in A* search (use domain knowledge to improve algorithm)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 467,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Proof:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Proof:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 468,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- E_\u03b8[b(s)] = constant\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- E_\u03b8[b(s)] = constant",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 469,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- \u2207_\u03b8 E_\u03b8[b(s)] = 0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- \u2207_\u03b8 E_\u03b8[b(s)] = 0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 470,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- \u2207_\u03b8 \u03a3_a \u03c0_\u03b8(a | s) b(s) = 0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- \u2207_\u03b8 \u03a3_a \u03c0_\u03b8(a | s) b(s) = 0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 471,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- \u03a3_a \u2207_\u03b8 \u03c0_\u03b8(a | s) b(s) = 0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- \u03a3_a \u2207_\u03b8 \u03c0_\u03b8(a | s) b(s) = 0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 472,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- \u03a3_a \u03c0_\u03b8(a | s) \u2207_\u03b8 log \u03c0_\u03b8(a | s) b(s) = 0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- \u03a3_a \u03c0_\u03b8(a | s) \u2207_\u03b8 log \u03c0_\u03b8(a | s) b(s) = 0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 473,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(a | s) * b(s)] = 0\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(a | s) * b(s)] = 0",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 474,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"This is basically the reverse direction of the policy gradient theorem.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "This is basically the reverse direction of the policy gradient theorem.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 476,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Enhancements to reduce variance:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Enhancements to reduce variance:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 477,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"1. Baselines: subtract off a baseline b(s) from the utility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Baselines: subtract off a baseline b(s) from the utility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 478,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (utility(\ud835\udf0f) - b(s_{t-1}))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (utility(\ud835\udf0f) - b(s_{t-1}))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 479,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"2. Use returns-to-go instead of utility\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Use returns-to-go instead of utility",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 480,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (r_t + \u03b3 r_{t+1} + \u03b3^2 r_{t+2} + ... - b(s_{t-1}))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (r_t + \u03b3 r_{t+1} + \u03b3^2 r_{t+2} + ... - b(s_{t-1}))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 481,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"3. Use a **biased** estimate of the value function Q(s, a) instead of utility(\ud835\udf0f) (bootstrapping).\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Use a **biased** estimate of the value function Q(s, a) instead of utility(\ud835\udf0f) (bootstrapping).",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 482,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (Q(s_t, a_t) - b(s_{t-1}))\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "J(\u03b8, \ud835\udf0f) = \u03a3_t log \u03c0_\u03b8(a_t | s_{t-1}) * (Q(s_t, a_t) - b(s_{t-1}))",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 484,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 485,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- The game: find a low-bias, low-variance estimate of E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- The game: find a low-bias, low-variance estimate of E_\u03b8[\u2207_\u03b8 log \u03c0_\u03b8(\ud835\udf0f) * utility(\ud835\udf0f)]",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 486,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- Baselines: a general-purpose strategy to reduce variance, still unbiased\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Baselines: a general-purpose strategy to reduce variance, still unbiased",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 487,
          "function_name": "policy_gradient_enhancements",
          "code": "text(\"- Bootstrapping: reduce variance but introduce a bit of bias\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Bootstrapping: reduce variance but introduce a bit of bias",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        },
        {
          "path": "policy_gradient.py",
          "line_number": 262,
          "function_name": "policy_gradient",
          "code": "policy_gradient_enhancements()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 21,
          "function_name": "main",
          "code": "policy_gradient()"
        }
      ],
      "env": {},
      "renderings": []
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 23,
          "function_name": "main",
          "code": "text(\"Summary:\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Summary:",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 24,
          "function_name": "main",
          "code": "text(\"- Model-based: estimate the MDP\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Model-based: estimate the MDP",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 25,
          "function_name": "main",
          "code": "text(\"- Value-based: estimate Q-values\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Value-based: estimate Q-values",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 26,
          "function_name": "main",
          "code": "text(\"- Policy-based: estimate policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "- Policy-based: estimate policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 28,
          "function_name": "main",
          "code": "text(\"Form of algorithm\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Form of algorithm",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 29,
          "function_name": "main",
          "code": "text(\"1. Generate rollouts from exploration policy\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "1. Generate rollouts from exploration policy",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 30,
          "function_name": "main",
          "code": "text(\"2. Form some loss function (parameters of MDP, Q-values, or policy)\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "2. Form some loss function (parameters of MDP, Q-values, or policy)",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 31,
          "function_name": "main",
          "code": "text(\"3. Update parameters using gradient step\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "3. Update parameters using gradient step",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 33,
          "function_name": "main",
          "code": "text(\"In MDPs and RL, we're maximizing expected utility.\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "In MDPs and RL, we're maximizing expected utility.",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    },
    {
      "stack": [
        {
          "path": "policy_gradient.py",
          "line_number": 34,
          "function_name": "main",
          "code": "text(\"Next time: what if there are adversaries in the environment?\")"
        }
      ],
      "env": {},
      "renderings": [
        {
          "type": "markdown",
          "data": "Next time: what if there are adversaries in the environment?",
          "style": {},
          "external_link": null,
          "internal_link": null
        }
      ]
    }
  ]
}